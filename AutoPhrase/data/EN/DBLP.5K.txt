OQL[C++]: Extending C++ with an Object Query Capability.
Transaction Management in Multidatabase Systems.
Overview of the ADDS System.
Multimedia Information Systems: Issues and Approaches.
Active Database Systems.
Where Object-Oriented DBMSs Should Do Better: A Critique Based on Early Experiences.
Distributed Databases.
An Object-Oriented DBMS War Story: Developing a Genome Mapping Database in C++.
Cooperative Transactions for Multiuser Environments.
Schema Architecture of the UniSQL/M Multidatabase System
Physical Object Management.
Introduction to Part 1: Next-Generation Database Technology.
Object-Oriented Database Systems: Promises, Reality, and Future.
Introduction to Part 2: Technology for Interoperating Legacy Databases.
On Resolving Schematic Heterogeneity in Multidatabase Systems.
Requirements for a Performance Benchmark for Object-Oriented Database Systems.
On View Support in Object-Oriented Databases Systems.
The POSC Solution to Managing E&P Data.
C++ Bindings to an Object Database.
Authorization in Object-Oriented Databases.
Query Processing in Multidatabase Systems.
Management of Uncerainty in database Systems.
Parallel Relational Database Systems.
Query Processing in Object-Oriented Database Systems.
Specification and Execution of Transactional Workflows.
Spatial Data Structures.
An overview is presented of the use of spatial data structures in spatial databases. The focus is on hierarchical data structures, including a number of variants of quadtrees, which sort the data with respect to the space occupied by it. Such techniques are known as spatial indexing methods. Hierarchical data structures are based on the principle of recursive decomposition. They are attractive because they are compact and depending on the nature of the data they save space as well as time and also facilitate operations such as search. Examples are given of the use of these data structures in the representation of different data types such as regions, points, rectangles, lines, and volumes.
Spatial Data Models and Query Processing.
Pegasus: A Heterogeneous Information Management System.
Temporal Object-Oriented Databases: A Critical Comparison.
The OMG Object Model.
EDA/SQL.
The Changing Database Standards Landscape.
Modern Database Systems: The Object Model, Interoperability, and Beyond.
Inequalities: Theory of Majorization and Its Application.
Version Control in an Object-Oriented Architecture.
The GemStone Data Management System.
Storage Management in EXODUS.
A Distributed Object Manager for the Smalltalk-80 System.
Objects, Messages, and Rules in Database Design.
Active Objects: Ealities and Possibilities.
Overview of the Iris DBMS.
Features of the ORION Object-Oriented Database System.
Indexing Techniques for Object-Oriented Databases.
My Cat Is Object-Oriented.
Making Database Systems Fast Enough for CAD Applications.
Optimizing Smalltalk Message Performance.
The Common List Object-Oriented Programming Language Standard.
Object Orientation as Catalyst for Language-Database Inegration.
A Survey of Object-Oriented Concepts.
Integrated Office Systems.
Proteus: A Frame-Based Nonmonotonic Inference System.
Concurrency Control and Object-Oriented Databases.
A Shared View of Sharing: The Treaty of Orlando.
Pogo: A Declarative Representation System for Graphics.
Concurrent Object-Oriented Programming Languages.
Directions in Object-Oriented Research.
A Proposal for a Formal Model of Objects.
OZ+: An Object-Oriented Database System.
The Commercial INGRES Epilogue.
Design of Relational Systems (Introduction to Section 1).
Supporting Studies on Relational Systems (Introduction to Section 2).
Distributed Database Systems (Introduction to Section 3).
The Design and Implementation of Distributed INGRES.
User Interfaces for Database Systems (Introduction to Section 4).
Extended Semantics for the Relational Model (Introduction to Section 5).
Database Design (Introduction to Section 6).
Title, Preface, Contents.
References.
TeX: The Program
Foundations of Databases.
LaTeX: User's Guide & Reference Manual
The Design and Analysis of Computer Algorithms.
Specifying Systems, The TLA+ Language and Tools for Hardware and Software Engineers
From the Book: This book will teach you how to write specifications of computer systems, using the language TLA+. It's rather long, but most people will read only Part I, which comprises the first 83 pages. That part contains all that most engineers need to know about writing specifications; it assumes only the basic background in computing and knowledge of mathematics expected of an undergraduate studying engineering or computer science. Part II contains more advanced material for more sophisticated readers. The remainder of the book is a reference manualPart III for the TLA+ tools and Part IV for the language itself. The TLA World Wide Web page contains material to accompany the book, including the TLA+ tools, exercises, references to the literature, and a list of corrections. There is a link to the TLA Web page on http://lamport.org.What Is a Specification?Writing is nature's way of letting you know how sloppy your thinking is.-GuindonA specification is a written description of what a system is supposed to do. Specifying a system helps us understand it. It's a good idea to understand a system before building it, so it's a good idea to write a specification of a system before implementing it.This book is about specifying the behavioral properties of a systemalso called its functional or logical properties. These are the properties that specify what the system is supposed to do. There are other important kinds of properties that we don't consider, including performance properties. Worst-case performance can often be expressed as a behavioral propertyforexample, Chapter 9 explains how to specify that a system must react within a certain length of time. However, specifying average performance is beyond the scope of the methods described here.Our basic tool for writing specifications is mathematics. Mathematics is nature's way of letting you know how sloppy your writing is. It's hard to be precise in an imprecise language like English or Chinese. In engineering, imprecision can lead to errors. To avoid errors, science and engineering have adopted mathematics as their language.The mathematics we use is more formal than the math you've grown up with. Formal mathematics is nature's way of letting you know how sloppy your mathematics is. The mathematics written by most mathematicians and scientists is not really precise. It's precise in the small, but imprecise in the large. Each equation is a precise assertion, but you have to read the accompanying words to understand how the equations relate to one another and exactly what the theorems mean. Logicians have developed ways of eliminating those words and making the mathematics completely formal, and hence completely precise.Most mathematicians and scientists think that formal mathematics, without words, is long and tiresome. They're wrong. Ordinary mathematics can be expressed compactly in a precise, completely formal language. It takes only about two dozen lines to define the solution to an arbitrary differential equation in the Differential Equations module of Chapter 11. But few specifications need such sophisticated mathematics. Most require only simple application of a few standard mathematical concepts.Why TLA+?We specify a system by describing its allowed behaviorswhat it may do in the course of an execution. In 1977, Amir Pnueli introduced the use of temporal logic for describing system behaviors. In principle, a system could be described by a single temporal logic formula. In practice, it couldn't. Pnueli's temporal logic was ideal for describing some properties of systems, but awkward for others. So, it was usually combined with a more traditional way of describing systems.In the late 1980s, I invented TLA, the Temporal Logic of Actionsa simple variant of Pnueli's original logic. TLA makes it practical to describe a system by a single formula. Most of a TLA specification consists of ordinary, nontemporal mathematics. Temporal logic plays a significant role only in describing those properties that it's good at describing. TLA also provides a nice way to formalize the style of reasoning about systems that has proved to be most effective in practicea style known as assertional reasoning. However, this book is about specification; it says almost nothing about proofs.Temporal logic assumes an underlying logic for expressing ordinary mathematics. There are many ways to formalize ordinary math. Most computer scientists prefer one that resembles their favorite programming language. I chose instead the one that most mathematicians preferthe one logicians call first-order logic and set theory.TLA provides a mathematical foundation for describing systems. To write specifications, we need a complete language built atop that foundation. I initially thought that this language should be some sort of abstract programming language whose semantics would be based on TLA. I didn't know what kind of programming language constructs would be best, so I decided to start writing specifications directly in TLA. I intended to introduce programming constructs as I needed them. To my surprise, I discovered that I didn't need them. What I needed was a robust language for writing mathematics.Although mathematicians have developed the science of writing formulas, they haven't turned that science into an engineering discipline. They have developed notations for mathematics in the small, but not for mathematics in the large. The specification of a real system can be dozens or even hundreds of pages long. Mathematicians know how to write 20-line formulas, not 20-page formulas. So, I had to introduce notations for writing long formulas. What I took from programming languages were ideas for modularizing large specifications.The language I came up with is called TLA+. I refined TLA+ in the course of writing specifications of disparate systems. But it has changed little in the last few years. I have found TLA+ to be quite good for specifying a wide class of systemsfrom program interfaces (APIs) to distributed systems. It can be used to write a precise, formal description of almost any sort of discrete system. It's especially well suited to describing asynchronous systemsthat is, systems with components that do not operate in strict lock-step.About This BookPart I, consisting of Chapters 1 through 7, is the core of the book and is meant to be read from beginning to end. It explains how to specify the class of properties known as safety properties. These properties, which can be specified with almost no temporal logic, are all that most engineers need to know about. After reading Part I, you can read as much of Part II as you like. Each of its chapters is independent of the others. Temporal logic comes to the fore in Chapter 8, where it is used to specify the additional class of properties known as liveness properties. Chapter 9 describes how to specify real-time properties, and Chapter 10 describes how to write specifications as compositions. Chapter 11 contains more advanced examples. The three chapters in Part III serve as the reference manual for three TLA+ tools: the Syntactic Analyzer, the TLATEX typesetting program, and the TLC model checker. If you want to use TLA+, then you probably want to use these tools. They are available from the TLA Web page. TLC is the most sophisticated of them. The examples on the Web can get you started using it, but you'll have to read Chapter 14 to learn to use TLC effectively.Part IV is a reference manual for the TLA+ language. Part I provides a good enough working knowledge of the language for most purposes. You need look at Part IV only if you have questions about the fine points of the syntax and semantics. Chapter 15 gives the syntax of TLA+. Chapter 16 describes the precise meanings and the general forms of all the built-in operators of TLA+; Chapter 17 describes the precise meaning of all the higher-level TLA+ constructs such as definitions. Together, these two chapters specify the semantics of the language. Chapter 18 describes the standard modulesexcept for module RealTime, described in Chapter 9, and module TLC, described in Chapter 14. You might want to look at this chapter if you're curious about how standard elementary mathematics can be formalized in TLA+.Part IV does have something you may want to refer to often: a mini-manual that compactly presents lots of useful information. Pages 268-273 list all TLA+ operators, all user-definable symbols, the precedence of all operators, all operators defined in the standard modules, and the ASCII representation of symbols.
Data Structures and Algorithms.
Databases and Transaction Processing: An Application-Oriented Approach
The AWK Programming Language
The Java Virtual Machine Specification
Compilers: Princiles, Techniques, and Tools.
Algorithms
This presentation is a tutorial on AutoView, an AutoMod extension package that recreates model animation according to a user-defined script. AutoView allows users to restart animation and move back and forth in time and 3-D space. Animation is created using text files generated from an AutoMod simulation model. By creating a user-defined camera description file, animation can take place that allows for panning from one view to another, as well as for attaching the camera to a simulated load or vehicle and traveling with that entity during portions of the animation. The animation is provided at an enhanced response time because it does not have the logical and statistical processes of AutoMod (which can slow animation). This tutorial demonstrates several animation scripts previously constructed with AutoView, discusses the creation of AutoView files from AutoMod, and describes the development of a camera description file.
The Java Programming Language
Algorithms, 2nd Edition
The Java Programming Language, Second Edition
Operating System Concepts, 4th edition.
The Java Programming Language, Third Edition
The TeXbook
Modern Information Retrieval
:This is a rigorous and complete textbook for a first course on information retrieval from the computer science (as opposed to a user-centred) perspective. The advent of the Internet and the enormous increase in volume of electronically stored information generally has led to substantial work on IR from the computer science perspective - this book provides an up-to-date student oriented treatment of the subject.
Conceptual Structures: Information Processing in Mind and Machine
Concurrency Control and Recovery in Database Systems.
Code Reading: The Open Source Perspective
Intelligent Database Systems
:a valuable resource for anyone working on intelligent database systems. Good background coverage of both the field of databases and that of AI are provided in the first part of the book, and later there are some excellent analytical discussions of relevant projects. The authors adopt a very readable style which enables a complex topic to become much more accessible." - Jenny Carter, Department of Computer Science, De Montfort University This book provides a state of the art guide to the new developments in expert database systems, from the unique perspective of both the database and AI areas. It gives complete and detailed coverage of the latest research and practice, including all the need-to-know technical and theoretical approaches in the area. Drawing on their extensive experience, the authors evaluate how AI techniques can be integrated with present and future database systems and knowledge based management systems, incorporating AI expertise into system design. The book also addresses the techniques developed recently which are directly used, or are the basis, for data retrieval across the world wide web. If you are a designer or developer working in the database community, with database and AI products or applications, this book will help you to understand crucial research developments and to apply the results in practice. Includes: &middot; Mechanisms for handling data and knowledge including XML, web indexing, search engines & data mining &middot; Object Oriented Database Management Systems and object-relational DBMS &middot; Data modeling including techniques suchas Entity Relationship, Functional Data & Semantic Database models, as well as using the notations for modeling (OMT & UML) Plus! Extended case studies of commercial systems About the authors: Elisa Bertino is a well known expert in the integration of AI and database techniques, areas of O-O, distributed, deductive and multimedia databases and database security. She has chaired and given tutorials at many international conferences and published hundreds of journal papers and a book. Elisa is currently a professor of computer science at the University of Milan. Barbara Catania is an assistant professor of computer science at the University of Genova, specializing in deductive and multimedia databases, and indexing techniques in object-oriented and constraint databases. She has presented at a number of international conferences and co-authored a book. Gian Piero Zarri is an internationally renowned consultant and researcher in the areas of knowledge-based systems, natural-language processing, databases and information retrieval systems. He is on the editorial board of a number of international scientific journals and on the program committee of many conferences on Knowledge-Based Systems. Gian Piero currently works as Research Director for CNRS, the French National Centre for Scientific Research.
How to Set Up and Maintain a World Wide Web Site: The Guide for Information Providers.
Object-Oriented Database Systems
Advanced Programming in the UNIX Environment
PROLOG Programming for Artificial Intelligence, Second Edition
TCP/IP Illustrated, Volume 1: The Protocols
TCP/IP Illustrated, Volume 1 is a complete and detailed guide to the entire TCP/IP protocol suite - with an important difference from other books on the subject. Rather than just describing what the RFCs say the protocol suite should do, this unique book uses a popular diagnostic tool so you may actually watch the protocols in action.By forcing various conditions to occur - such as connection establishment, timeout and retransmission, and fragmentation - and then displaying the results, TCP/IP Illustrated gives you a much greater understanding of these concepts than words alone could provide. Whether you are new to TCP/IP or you have read other books on the subject, you will come away with an increased understanding of how and why TCP/IP works the way it does, as well as enhanced skill at developing aplications that run over TCP/IP.
Database Security.
TCP/IP Illustrated, Volume 3: TCP for Transactions, HTTP, NNTP, and the UNIX Domain Protocolls
Object Data Management: Object-Oriented and Extended Relational Database Systems
Object Data Management: Object-Oriented and Extended Relational Database Systems (Revised Edition)
The C++ Programming Language, First Edition
Designing Database Applications with Objects and Rules: The IDEA Methodology
The C++ Programming Language, Second Edition
The Java Developers Almanac 1999
Introduction to Data Mining
The Java Class Libraries - An Annotated Reference
Datenmodelle, Datenbanksprachen und Datenbank-Management-Systeme
The Java Class Libraries, Second Edition, Volume 2
Datenmodelle, Datenbanksprachen und Datenbank-Management-Systeme. 2. Auflage
The Java Class Libraries, Second Edition, Volume 1
Das DB2-Handbuch.
The Java Class Libraries, Second Edition, Volume 1, Supplement for the Java 2 Platform Standard Edition, v1.2
Time Series Prediction: Forecasting the Future and Understanding the Past.
The Relational Model for Database Management, Version 2
From the Preface (See Front Matter for full Preface) An important adjunct to precision is a sound theoretical foundation. The relational model is solidly based on two parts of mathematics: firstorder predicate logic and the theory of relations. This book, however, does not dwell on the theoretical foundations, but rather on all the features of the relational model that I now perceive as important for database users, and therefore for DBMS vendors. My perceptions result from 20 years of practical experience in computing and data processing (chiefly, but not exclusively, with large-scale customers of IBM), followed by another 20 years of research. I believe that this is the first book to deal exclusively with the relational approach. It does, however, include design principles in Chapters 21 and 22. It is also the first book on the relational model by the originator of that model. All the ideas in the relational model described in this book are mine, except in cases where I explicitly credit someone else. In developing the relational model, I have tried to follow Einstein's advice, "Make it as simple as possible, but no simpler." I believe that in the last clause he was discouraging the pursuit of simplicity to the extent of distorting reality. So why does the book contain 30 chapters and two appendixes? To answer this question, it is necessary to look at the history of research and development of the relational model.
Java Platform Performance - Strategies and Tactics
:This book addresses a vital issue for all those developing software for the Java™ platform: how to achieve maximum performance and scalability for their applications. Drawing on the authors' knowledge of the Java programming language and their extensive experience working on performance issues, the book reveals common mistakes and misconceptions concerning the performance characteristics of Java technologies. It offers overall development strategies and concrete, battle-tested techniques to dramatically improve the performance of applications constructed with the Java programming language. Java™ Platform Performance highlights the importance of integrating performance evaluation into the application development process and discusses measurement techniques. The book then presents practical tactics for enhancing application performance in the areas of I/O, RAM footprint, small object management, algorithms, data structures, Swing, and deployment. Specific topics covered include: Incorporating performance evaluation into the development process Profiling and benchmarking Building scalable, fast Swing GUIs Using high-speed I/O Computing and controlling the RAM footprint Reducing the number of classes Eliminating temporary objects Selecting high-performance algorithms and data structures Using Java native code and applet packaging efficiently Garbage collection Java HotSpot™ technology With an understanding of the performance issues and specific techniques for reducing overhead discussed in this book, you will have theinformation you need to enhance the efficiency, speed, and scalability of your software.
Advanced C++: Programming Syles and Idioms
Deductive Databases and Logic Programming
High performance compilers for parallel computing
An Introduction to Database Systems
:The sixth edition of this well-respected text/reference--which has been almost completely rewritten--continues to be the most comprehensive and up-to-date treatment of database technology currently available. Readers will gain a strong working knowledge of the overall structure, concepts, and objectives of database systems and will become familiar with the theoretical principles underlying the construction of such systems.
TCP/IP Illustrated, Volume 2: The Implementation
An Introduction to Database Systems, 2nd Edition
Human Behaviour and the Principle of Least Effort: an Introduction to Human Ecology
An Introduction to Database Systems, 3rd Edition
:For over 25 years, C. J. Date's An Introduction to Database Systems has been the authoritative resource for readers interested in gaining insight into and understanding of the principles of database systems. This revision continues to provide a solid grounding in the foundations of database technology and to provide some ideas as to how the field is likely to develop in the future.. "Readers of this book will gain a strong working knowledge of the overall structure, concepts, and objectives of database systems and will become familiar with the theoretical principles underlying the construction of such systems.
The INGRES Papers: Anatomy of a Relational Database System
An Introduction to Database Systems, Volume II.
A Guide to DB2, 1st Edition
An Introduction to Database Systems, Volume I, 4th Edition.
An Introduction to Database Systems, Volume I, 5th Edition.
An Introduction to Database Systems, 6th Edition.
Relational Database Writings 1989-1991
A Guide to SQL Standard, 3rd Edition
A Guide to SQL Standard, 4th Edition
A Guide to DB2, 2nd Edition
A Guide to DB2, 3rd Edition
Cryptography and Data Security
From the Preface (See Front Matter for full Preface) Electronic computers have evolved from exiguous experimental enterprises in the 1940s to prolific practical data processing systems in the 1980s. As we have come to rely on these systems to process and store data, we have also come to wonder about their ability to protect valuable data. Data security is the science and study of methods of protecting data in computer and communication systems from unauthorized disclosure and modification. The goal of this book is to introduce the mathematical principles of data security and to show how these principles apply to operating systems, database systems, and computer networks. The book is for students and professionals seeking an introduction to these principles. There are many references for those who would like to study specific topics further. Data security has evolved rapidly since 1975. We have seen exciting developments in cryptography: public-key encryption, digital signatures, the Data Encryption Standard (DES), key safeguarding schemes, and key distribution protocols. We have developed techniques for verifying that programs do not leak confidential data, or transmit classified data to users with lower security clearances. We have found new controls for protecting data in statistical databases--and new methods of attacking these databases. We have come to a better understanding of the theoretical and practical limitations to security.
The Annotated C++ Reference Manual.
Functional Programming
Relational Databases and Knowledge Bases
Natural Language Processing in PROLOG
Genetic Algorithms in Search Optimization and Machine Learning
Smalltalk-80: The Language and Its Implementation.
From the Preface (See Front Matter for full Preface) Advances in the design and production of computer hardware have brought many more people into direct contact with computers. Similar advances in the design and production of computer software are required in order that this increased contact be as rewarding as possible. The Smalltalk-80 system is a result of a decade of research into creating computer software that is appropriate for producing highly functional and interactive contact with personal computer systems. This book is the first detailed account of the Smalltalk-80 system. It is divided into four major parts: Part One -- an overview of the concepts and syntax of the programming language. Part Two -- an annotated and illustrated specification of the system's functionality. Part Three -- an example of the design and implementation of a moderate-size application. Part Four -- a specification of the Smalltalk-80 virtual machine.
The Java Language Specification.
A Retargetable C Compiler: Design and Implementation
Objektorientierte Datenbanken: Konzepte, Modelle, Systeme.
Objektorientierte Datenbanken: Konzepte, Modelle, Systeme, 2. Auflage.
Introduction to Automata Theory, Languages and Computation.
An Introduction to Parallel Algorithms
Introduction to Expert Systems, 1st Edition.
Introduction to Expert Systems, 2nd Edition.
:The third edition of Peter Jackson's book, Introduction to Expert Systems, updates the technological base of expert systems research and embeds those results in the context of a wide variety of application areas. The earlier chapters take a more practical approach to the basic topics than the previous editions, while the later chapters introduce new topic areas, such as case-based reasoning, connectionist systems, and hybrid systems. Results in related areas, such as machine learning and reasoning with uncertainty are also accorded a thorough treatment.
Introduction to Expert Systems, 3rd Edition.
:The third edition of Peter Jackson's book, Introduction to Expert Systems, updates the technological base of expert systems research and embeds those results in the context of a wide variety of application areas. The earlier chapters take a more practical approach to the basic topics than the previous editions, while the later chapters introduce new topic areas, such as case-based reasoning, connectionist systems, and hybrid systems. Results in related areas, such as machine learning and reasoning with uncertainty are also accorded a thorough treatment.
The Practice of Programming
Object-Oriented Concepts, Databases, and Applications.
The Art of Computer Programming, Volume I: Fundamental Algorithms
The Art of Computer Programming, Volume II: Seminumerical Algorithms
The Art of Computer Programming, Volume III: Sorting and Searching
The Art of Computer Programming, Volume I: Fundamental Algorithms, 2nd Edition.
The Art of Computer Programming, Volume II: Seminumerical Algorithms, 2nd Edition
Inside the C++ Object Model
Inside the C++ Object Model focuses on the underlying mechanisms that support object-oriented programming within C++: constructor semantics, temporary generation, support for encapsulation, inheritance, and "the virtuals"-virtual functions and virtual inheritance. This book shows how your understanding the underlying implementation models can help you code more efficiently and with greater confidence. Lippman dispells the misinformation and myths about the overhead and complexity associated with C++, while pointing out areas in which costs and trade offs, sometimes hidden, do exist. He then explains how the various implementation models arose, points out areas in which they are likely to evolve, and why they are what they are. He covers the semantic implications of the C++ object model and how that model affects your programs.Highlights Explores the program behavior implicit in the C++ Object Model's support of object-oriented programming. Explains the basic implementation of the object-oriented features and the trade offs implicit in those features. Examines the impact on performance in terms of program transformation. Provides abundant program examples, diagrams, and performance measurements to relate object-oriented concepts to the underlying object model.If you are a C++ programmer who desires a fuller understanding of what is going on "under the hood," then Inside the C++ Object Model is for you!
Introduction to Linear and Nonlinear Programming.
Design of Relational Databases
Tcl and the Tk Toolkit
From the Book:Tcl was born of frustration. In the early 1980s my students and I developed a number of interactive tools at the University of California at Berkeley, mostly for integrated circuit design, and we found ourselves spending a lot of time building bad command languages. Each tool needed to have a command language of some sort, but our main interest was in the tool rather than its command language. We spent as little time as possible on the command language and always ended up with a language that was weak and quirky. Furthermore, the command language for one tool was never quite right for the next tool, so we ended up building a new bad command language for each tool. This became increasingly frustrating.In the fall of 1987 it occurred to me that the solution was to build a reusable command language. If a general-purpose scripting language could be built as a C library package, then perhaps it could be reused for many different purposes in many different applications. Of course, the language would need to be extensible so that each application could add its own specific features to the core provided by the library. In the spring of 1988 I decided to implement such a language, and the result was Tcl. Tk was also born of frustration. The basic idea for Tk arose in response to Apple's announcement of HyperCard in the fall of 1987. HyperCard generated tremendous excitement because of the power of the system and the way in which it allowed many different interactive elements to be scripted and work together. However, I was discouraged. The HyperCard system had obviously taken a large development effort, and it seemed unlikely to me that a small group such as a university researchproject could ever mount such a massive effort. This suggested that we would not be able to participate in the development of new forms of interactive software in the future.I concluded that the only hope for us was a component approach. Rather than building a new application as a self-contained monolith with hundreds of thousands of lines of code, we needed to find a way to divide applications into many smaller reusable components. Ideally each component would be small enough to be implemented by a small group, and interesting applications could be created by assembling components. In this environment it should be possible to create an exciting new application by developing one new component and then combining it with existing components.The component-based approach requires a powerful and flexible "glue"for assembling the components, and it occurred to me that perhaps a shared scripting language could provide that glue. Out of this thinking grew Tk, an X11 toolkit based on Tcl. Tk allows components to be either individual user-interface controls or entire applications; in either case components can be developed independently and Tcl can be used to assemble the components and communicate between them.I started writing Tcl and Tk as a hobby in my spare time. As other people began to use the systems I found myself spending more and more time on them, to the point where today they occupy almost all of my waking hours and many of my sleeping ones.Tcl and Tk have succeeded beyond my wildest dreams. The Tcl/Tk developer community now numbers in the tens of thousands and there are thousands of Tcl applications in existence or under development. The application areas for Tcl and Tk cover virtually the entire spectrum of graphical and engineering applications, including computer-aided design, software development, testing, instrument control, scientific visualization, and multimedia. Tcl is used by itself in many applications, and Tcl and Tk are used together in many others. Tcl and Tk are being used by hundreds of companies, large and small, as well as universities and research laboratories.One benefit that came as a surprise to me is that it is possible to create interesting graphical user interfaces (GUIs) entirely as Tcl scripts. I had always assumed that every Tcl application would contain some new C code that implements new Tcl commands, plus some Tcl scripts that combine the new commands with the built-in facilities provided by Tcl. However, once a simple Tcl/Tk application called wish became available, many people began creating user interfaces by writing Tcl scripts for it, without writing any C code at all! It turned out that the Tcl and Tk commands provide a high-level interface to GUI programming that hides many of the details faced by a C programmer. As a result, it is much easier to learn how to use wish than a C-based toolkit, and user interfaces can be written with much less code. Most Tcl/Tk users never write any C code at all and most of the Tcl/Tk applications consist solely of Tcl scripts.This book is intended as an introduction to Tcl and Tk for programmers who plan to write or modify Tcl/Tk applications. I assume that readers have programmed in C and have at least passing familiarity with a shell such as sh or csh or ksh. I also assume that readers have used the X Window System and are familiar with basic ideas such as using the mouse, resizing windows, etc. No prior experience with Tcl or Tk is needed in order to read this book, and you need not have written X applications using other toolkits such as Motif.The book is organized so that you can learn Tcl without learning Tk if you wish. Also, the discussion of how to write Tcl scripts is separate from the discussion of how to use the C library interfaces provided by Tcl and Tk. The first two parts of the book describe Tcl and Tk at the level of writing scripts, and the last two parts describe the C interfaces for Tcl and Tk; if you are like the majority of Tcl/Tk users who only write scripts, you can stop after reading the first two parts.In spite of my best efforts, I'm sure that there are errors in this edition of the book. I'm interested in hearing about any problems that you encounter, whether they are typos, formatting errors, sections or ideas that are hard to understand, or bugs in the examples. I'll attempt to correct the problems in future printings of the book. The best way to report problems is with electronic mail sent to tclbookbugs@cs.berkeley.edu.Many people have helped in the creation of this book. First and foremost I would like to thank Brian Kernighan, who reviewed several drafts of the manuscript with almost terrifying thoroughness and uncovered numerous problems both large and small. I am also grateful for the detailed comments provided by the other Addison-Wesley technical reviewers: Richard Blevins, Gerard Holzmann, Curt Horkey, Ron Hutchins, Stephen Johnson, Oliver Jones, David Korn, Bill Leggett, Don Libes, Kent Margraf, Stuart McRobert, David Richardson, Alexei Rodrigues, Gerald Rosenberg, John Slater, and Win Treese. Thanks also to Bob Sproull, who read the next-to-last draft from cover to cover and provided countless bug fixes and suggestions.I made early drafts of the manuscript available to the Tcl/Tk community via the Internet and received countless comments and suggestions from all over the world in return. I'm afraid that I didn't keep careful enough records to acknowledge all the people who contributed in this way, but the list of contributors includes at least the following people: Marvin Aguero, Miriam Amos Nihart, Jim Anderson, Frederik Anheuser, Jeff Blaine, John Boller, David Boyce, Terry Brannon, Richard Campbell, J. Cazander, Wen Chen, Richard Cheung, Peter Chubb, De Clarke, Peter Collinson, Peter Costantinidis, Alistair Crooks, Peter Davies, Tal Dayan, Akim Demaille, Mark Diekhans, Matthew Dillon, Tuan Doan, Tony Duarte, Paul DuBois, Anton Eliens, Marc R. Ewing, Luis Fernandes, Martin Forssen, Ben Fried, Matteo Frigo, Andrej Gabara, Steve Gaede, Sanjay Ghemawat, Bob Gibson, Michael Halle, Jun Hamano, Stephen Hansen, Brian Harrison, Marti Hearst, Fergus Henderson, Kevin Hendrix, David Herron, Patrick Hertel, Carsten Heyl, Leszek Holenderski, Jamie Honan, Rob W.W. Hooft, Nick Hounsome, Christopher Hylands, Jonathan Jowett, Poul-Henning Kamp, Karen L. Karavanic, Sunil Khatri, Vivek Khera, Jon Knight, Roger Knopf, Ramkumar Krishnan, Dave Kristol, Peter LaBelle, Tor-Erik Larsen, Tom Legrady, Will E. Leland, Kim Lester, Joshua Levy, Don Libes, Oscar Linares, David C.P. Linden, Toumas J. Lukka, Steve Lord, Steve Lumetta, Earlin Lutz, David J. Mackenzie, B.G. Mahesh, John Maline, Graham Mark, Stuart McRobert, George Moon, Michael Morris, Russell Nelson, Dale K. Newby, Richard Newton, Peter Nguyen, David Nichols, Marty Olevitch, Rita Ousterhout, John Pierce, Stephen Pietrowicz, Anna Pluzhnikov, Nico Poppelier, M.V.S. Ramanath, Cary D. Renzema, Mark Roseman, Samir Tiongson Saxena, Jay Schmidgall, Dan M. Serachitopol, Hume Smith, Frank Stajano, Larry Streepy, John E. Stump, Michael Sullivan, Holger Teutsch, Bennett E. Todd, Glenn Trewitt, D.A. Vaughan-Pope, Richard Vieregge, Larry W. Virden, David Waitzman, Matt Wartell, Glenn Waters, Wally Wedel, Juergen Weigert, Mark Weiser, Brent Welch, Alex Woo, Su-Lin Wu, Kawata Yasuro, Chut Ngeow Yee, Richard Yen, Stephen Ching-SingYen, and Mike Young.Many many people have made significant contributions to the development of Tcl and Tk. Without all of their efforts there would have been nothing of interest to write about in this book. Although I cannot hope to acknowledge all the people who helped to make Tcl and Tk what they are today, I would like to thank the following people specially: Don Libes, for writing the first widely used Tcl application; Mark Diekhans and Karl Lehenbauer, for TclX; Alastair Fyfe, for supporting the early development of Tcl; Mary Ann May-Pumphrey, for developing the original Tcl test suite; George Howlett, Michael McLennan, and Sani Nassif, for the BLT extensions; Kevin Kenny, for showing that Tcl can be used to communicate with almost any imaginable program; Joel Bartlett, for many challenging conversations and for inspiring Tk's canvas widget with his ezd program; Larry Rowe, for developing Tcl-DP and for providing general advice and support; Sven Delmas, for developing the XF application builder based on Tk; and Andrew Payne, for the widget tour and for meritorious Tcl evangelism.Several companies have provided financial support for the development of Tcl and Tk, including Digital Equipment Corporation, Hewlett-Packard Corporation, Sun Microsystems, and Computerized Processes Unlimited. I am particularly grateful to Digital's Western Research Laboratory and its director, Richard Swan, for providing me with a one-day-per-week hideaway where I could go to gather my thoughts and work on Tcl and Tk.Terry Lessard-Smith and Bob Miller have provided fabulous administrative support for this and all my other projects. I don't know how I would get anything done without them.Finally, I owe a special debt to my colleague and friend Dave Patterson, whose humor and sage advice have inspired and shaped much of my professional career, and to my wife Rita and daughters Kay and Amy, who have tolerated my workaholic tendencies with more cheer and affection than I deserve. John Ousterhout Berkeley, California February, 1994
Functional Programming and Parallel Graph Rewriting
Web Caching and Replication
Web caching and replication provides essential material based on the extensive real-world experience of two experts from AT & T Labs. This comprehensive examination of caching, replication, and load-balacing practices for the Web brings together information from and for the commercial world, including real-life products; technical standards communities, such as IETF and W3C; and academic research.
Mehrrechner-Datenbanksysteme - Grundlagen der verteilten und parallelen Datenbankverarbeitung.
Advanced Prolog: Techniques and Examples
Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer.
The Design and Analysis of Spatial Data Structures
Temporal Deductive Databases.
The Historical Relational Data Model (HRDM) Revisited.
On the Completeness of Query Languages for Grouped and Ungrouped Historical Data Models.
A Temporal Model and Query Language for EER Databases.
The Time Index and the Monotonic B+-tree.
Ben-Zvi's Pioneering Work in Relational Temporal Databases.
Temporal Databases: A Prelude to Parametric Data.
Object and Spreadsheet Histories.
Differential Query Processing in Transaction-Time Databases.
Indexing Techniques for Historical Databases.
Stream Processing: Temporal Query Processing and Optimization.
Transaction-Time Databases.
The Interval-extended Relational Model and Its Applications to Valid-time Databases.
temporal Reasoning.
Temporal Extensions to the Relational Model and SQL.
HSQL: A Historical Query Language.
Join Processing and Optimization in Temporal Relational Databases.
A Temporal Data Model Based on Time Sequences.
An Overview of TQuel.
A Generalized relational Framework for Modeling Temporal Data.
Applications of temporal Databases to Knowledge-based Simulations.
Integrating Temporal Data in a Heterogeneous Environment.
A Uniform Model for Temporal and Versioned Object-oriented Databases.
Relational Database Theory
Conceptual Database Design: An Entity-Relationship Approach.
Object-Oriented Design with Applications
Fundamentals of Database Systems
Fundamentals of Database Systems, 2nd Edition.
:Fundamentals of Database Systems combines clear explanations of theory and design, broad coverage of models and real systems, and excellent examples with up-to-date introductions to modern database technologies. This edition is completely revised and updated, and reflects the latest trends in technological and application development. Professors Elmasri and Navathe focus on the relational model and include coverage of recent object-oriented developments. They also address advanced modeling and system enhancements in the areas of active databases, temporal and spatial databases, and multimedia information systems. This edition also surveys the latest application areas of data warehousing, data mining, web databases, digital libraries, GIS, and genome databases. New to the Third Edition Reorganized material on data modeling to clearly separate entity relationship modeling, extended entity relationship modeling, and object-oriented modeling Expanded coverage of the object-oriented and object/relational approach to data management, including ODMG and SQL3 Uses examples from real database systems including OracleTM and Microsoft AccessAE Includes discussion of decision support applications of data warehousing and data mining, as well as emerging technologies of web databases, multimedia, and mobile databases Covers advanced modeling in the areas of active, temporal, and spatial databases Provides coverage of issues of physical database tuning Discusses current database application areas of GIS, genome, and digital libraries
Crafting a Compiler with C
Introduction to Parallel Computing.
Computing with Logic: Logic Programming with Prolog
Temporal Databases: Theory, Design, and Implementation
Automaten, Sprachen und Maschinen für Anwender
Rechnerorganisation
Rechnerorganisation, 2. Auflage
Computer-Grafik
Computer-Grafik, 2. Auflage
Planen: Einführung in die Planerstellungsmethoden der künstlichen Intelligenz
Kryptologie
Theoretische Grundlagen relationaler Datenbanksysteme
Rechnernetzwerksystemarchitekturen und Datenkommunikation
Datenbankmaschinen: Performanz durch Parallelität
Parallele Transaktionen in Datenbanksystemen
Syntaxanalyse, 3. Auflage
C
Algorithmen und Datenstrukturen
Algorithmen und Datenstrukturen, 2. Auflage
Künstliche Intelligenz: Überblick und Grundlagen; grundlegende Konzepte und Methoden zur Realisierung von Systemen der künstlichen Intelligenz
Geo-Datenbanksysteme: Eine Speicher- und Zugriffsarchitektur
Logik für Informatiker
Logik für Informatiker, 2. Auflage
Logik für Informatiker, 3. Auflage
Softwaretechnologie: Eine Einführung, 3. Auflage
Softwaretechnologie: Eine Einführung, 4. Auflage
Mikroprozessoren: vom Bauteil zur Anwendung, 2. Auflage
Objektorientierte Schemaentwicklung: Ein kategorialer Ansatz für Datenbanken und Programmierung
Datenbanksysteme I
Datenbanksysteme II
Datenbanksysteme I, 2. Auflage
Datenbanksysteme I, 3. Auflage
Compilerbau I: Analyse
Compilerbau II: Synthese und Optimierung
Betriebssysteme: Parallele Prozesse, 3. Auflage
Compilerbau I: Analyse, 2. Auflage
Information Retrieval.
Building an Optimizing Compiler.
Introduction to Knowledge Base Systems
Debuggers for Programming Languages.
Software Pipelining.
Compilation for Distributed Memory Architectures.
Dynamic Compilation.
Register Allocation.
Instruction Scheduling.
Data Flow Testing.
Profile-Guided Compiler Optimizations.
Data Flow Analysis.
Optimizations for Object-Oriented Languages.
Program Slicing.
Automatic Generation of Code Optimizers from Formal Specifications.
Introduction to Operational Semantics.
Architecture Description Languages for Retargetable Compilation.
Retargetable Very Long Instuction Word Compiler Framework for Digital Signal Processors.
Dependence Analysis and Parallelizing Transformations.
Automatic Data Distribution.
Instruction Selection Using Tree Parsing.
Scalar Compiler Optimizations on the Static Single Assignment Form and the Flow Graph.
Type Systems in Programming Languages.
Compiling Safe Mobile Code.
Shape Analysis and Applications.
A Comparative Case Study of Distributed Network Architectures for Different Automotive Applications.
LonWorks/EIA-709 Networks EIA 709 Protocol (Lon Talk).
Distributed Components in Microsoft Platforms - Technology Overview.
The Quest for Real-Time Behavior in Ethernet.
CORBA in Manufacturing - Technology Overview.
IPv6, IPSec, and VPNs.
Overview and Classification of IP Routing Protocols - IP Routing: Interior and Exterior Routing Protocols.
Java Technology and Industrial Applications.
Internal Architecture and Features of Real-Time Embedded Operation Systems.
Middleware.
Network On-Chip Design for Gigascale Systems-on-Chip.
Principles and Features of PROFINET.
Knowledge Connect - An Approach for an Industrial IT Service Tool.
Intelligent Space and Mobile Robots.
Software for Wireless Sensor Networks.
Web-based Enterprise Computing Development using J2EE.
Holonic Manufacturing Systems: A Technical Overview.
The GRAFCET Specification Language.
The IDA Standard.
Embedded Software in the SoC World. The Concept of HdS in View of the HW and SW Design Challenge.
Platform-Based and Derivative Design.
Internet Programming Languages.
FOUNDATION Fieldbus: History and Features.
MPLS - Multiprotocol Label Switching.
Operating Principles and Features of CAN Networks.
Robot Tactile Sensing.
Hardware/Software Interfaces Design for SoC.
Giving Robots a Sense of Smell.
Mail Transfer and File Transfer Protocol.
The Integrated Services Architecture and RSVP.
Collaborative (Agent-Based) Factory Automation.
Linking Factory Floor and the Internet.
Which Network for Which Application.
Internet Firewalls.
Interconnection of Wireline and Wireless Fieldbusses.
IEEE 1394 for Factory Automation.
Intelligent Sensors: Analysis and Design.
Securtity in Automation Networks.
Integration Technologies of Field Devices in Distributed Control and Engineering Systems.
The Standard Message Specification for Industrial Automation Systems -ISO 9506 (MMS).
RTP, RTCP, and RTSP - Internet Protocols for Real-Time Multimedia Communication.
Smart Power Systems Rely on Standards for Information Models and Messaging - IEC 61850.
The Fundamentals of Web Services.
Open System Architecture for Controls within Automation Systems (OSACA).
Programming Web Services with .net and java.
A Survey of Congestion and QoS Control Mechanisms for the Internet.
Languages for Embedded Systems.
Switched Ethernet in Automation Networking.
Enterprise-Manufacturing Data Exchange Using XML.
Programming with the IEC 61131-3 Languages and the MatPLC.
The Dynamic Host Configuration Protocol.
Security in Embedded Systems.
The Internet Protocol.
PROFIsafe - Safety Technology with PROFIBUS.
ARP - Address Resolution Protocol.
XML for the Exchange of Automation Project Information.
The WORLDFIP Fieldbus.
IP-Mobility for Cellular and Wireless Networks.
Unified Modeling Language: The Industry Standard for Software Development.
A Survey on Self-Organizing Wireless Sensor Networks.
Web Servers, Clients, and Browsers.
The Hypertext Transfer Protocol and Uniform Resource Identifier.
Introduction to Multisensor Data Fusion.
Models of Computation for Embedded Systems.
Industrial IT-Based Network Management.
Hardware-level Design Languages.
Integration Between Production and Business Systems.
From Holonic Control to Virtual Enterprises: The Multi-Agent Approach.
Acheiving Reconfigurability of Automation Systems by Using the New International Standard IEC 61499: A Developer's View.
IP-QoS: Scalable and Flexible Quality-of-Service with Differentiated Services.
Real-Time Systems.
Wireless LAN Technology for the Factory Floor.
Internet Still Image and Video Formats.
Principles of Lower-Layer Protocols for Data Communications.
Network Management: Basic Notions and Frameworks.
Multicast.
HTTP Digest Authentication - Theory and Practice.
Web Services for Integrated Automation Systems - Challenges, Solutions and Future.
Verification Languages.
Mobile IP Routing.
Remote Monitoring and Control over the Internet.
Power Aware Embedded Computing.
Robot Vision.
Bluetooth.
PROFIBUS - Open Solutions for the World of Automation.
The Fundamentals of the Quality of Service.
Ad Hoc Networks.
Applications of Haptics in Design and Manufacturing.
Implementation af a Virtual Factory Communication System using the Manufacturing Message Specification Standard.
Ultrasonic Sensors in Robotics.
Microsoft's .NET.
Multiagent-based Architecture for Plant Automation.
Introduction to e-Manufacturing.
Time-Triggered Communication Networks.
Internet-Based Telemanipulation.
Network Security and Secure Applications.
Simple Network Management Protocol SNMP.
TCP/IP Architecture, Protocols, and Services.
OPC - Openness, Productivity, and Connectivity.
Design of Embedded Systems.
A Smart Transducer Interface Standard for Sensors and Actuators.
LonWorksTM over IP.
The Transmission Control Protocol.
Multidimensional Databases.
The User Datagram protocol.
Networked Control Systems Overview.
System-on-Chip and Network-on-Chip Design.
Wireless Local Area Networks and Wireless Personal Area Networks (WLANs and WPANs).
Open Controller Enabled by an Advanced Real-Time Network (OCEAN).
IT Security for Automation Systems.
The JEVIS Service Platform - Distributed Energy Data Acquisition and Management.
An Introductory Survey of Networked Embedded Systems.
Thread Management for Shared-Memory Multiprocessors.
Parallelizing Compilers.
Renderman®: An Interface for Image Synthesis.
Object-Oriented Databases.
Network and Internet Security.
Concurrent/Distributed Programming Paradigm.
Parallel Algorithms.
Formal Models and the Specification Process.
User Interface Design Activities.
Virtual Reality.
Memory Systems.
Type Systems.
Computational Fluid Dynamics.
Rules in Data-Based Systems.
Combinatorial Optimization.
Distributed and Multiprocessor Scheduling.
Computational Reacting Flow.
Logic Programming and Constraint Logic Programming.
Pattern Matching and Text Compression Algorithms.
Process and Device Scheduling.
Explanation-Based Learning.
Planning and Scheduling.
Traditional Software Design.
Virtual Memory.
Virtual memory is the simulation of a storage space so large that programmers do not need to reprogram or recompile their works when the capacity of a local memory or the configuration of a network changes. The name, borrowed from optics, recalls the virtual images formed by mirrors and lenses--images that are not there but behave as if they are. The designers of the Atlas Computer at the University of Manchester invented virtual memory in the 1950s to eliminate a looming programming problem: planning and scheduling data transfers between main and secondary memory and recompiling programs for each change of size of main memory. Virtual memory is even more useful in the computers of the 1990s, which have more things to hide-on-chip caches, separate RAM chips, local disk storage, network file servers (q.v.), large numbers of separately compiled program modules, other computers on the local bus or local network, or the Internet. The story of virtual memory from then to now is a story about machines helping programmers solve problems in storage allocation, protection of information, sharing and reuse of objects, and linking of program components. Virtual memory, common in all computers and operating systems from the smallest microprocessor to the largest supercomputer, is now invading the Internet.
Algebraic Algorithms.
Distributed File Systems and Distributed Memory.
Real-Time and Embedded Systems.
Output Devices and Techniques.
Interactive Techniques.
Advanced Geometric Modeling.
Digital Logic.
Malicious Software and Hacking.
What Is an Operating System?
Data Structures.
Mainstream Rendering Techniques.
Qualitative Reasoning.
Genetic Algorithms.
Concurrency Control and Recovery.
Verification and Validation.
Software Qualities and Principles.
Functional Programming Languages.
The Organizational Context of Development and Use.
The Object-Oriented Language Paradigm.
Overview of Three-Dimensional Graphics.
International User-Interface Standardization.
Busses.
Parallel Architectures.
Interactive Software Technology.
Computer Vision.
Query Optimization.
Input Devices and Techniques.
Database Security and Privacy.
Formal Models and Computability.
The Imperative Language Paradigm.
Ethical Issues for Computer Scientists and Engineers.
Neural Networks.
We present an overview of current research on artificial neural networks, emphasizing a statistical perspective. We view neural networks as parameterized graphs that make probabilistic assumptions about data, and view learning algorithms as methods for finding parameter values that look probable in the light of the data. We discuss basic issues in representation and learning, and treat some of the practical issues that arise in fitting networks to data. We also discuss links between neural networks and the general formalism of graphical models.
Digital Computer Architecture.
Computational Ocean Modeling.
Volume Visualization.
Graph and Network Algorithms.
Task Analysis and the Design of Functionality.
Computational Biology.
Search.
Case Study in Algorithms: VLSI Layout.
Protection (Security) Models and Policy.
Computational Geometry.
Robotics.
Compilers and Interpreters.
Complexity Theory.
Logic-Based Deductive Reasoning.
Computer Animation.
Secondary Storage and File Systems.
The SQL Language: A Case Study.
Randomized Algorithms.
Overview of Distributed Operating Systems.
Interface Software Technology.
Security and Privacy Issues in Computer and Communication Systems.
Computational Structural Mechanics.
Distributed and Parallel Database Systems.
Routing Protocols.
Development Strategies and Project Management.
Internetworking.
Software Tools and Environments.
Geometric Primitives.
Access Methods.
Testing: Principles and Practice.
Online Support Systems: Tutorials, Documentation, and Help.
Computational Electromagnetics.
Scientific Visualization.
Data Models.
Network Organization and Topologies.
Computer Science and Engineering: The Discipline and Its Impact.
Process Synchronization and Interprocess Communications.
Usability Engineering.
:Written by the author of the best-selling HyperText & HyperMedia, this book provides an excellent guide to the methods of usability engineering. Special features: emphasizes cost-effective methods that will help developers improve their user interfaces immediately, shows you how to avoid the four most frequently listed reasons for delay in software projects, provides step-by-step information about which methods to use at various stages during the development life cycle, and offers information on the unique issues relating to informational usability. You do not need to have previous knowledge of usability to implement the methods provided, yet all of the latest research is covered.
Knowledge-Based Systems for Natural Language Processing.
Run Time Environments and Memory Management.
Database Performance Measurement.
Graphical Models for Probabilistic and Causal Reasoning.
Foundational Calculi for Programming Languages.
Decision Trees and Instance-Based Classifiers.
Basic Techniques for Design and Analysis of Algorithms.
Text Databases and Information Retrieval.
The Human Factor in Programming and Software Development.
Authentication, Access Controls, and Intrusion Detection.
Programming Language Semantics.
Like English, French, and other "natural" languages, a programming language possesses both a syntax (grammatical laws that define the well-formed sentences) and a semantics (rules for giving meaning to programming language is a simple enough "artificial" language that precise definitions can be formulated for its syntax and semantics. The benefits of such precise definitions are: (1) the definitions standardize the programming language, so that implementors and users can agree on how the language bahaves; (2) the definitions can be analyzed for correctness and effciency properties; and (3) they can be used as input to automated prototyping tools like complier generators.
Understanding Spoken Language.
Tuning Database Design for High Performance.
Software Support Challenges for Heterogeneous Computing.
Software Process Models.
Geometry-Grid Generation.
High-Speed Computer Arithmetic.
Sampling, Reconstruction, and Antialiasing.
Multimedia.
The Industrial Information Technology Handbook
Design of Compilers - Techniques of Programming Language Translation
Handbook of Applied Cryptography
The Computer Science and Engineering Handbook
The Compiler Design Handbook: Optimizations and Machine Code Generation
Fundamentals of Computer Algorithms.
The Theory of Relational Databases.
A Logical Language for Data and Knowledge Bases.
The Theory of Database Concurrency Control
Introduction to Compiler Construction
Data Compression: Methods and Theory.
Principles of Database Systems, 1st Edition
Principles of Database Systems, 2nd Edition
Principles of Database and Knowledge-Base Systems, Volume I
Principles of Database and Knowledge-Base Systems, Volume II
Algorithmic Studies in Mass Storage Systems.
An Introduction to Unification-Based Approaches to Grammar
Word Order and Constituent Structure in German
Nonmonotonic Reasoning: An Overview
Specification Language.
An Introduction to Pascal-Plus.
Algorithms for Parallel Computers.
Concurrent Pascal - An Appraisal.
A Model for Communicating Sequential Processes.
Modules and Visibility in the Ada Programming Language.
Information Systems: Modelling, Sequencing and Transformations.
Specification and design of an information system conventionally starts from consideration of the system function. This paper argues that consideration may more properly be given first to the system as a model of the reality with which it is concerned, the function being subsequently superimposed on the model. The form of model proposed is a network of sequential processes communicating by serial data streams. Such a model permits a clear representation of change or activity over time, and it also prevents over-specification of sequencing by separating problem-oriented from solution-oriented sequencing constraints. The model, however, cannot be efficiently executed on uniprocessor hardware without transformation. Some relevant kinds of transformation are mentioned, and the derivation, by means of them, of conventional information system configurations from the proposed model.
A Structured Operating System.
Languages for Parallel Computers.
Parallel Processing in Ada.
A Structured Compiler.
Compiling with Continuations
This book shows how continuation-passing style is used as an intermediate representation to perform optimizations and program transformations. Continuations can be used to compile most programming languages. The method is illustrated in a compiler for the programming language Standard ML. Prior knowledge of ML, however, is not necessary, as the author carefully explains each concept as it arises. This is the first book to show how concepts from the theory of programming languages can be applied to the production of practical optimizing compilers for modern languages like ML. All the details of compiling are covered, including the interface to a runtime system and garbage collector.
Modern Compiler Implementation in Java: Basic Techniques
:This textbook describes all phases of a modern compiler. It includes good coverage of current techniques in code generation and register allocation, as well as functional and object-oriented languages, that is missing from most books. In a concise way, the author describes the most accepted and successful techniques, rather than giving an exhaustive catalog of every possible variant. Detailed descriptions of the interfaces between modules of a compiler are illustrated with actual Java classes. A unique feature of the book is a well designed compiler implementation project in Java, including front-end and high-tech back-end phases, giving a practical example of Java programming for students which will also interest computer professionals.
Modern Compiler Implementation in C: Basic Techniques
:This textbook describes all phases of a modern compiler. It includes good coverage of current techniques in code generation and register allocation, as well as functional and object-oriented languages, that is missing from most books. In a concise way, the author describes the most accepted and successful techniques, rather than giving an exhaustive catalog of every possible variant. Detailed descriptions of the interfaces between modules of a compiler are illustrated with actual C header files. A unique feature of the book is a well designed compiler implementation project in C, including front-end and high-tech back-end phases, useful for undergraduate and graduate students as well as computer professionals needing a reference on compiler implementation.
Modern Compiler Implementation in ML: Basic Techniques
:This textbook describes all phases of a modern compiler. It includes good coverage of current techniques in code generation and register allocation, as well as functional and object-oriented languages, that is missing from most books. In a concise way, the author describes the most accepted and successful techniques, rather than giving an exhaustive catalog of every possible variant. Detailed descriptions of the interfaces between modules of a compiler are illustrated with actual ML signatures. A unique feature of the book is a well designed compiler implementation project in ML, including front-end and high-tech back-end phases, useful for undergraduate and graduate students as well as computer professionals needing a reference on compiler implementation.
Modern Compiler Implementation in Java
Modern Compiler Implementation in C
Modern Compiler Implementation in ML
Modern Compiler Implementation in Java, 2nd edition.
An Introduction to Mathematical Taxonomy
Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids
Algorithms on Strings, Trees, and Sequences - Computer Science and Computational Biology
Introduction to Combinators and Lambda-Calculus.
On the Construction of Programs
LEDA: A Platform for Combinatorial and Geometric Computing.
Combinatorial and geometric computing is a core area of computer science (CS). In fact, most CS curricula contain a course in data structures and algorithms. The area deals with objects such as graphs, sequences, dictionaries, trees, shortest paths, flows, matchings, points, segments, lines, convex hulls, and Voronoi diagrams and forms the basis for application areas such as discrete optimization, scheduling, traffic control, CAD, and graphics. There is no standard library of the data structures and algorithms of combinatorial and geometric computing. This is in sharp contrast to many other areas of computing. There are, for example, packages in statistics (SPSS), numerical analysis (LINPACK, EISPACK), symbolic computation (MAPLE, MATHEMATICA), and linear programming (CPLEX).
Randomized Algorithms.
Numerical Recipes in C, 2nd Edition.
Data Refinement: Model-oriented Proof Theories and their Comparison
Concurrency Verification: Introduction to Compositional and Noncompositional Methods
Wiki - Planen, Einrichten, Verwalten
Technologiebewusstes Design von Web-Anwendungen.
Semantisches Web - Das Netz der Bedeutungen im Netz der Dokumente.
Sicherheit von Web-Anwendungen.
Betrieb und Wartung von Web-Anwendungen.
Architektur von Web-Anwendungen.
Entwicklungsprozess von Web-Anwendungen.
Implementierungstechnologien für Web-Anwendungen.
Requirements Engineering für Web-Anwendungen.
Usability von Web-Anwendungen.
Web Engineering - Die Disziplin zur systematischen Entwicklung von Web-Anwendungen.
Performanz von Web-Anwendungen.
Web-Projektmanagement.
Modellierung von Web-Anwendungen.
Testen von Web-Anwendungen.
Semistrukturierte Datenmodelle und XML.
Benchmarking von XML-Datenbanksystemen.
Indexstrukturen für XML.
Web-basiertes Lernen: Eine Übersicht über Stand und Entwicklungen.
Architektur von Web-Informationssystemen.
Web Services.
Speicherung von XML-Dokumenten.
Anfragen, Ändern und Transformieren von XML.
Kommerzielle Systeme zur Speicherung, Verwaltung und Anfrage von XML-Dokumenten.
Data-Warehouse-Einsatz zur Web-Zugriffsanalyse.
Datenintegration und Mediatoren.
XML Schema.
Suchmaschinen.
Web Caching.
Object-Oriented Reengineering Patterns
:The documentation is missing or obsolete, and the original developers have departed. Your team has limited understanding of the system, and unit tests are missing for many, if not all, of the components. When you fix a bug in one place, another bug pops up somewhere else in the system. Long rebuild times make any change difficult. All of these are signs of software that is close to the breaking point. Many systems can be upgraded or simply thrown away if they no longer serve their purpose. Legacy software, however, is crucial for operations and needs to be continually available and upgraded. How can you reduce the complexity of a legacy system sufficiently so that it can continue to be used and adapted at acceptable cost? Based on the authors' industrial experiences, this book is a guide on how to reverse engineer legacy systems to understand their problems, and then reengineer those systems to meet new demands. Patterns are used to clarify and explain the process of understanding large code bases, hence transforming them to meet new requirements. The key insight is that the right design and organization of your system is not something that can be evident from the initial requirements alone, but rather as a consequence of understanding how these requirements evolve.
Objektorientierte Datenbanksysteme. Ein Praktikum.
Digitale Bibliotheken - Informatik-Lösungen für globale Wissensmärkte
Objektrelationale und objektorientierte Datenbankkonzepts und -systeme
Mobile Datenbanken und Informationssysteme: Konzepte und Techniken
Web Engineering: Systematische Entwicklung von Web-Anwendungen
Linux Sicherheit: Security mit Open-Source-Software, Grundlagen und Praxis
Informationsintegration: Architekturen und Methoden zur Integration verteilter und heterogener Datenquellen.
Systemsoftware: Grundlagen moderner Betriebssysteme
Web & Datenbanken. Konzepte, Architekturen, Anwendungen
Datenbanken & Java - JDBC, SQLJ und ODMG
HTML & XHTML - die Sprachen des Web, Lehrgang und Referenz, 5. Auflage
SQL:1999 & SQL:2003 - Objektrelationales SQL, SQLJ & SQL/XML
Wissensbasierte Textverarbeitung: Schriftsatz und Typographie - Möglichkeiten einer intelligenteren Textverarbeitung
Anfrageverarbeitung in Komplexobjekt-Datenbanksystemen
A Formal Model for Lazy Implementations of a Prolog-Compatible Functional Language.
Garbage Collection in Prolog Interpreters.
Deduction Revision by Intelligent Backtracking.
Should Prolog be List or Record Oriented.
Finding Backtrack Points for Intelligent Backtracking.
POLER - Implementation of a POP-2-based PLANNER.
A Note on Micro-Planer.
An Interpreting Algorithm for Prolog Programs.
What the naive user wants from Prolog.
A Prolog Interpreter Working with Infinite Terms.
Exeter Prolog - some thoughts on Prolog design by a LISP user.
The Control of Searching and Backtracking in String Pattern Matching.
Efficient Implementation of Unification of Cyclic Structures.
How to Implement Prolog on a LISP Machine.
The 'Marseille Interpreter' - a Personal Perspective.
Prolog - a Panacea?
Integrating Prolog in the POPLOG Environment.
Although Prolog undoubtedly has its good points, there are some tasks (such as writing a screen editor or network interface controller) for which it is not the language of choice. The most natural computational concepts [2] for these tasks are hard to reconcile with Prolog's declarative nature. Just as there is a need for even the most committed Prolog programmer to use "conventional" languages for some tasks, so too is there a need for "logic" oriented components in conventional applications programs, such as CAD systems [7] and relational databases [5]. At Sussex, the problems of integrating logic with procedural programming are being addressed by two projects. One of these [4] involves a distributed ring of processors communicating by message passing. The other project is the POPLOG system, a mixed language AI programming environment which runs on conventional hardware. This paper describes the way in which we have integrated Prolog into POPLOG.
A Proposal for Distributed Programming in Logic.
Associative Evaluation of Prolog Programs.
The World's Shortest Prolog Interpreter?
Formal Vienna-Definition-Method Models of Prolog.
Logic Control with Logic.
Epilog: A Language for Extended Programming in Logic.
W-Grammars for Logic Programming.
The Taming of the Sleuth.
EPILOG: Re-interpreting and Extending Prolog for a Multiprocessor Environment.
System Simulation and Cooperative Problem-solving on a Prolog Basis.
On Prolog DBMS Connections: A Step Forward from Educe.
Grid Files for Efficient Prolog Clause Access.
A Source-to-Source Meta-Translation System for Database Query Languages - Implementation in Prolog.
A Flexible Prolog-Based Lexical Database System.
A Generalized Interface Between Prolog and Relational Databases.
A Prolog-Relational Database Interface.
TREQL (Thornton Research Easy Query Language): An Intelligent Front-End to a Relational Database.
Modular Commitment in Persistent Prolog.
Implementing Query Languages in Prolog.
On Transaction Processing, Knowledge-Based Systems and Databases.
An Object-Oriented Database for Storage and Analysis of Protein Structure Data.
The NU-Prolog Deductive Database System.
Software Configuration Management Using Prolog.
Guarded Default Databases: A Prototype Implementation.
An Interface from Prolog to a Binary Relational Database.
A Data-Driven Execution Mechanism for Transaction-Oriented Information Systems.
The Interaction Between BIM-Prolog and Relational Databases.
Recursive Query Processing: Fundamental Algorithms and the DedGin System.
The TRACE Club Expert System and Databases.
Benchmarking Prolog for Database Applications.
Abstract Interpretation of Declarative Languages
Statistical and Scientific Databases
Machine Learning, Neural and Statistical Classification
Pascal Implementation: The P4 Compiler and Interpreter
Classical Type Theory.
Unification Theory.
Normal Form Transformations.
Automated Deduction for Many-Valued Logics.
Resolution Theorem Proving.
Proof-Assistants Using Dependent Type Systems.
Solving Numerical Constraints.
The Automation of Proof by Mathematical Induction.
Reasoning in Expressive Description Logics.
Automated Reasoning in Geometry.
Model Checking.
Inductionless Induction.
The Early History of Automated Deduction.
The Inverse Method.
Equality Reasoning in Sequent-Based Calculi.
Rewriting.
Nonmonotonic Reasoning: Towards Efficient Calculi and Implementations.
Higher-Order Unification and Matching.
Resolution Decision Procedures.
Tableaux and Related Methods.
Model Elimination and Connection Tableau Procedures.
Paramodulation-Based Theorem Proving.
Computing Small Clause Normal Forms.
Encoding Two-Valued Nonclassical Logics in Classical Logic.
Logical Frameworks.
Term Indexing.
Preface.
Connections in Nonclassical Logics.
Combining Superposition, Sorts and Splitting.
Computer-Aided Database Design: the DATAID approach.
A Software Engineering Approach to Database Design: The Galileo Project.
A Tool for Modeling Dynamics in Conceptual Design.
GINCOD: A Graphical Tool for Conceptual Design of Data Base Applications.
The Logical Design in the DATAID Project: The EASYMAP System.
A Workbench for Conceptual Design in Galileo.
DATAID-D: Methodology for Distributed Database Design.
Dynamics in Logical Database Design.
Architecture of a Physical Design Tool for Relational DBMSs.
Important Issues in Database Design Methodologies and Tools.
Integrated Tools for Physical Database Design in CODASYL Environment.
Concepts, Implementation, and Applications of a Typed Logic Programming Language.
Combinatorial Problem Solving in Constraint Logic Programming with Cooperating Solvers.
The WAM - Definition and Compiler Correctness.
Using Constraint Logic Programming for Industrial Scheduling Problems.
Temporal Logic Programming Applied to Image Sequence Evaluation.
Goal-Directed Forward Chaining: Tuple-oriented Bottom-up Approach.
Logic Program Modules for Interoperable Information Systems.
Robot Control Systems as Contextual Logic Programs.
Logic Programming - Past or Future?
Polymorphic Feature Types.
Automatic Verification of Parallel Logic Programs: Termination.
Scheduling and Meta-Scheduling.
Efficient Object-Oriented Programming in Prolog.
A Generic Scheduling Framework developed in Prolog.
Dialogo: An Interactive Environment for Conceptual Design in Galileo.
Requirements Collection and Analysis.
INCOD-DTE: A System for Interative Conceptual Design of Data, Transactions and Events.
Views Integration.
Logical Design in Codasyl and Relational Environment.
A Separability-Based Method for Secondary Index Selection in Physical Database Design.
A Set of Integrated Tools for the Conceptual Design of Database Schemas and Transactions.
Views Conceptual Design.
Methodology and Tools for Data Base Design in the DATAID Project.
NLDA: A Natural Language Reasoning System for the Analysis of Data Base Requirements.
Physical Data Base Design for Codasyl DBMS.
Algorithms for Finding Patterns in Strings.
Logic Programming.
Functional Programming and Lambda Calculus.
Context-Free Languages.
Machine Models and Simulation.
The Complexity of Finite Functions.
Graph Rewriting: An Algebraic and Logic Approach.
Recursive Applicative Program Schemes.
Methods and Logics for Proving Programs.
Rewrite Systems.
Temporal and Modal Logic.
Semantic Domains.
A Catalog of Complexity Classes.
Elements of Relational Database Theory.
The goal of this paper is to provide a systematic and unifying introduction to relational database theory, including some of the recent developments in database logic programming. The first part of the presentation covers the two basic components of the relational data model: its specification component, that is the database scheme with dependencies, and its operational component, that is the relational algebra query language. The choice of basic constructs for specifying the semantically meaningful databases and for querying them is justified through an in-depth investigation of their properties. Some important research themes are reviewed in this context: the analysis of the hypergraph syntax of a database scheme and the extensions of the query language using deduction or universal relation assumptions. The subsequent parts of the presentation are structured around the two fundamental concepts illustrated in the first part, dependencies and queries. The main themes of dependency theory are implication problems and applications to database scheme design. Queries are classified in a variety of ways, with emphasis on the connections between the expressibility of query languages, finite model theory and logic programming. The theory of queries is very much related to research on database logic programs, which are an elegant formalism for the study of the principles of knowledge base systems. The optimization of such programs involves both techniques developed for the relational data model and new methods for analyzing recursive definitions. The exposition closes with a discussion of how relational database theory deals with the problems of complex objects, incomplete information and database updates.
Parallel Algorithms for Shared-Memory Machines.
Logics of Programs.
None Available
Distributed Computing: Models and Methods.
Graph Algorithms.
VLSI Theory.
Algorithms in Number Theory.
Kolmogorov Complexity and its Applications.
Data Structures.
Operational and Algebraic Semantics of Concurrent Processes.
Type Systems for Programming Languages.
Denotational Semantics.
Finite Automata.
Communication Networks.
Cryptography.
Formal Language and Power Series.
Algorithmic Motion Planning in Robotics.
Machine-Independent Complexity Theory.
Algebraic Complexity Theory.
Automata on Infinite Objects.
General Purpose Parallel Architectures.
Average-Case Analysis of Algorithms and Data Structures.
Algebraic Specification.
Computational Geometry.
Suffix, prefix and maximal tree codes.
Fixed point characterization of weak monadic logic definable sets of trees.
Recognizing sets of labelled acyclic graphs.
Recognizable sets of unrooted trees.
Structural complexity of classes of tree languages.
Decidability of the inclusion in monoids generated by tree transformation classes.
Unification procedures in automated deduction methods based on matings: A survey.
Algebraic specification of action trees and recursive processes.
Trees and algebraic semantics.
Tree-adjoining grammars and lexicalized grammars.
Computing trees with graph rewriting systems with priorities.
Binary tree codes.
A monoid approach to tree automata.
A survey of tree transductions.
Rational and recognizable infinite tree sets.
Automata on infinite trees and rational control.
Interpretability and tree automata: A simple way to solve algorithmic problems on graphs closely related to trees.
Ambiguity and valuedness.
A short proof of the factorization forest theorem.
A theory of tree language varieties.
Computer-Aided Database Design: the DATAID approach.
Computability, Complexity, Logic (English translation of "Berechenbarkeit, Komplexität, Logik" from 1985)
Methodology and Tools for Data Base Design.
Data and Reality, 1st edition.
Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity
Handbook of Theoretical Computer Science, Volume B: Formal Models and Semantics
Tree Automata and Languages.
Handbook of Automated Reasoning (in 2 volumes)
Classification, Estimation and Pattern Recognition.
Méthodes de Programmation, 1st edition
Méthodes de Programmation, 3rd edition
LogOut - Warum Computer nichts im Klassenzimmer zu suchen haben und andere High-Tech-Ketzereien
Fractal Geometry of Nature.
Fractals, Chaos, Power Laws: Minutes From an Infinite Paradise.
Computers and Intractability: A Guide to the Theory of NP-Completeness.
Spatial Analysis and GIS.
Deterministic Translation Grammars
The Analysis of a Practical and Nearly Optimal Priorty Queue
Queueing Network Models of Multiprogramming
A Practical Formal Semantic Definition and Verification System for TYPED LISP
Linear Lists and Prorty Queues as Balanced Binary Trees
Computer Display of Curved Surfaces
The Application of Theorem Proving to Question-Answering Systems
Decidability Questions for Petri Nets
An understanding of the mathematical properties of Petri Nets is essential when one wishes to use Petri Nets as an abstract model for concurrent systems. The decidability of various problems which arise in this context is an important aspect of this question. The fact that these problems also arise in the context of other mathematical theories, such as commutative semigroups, closure under linear relations, Matrix Context-Free grammars, or Weak Counter Automata, provides further motivation. The Reachability Problem for Vector Addition Systems - whose decidability is still an open question - is of central importance. We show that a number of Petri Net problems are recursively equivalent to this problem. These include the Liveness Problem (e.g. can a system reach a deadlocked state?), the persistence problem (can a given transition ever be disabled by the firing of another transition?), and the membership and emptiness problems for certain classes of languages generated by Petri Nets. The power of the unrestricted Petri Net model is illustrated by various undecidable equivalence results. In particular, we show that the equality of Reachability Sets and the equivalence of two Petri Nets in terms of their language-generating capability are recursive undecidable. It is hoped that the constructions used to prove our results will shed some light on the source of the complexities of the unrestricted Petri Net model, and may eventually permit us to achieve an optimal balance between representational transparency and analytical power of the Petri Net model.
Control-Theoretic Formulation of Operating Systems Resource Management Policies
The Metanovel: Writing Stories by Computer
A Processor Design for the Efficient Implementation of APL
The Design and Construction of Flexible and Efficient Interactive Programming Systems
Reasoning from Incomplete Knowledge in a Procedural Deduction System
Axiomatic Proof Techniques for Parallel Programs
This thesis presents an axiomatic method for proving certain correctness properties of parallel programs. Axioms and inference rules for partial correctness are given for two parallel programming languages: The General Parallel Language and the Restricted Parallel Language. The General Language is flexible enough to represent most standard synchronizers (e.g. semaphores, events), so that programs using these synchronizers may be verified using the GPL deductive system. However, proofs for GPL programs are in general quite complex. The Restricted Language reduces this complexity by requiring shared variables to be protected by critical sections, so that only one process at a time has access to them. This discipline does not significantly reduce the power of the language, and it greatly simplifies the process of program verification. Although the axioms and inference rules are primarily intended for proofs of partial correctness, there are a number of other important properties of parallel programs. We give some practical techniques which use information obtained from a partial correctness proof to derive other correctness properties, including program termination, mutual exclusion, and freedom from deadlock. A number of examples of such proofs are given. Finally, the axioms and inference rules are shown to be consistent and complete (in a special sense) with respect to an interpretive model of parallel execution. Thus the deductive system gives an accurate description of program execution and is powerful enough to yield a proof of any true partial correctness formula.
Shellsort and Sorting Networks
Machine Perception of Three-Dimensional Solids
Source Language Debugging Tools
Quicksort
Queueing Models for Computer Systems with General Service Time Distributions
A Data Definition Facility for Programming Languages
Sketchpad, A Man-Machine Graphical Communication System
The Sketchpad system makes it possible for a man and a computer to converse rapidly through the medium of line drawings. Heretofore, most interaction between man and computers has been slowed down by the need to reduce all communication to written statements that can be typed; in the past, we have been writing letters to rather than conferring with our computers. For many types of communication, such as describing the shape of a mechanical part or the connections of an electrical circuit, typed statements can prove cumbersome. The Sketchpad system, by eliminating typed statements (except for legends) in favor of line drawings, opens up a new area of man-machine communication.
Automatic Verification of Programs with Complex Data Structures
Studies in Extensible Programming Languages
Automatic Generation of Assemblers
Understanding Goal-Based Stories
Predicate-Oriented Database Search Algorithms
Semantics For a Question-Answering System
The Hensel Lemma in Algebraic Manipulation
Cluster Analysis.
Objektorientierte Softwareentwicklung
Erfolgsschlüssel Objekttechnologie -- Managerführer zur Neuorganisation des Softwareprozesses
Logic for Computer Science: Foundations of Automatic Theorem Proving
An Introduction to Compiler Contruction
Artificial Intelligence Architectures for Composition and Performance Environment.
Dynamic Programming for Interactive Music Systems.
The Mechanization of Intelligence and the Human Aspects of Music.
Artificial Intelligence in Music Education: A Critical Review.
Music, Intelligence and Artificiality.
Regarding Music, Machines, Intelligence and the Brain: An Introduction to Music and AI.
Computer Analysis of Jazz Chord Sequence: Is Solar a Blues?
Musical Pattern Extraction and Similarity Assessment.
Interactive Music Systems in Ensemble Performance.
Symbolic AI versus Connectionism in Music Research.
On the Potential of Machine Learning for Music Research.
Musical Knowledge: What can Artificial Intelligence Bring to the Musician?
Artificial Intelligence and Music Education.
Readings in Music and Artificial Intelligence
Information Extraction from Free-Text Business Documents.
The objective of this chapter is an investigation of the applicability of information extraction techniques in real-world business applications dealing with textual data since business relevant data is mainly transmitted through free-text documents. In particular, we give an overview of the information extraction task, designing information extraction systems and some examples of existing information extraction systems applied in the financial, insurance and legal domains. Furthermore, we demonstrate the enormous indexing potential of lightweight linguistic text processing techniques applied in information extraction systems and other closely related fields of information technology which concern processing vast amounts of textual data.
Active Rules and Active Databases: Concepts and Application.
This chapter surveys the topic of active rules and active databases. We analyze the state of the art of active databases and active rules, their properties and applications. In particular, we describe the case of triggers following the SQL-1999 Standard Committee point of view. Then, we consider the case of dynamic constraints for which we use a temporal logic formalism. Finally, we discuss the applicability, limitations and partial solutions found when attempting to ensure the satisfaction of dynamic constraints.
Applying JAVA-Triggers for X-Link Management in the Industrial Framework.
This chapter focuses on referential link integrity problems. In the industrial context, the life cycle of a document plays a central role in describing the "steps out" of a product. Users realize some manipulations like creation, edition, suppression and querying under a multi-user environment, risking possible destruction or the alteration of the document's integrity. A classical impact is the infamous "Error 404: file not found." However, the user needs a notification alert mechanism to prevent and warrant the coherence of manipulations over all the life cycle processes of a product. The main objective of this chapter is to provide a generic relationship validation mechanism to remedy this shortcoming. We believe in the combination of some standard features of XML, specifically XLL specification as a support for integrity management and Java-Triggers approach as an alert method. This study, compared with actual approaches, proposes a solution based on active functionalities.
Novel Indexing Method of Relations Between Salient Objects.
Since the last decade, images have been integrated into several application domains such as GIS, medicine, etc. This integration necessitates new managing methods particularly in image retrieval. Queries should be formulated using different types of features such as low-level features of images (histograms, color distribution, etc.), spatial and temporal relations between salient objects, semantic features, etc. In this chapter, we propose a novel method for identifying and indexing several types of relations between salient objects. Spatial relations are used here to show how our method can provide high expressive power to relations in comparison to the traditional methods.
On the Computation of Recursion in Relational Databases.
A composite object represented as a directed graph is an important data structure which requires efficient support in CAD/CAM, CASE, office systems, software management, Web databases and document databases. It is cumbersome to handle such an object in relational database systems when it involves recursive relationships. In this chapter, we present a new encoding method to support the efficient computation of recursion. In addition, we devise a linear time algorithm to identify a sequence of reachable trees (w.r.t.) a directed acyclic graph (DAG), which covers all the edges of the graph. Together with the new encoding method, this algorithm enables us to compute recursion w.r.t, a DAG in time O(e), where e represents the number of edges of the DAG. More importantly, this method is especially suitable for a relational environment.
Building Signature-Trees on Path Signatures in Document Databases.
Java is a prevailing implementation platform for XML-based systems. Several high-quality in-memory implementations for the standardized XML-DOM API are available. However, persistency support has not been addressed. In this chapter, we discuss this problem and introduce PDOM (persistent DOM) to accommodate documents as permanent object sets. In addition, we propose a new indexing technique: path signatures to speed up the evaluation of path-oriented queries against document object sets, which is further enhanced by combining the technique of signature-trees with it to expedite scanning of signatures stored in a physical file.
Dealing with Relationship Cardinality Constraints in Relational Database Design.
Conceptual models are well-known tools to achieve a good design of information systems. Nevertheless, the understanding and use of all the constructs and constraints which are presented in such models are not an easy task and sometimes it is cause of loss of interest.In this chapter we have tried to study in depth and clarify the meaning of the features of conceptual models. The disagreements between main conceptual models, the confusion in the use of some of their constructs and some open problems in these models are shown. Another important topic treated in this chapter is the conceptual-to-logic schemata transformation process.Some solutions are presented in order to clarify the relationship construct and to extend the cardinality constraint concept in ternary relationships. How to preserve the cardinality constraint semantics in binary and ternary relationships for their implementation in a DBMS with active capabilities has also been developed.
Repairing and Querying Inconsistent Databases.
The integration of knowledge from multiple sources is an important aspect in several areas such as data warehousing, database integration, automated reasoning systems, active reactive databases and others. Thus a central topic in databases is the construction of integration systems, designed for retrieving and querying uniform data stored in multiple information sources. This chapter illustrates recent techniques for computing repairs as well as consistent answers over inconsistent databases. Often databases may be inconsistent with respect to a set of integrity constraints, that is, one or more integrity constraints are not satisfied. Most of the techniques for computing repairs and queries over inconsistent databases work for restricted cases and only recently there have been proposals to consider more general constraints. In this chapter we give an informal description of the main techniques proposed in the literature.
Unifying Access to Heterogeneous Document Databases Through Contextual Metadata.
Document databases available on the Internet carry massive information resources. To a person needing a piece of information on a specific domain, finding the piece, however, is often quite problematic even though there were a representative collection of databases available on the domain. The languages used in the content, the names of document types, their structures, the ways documents are organized and their retrieval techniques often vary in the databases. The databases containing legal information on the Internet offer a typical example. For finding relevant documents and for being able to interpret the content of the documents correctly, the user may need information about the context where the documents have been created. In this chapter we introduce a method for collecting contextual metadata and for representing the metadata to the users by graphical models. The solution is demonstrated by a case of retrieving information from distributed European legal databases.
Managing Document Taxonomies in Relational Databases.
This chapter addresses the challenge of applying relational database technologies to manage taxonomies, which are commonly used to classify documents, knowledge and websites into a hierarchy of topics. It first describes how denormalizing the data model can facilitate data retrieval from such topic hierarchies. It then shows how the typical data maintenance difficulties associated with denormalized data models can be solved using database triggers.
Database Management Issues in the Web Environment.
The focus of this chapter is on the progressive adaptation of database techniques to Web usage in a way quite similar to the evolution from integrated file management systems to database management systems. We review related and open issues, such as the semi-structured data and XML, integrity problem, query optimization problem, and integration issues in both the Web and Semantic Web environments. The representation of meta-information along with data opens up a new way to automatically process Web information due to the use of explicit semantic information. We hope that researchers will take into account traditional database techniques and how they can assist new Web technologies. In this sense, the amalgamation of Web and database technology appears to be very promising.
Interactive Indexing of Documents with a Multilingual Thesaurus.
With the growing significance of digital libraries and the Internet, more and more electronic texts become accessible to a wide and geographically disperse public. This requires adequate tools to facilitate indexing, storage and retrieval of documents written in different languages. We present a method for semi-automatic indexing of electronic documents and construction of a multilingual thesaurus, which can be used for query formulation and information retrieval. We use special dictionaries and user interaction in order to solve ambiguities and find adequate canonical terms in the language and an adequate abstract language-independent term. The abstract thesaurus is updated incrementally by new indexed documents and is used to search for documents using adequate terms.
Understanding Functional Dependency.
In explaining functional dependency to students, I have noticed in texts a mixture of two types of elements: intensional (or psychological or meaning) and extensional (patterns of repetition in the data). In this chapter I examine whether it is possible to consider functional dependency, in particular, in second and third normal forms, solely on an extensional basis. The Microsoft Access Analyzer utility seems to do so. I illustrate the mix of intensional and extensional elements in textbook definitions of functional dependency. I conclude that although in principle first, second and third normal form can be done solely by extensional means, in practice intensional considerations are indispensable. Finally, I discuss these questions with respect to the "higher order" normal forms, namely Boyce-Codd, fourth, fifth and Domain/Key normal form.
Keyword-Based Queries Over Web Databases.
In this chapter, we propose an approach to using keywords (as in a Web search engine) for querying databases over the Web. The approach is based on a Bayesian network model and provides a suitable alternative to the use of interfaces based on multiple forms with several fields. Two major steps are involved when querying a Web database using this approach. First, structured (database-like) queries are derived from a query composed only of the keywords specified by the user. Next, the structured queries are submitted to a Web database, and the retrieved results are presented to the user as ranked answers. To demonstrate the feasibility of the approach, a simple prototype Web search system based on the approach is presented. Experimental results obtained with this system indicate that the approach allows for accurately structuring the user queries and retrieving appropriate answers with minimum intervention from the user.
System of Information Retrieval in XML Documents.
This chapter introduces the process to retrieve units (or subdocuments) of relevant information from XML documents. For this, we use the Extensible Markup Language (XML) which is considered as a new standard for data representation and exchange on the Web. XML opens opportunities to develop a new generation of Information Retrieval System (IRS) to improve the interrogation process of document bases on the Web.Our work focuses instead on end-users who do not have expertise in the domain (like a majority of the end-users). This approach supports keyword-based searching like classical IRS and integrates structured searching with the search attributes notion. It is based on an indexing method of document tree leafs which authorize a content-oriented retrieval. The retrieval subdocuments are ranked according to their similarity with the user's query. We use a similarity measure which is a compromise between two measures: exhaustiveness and specificity.
A Taxonomy for Object-Relational Queries.
A comprehensive study of object-relational queries gives not only an understanding of full capability of object-relational query language but also a direction for query processing and optimization. This chapter classifies object-relational queries into REF queries, aggregate queries and inheritance queries. REF queries are queries involving REF pointers, whereas aggregation queries use either nested table structures or index on clusters. Finally, inheritance queries are queries on inheritance hierarchies.
Re-Engineering and Automation of Business Processes: Criteria for Selecting Supporting Tools.
Re-engineering of business processes and their automation is an activity very common in most organizations in order to keep or create a competitive business advantage in the changing business environment. Business Process Modeling Tools (BPMTs) and Workflow Management Systems (WFMSs) are the most popular tools used for business process transformation and automation of the redesigned business processes within and outside organization's boundaries. This chapter describes a set of criteria for selecting appropriate BPMTs and WFMSs among the diversity of the tools offered in the market in order to assist the interested manager or business process engineer to more successfully manage the business process transformation. While establishing the proposed criteria, we considered currently available technology and standards for visual enterprise support and inter-organizational business process modeling and automation.
Metrics for Data Warehouse Quality.
This chapter proposes a set of metrics to assess data warehouse quality. A set of data warehouse metrics is presented, and the formal and empirical validations that have been done with them. As we consider that information is the main organizational asset, one of our primary duties should be assuring its quality. Although some interesting guidelines have been proposed for designing "good" data models for data warehouses, more objective indicators are needed. Metrics are a useful objective mechanism for improving the quality of software products and also for determining the best ways to help professionals and researchers. In this way, our goal is to elaborate a set of metrics for measuring data warehouse quality which can help designers in choosing the best option among more than one alternative design.
Integrity Constraints in an Active Database Environment.
This chapter surveys the interaction between active rules and integrity constraints. First, we analyze the static case following the SQL-1999 Standard Committee point of view which, up to date, represents the state of the art. Then, we consider the case of dynamic constraints for which we use a temporal logic formalism. finally, we discuss the applicabilty, limitations and partial solutions found when attempting to ensure the satisfaction of dynamic constraints.
Integrity Constraints in Spatial Databases.
Preserving Relationship Cardinality Constraints in Relational Schemata.
Translating Advanced Integrity Checking Technology to SQL.
Introduction.
Consistent Queries Over Databases with Integrity Constraints.
Integrity Issues in the Web: Beyond Distributed Databases.
Functional Dependencies for Value Based Identification in Object-Oriented Databases.
Database Integrity: Fundamental and Current Implementations.
In Part I, this chapter survyes the state of the art of the semantic integrity constraints in some relational and object relational available database systems. In Part II, it also provides an overview of the SQL standard integrity issues and describes semantic integrity support in the following DBMSs: Oracle, IBM DB2, Informix, sybase and PostgreSQL.The major differences and similarities among these systems are analyzed in relation to the definition, semantics and fidelity to the SQL standard prescriptions.
Integrity Maintenance in Extensible Databases.
Project Management in Graduate Education.
Alignment of Business and Knowledge Management Strategy.
Project Management Best Practices to Increase Success.
Autopoietic Approach for Information System Development.
Sponsorship in IT Project Management.
A Systematic Approach for Information Systems Evaluation.
Organizing Multimedia Objects by Using Class Algebra.
ICT-Supported Gaming for Competitive Intelligence.
Virtual Communities in Practice.
Concepts and Dynamics of the Application Service Provider Industry.
IT Supporting Strategy Formulation.
Credit Card Users' Data Mining.
Better Executive Information with the Dashboard Approach.
Project Management Models in IT.
Human Motion Tracking and Recognition.
Expanding Data Mining Power with System Dynamics.
Enhancing Competitiveness of B2B and B2C E-Commerce.
Natural Computing.
Information Technology Usage in Nigeria.
Distance Education Success Factors.
Impacts of the Intranet on PDO.
Online Learning as a Form of Accomodation.
SMEs Amidst Global Technological Changes.
Legal Issues of Virtual Organizations.
Telemedicine in Healthcare Organisations.
Multicast Routing Protocols, Algorithms and its QoS Extensions.
Triggers, Rules and Constraints in Databases.
Data Mining for Combining Forecasts in Inventory Management.
Spatial Modeling of Risk Factors for Gender-Specific Child Mortality.
Content-Based Retrieval Concept.
Knowledge Management and Social Learning.
Interface Design Issues for Mobile Commerce.
Perturbations, Accuracy and Robustness in Neural Networks.
A Duplicate Chinese Document Image Retrieval System.
Multimedia Evaluations Based on Cognitive Science Findings.
One-to-One Video-Conferencing Education.
International Digital Studies Approach for Examining International Online Interactions.
Structure- and Content-Based Retrieval for XML Documents.
Strategic Utilization of Data Mining.
Incorporating Data Stream Analysis into Decision Support Systems.
Semantic Web Fundamentals.
Novel Indexing Method of Relations Between Salient Objects.
Democratic E-Governance.
Graph Encoding and Recursion Computation.
Strategic Knowledge Management in Public Organizations.
Organization and Management Issues of End User Computing.
Management Considerations for B2B Online Exchanges.
Behavioral Perspective of Groupware Adoption.
Addressing the Central Problem in Cyber Ethics through Stories.
Data Mining in Practice.
Web Technologies And Data Warehousing Strategies.
Similarity Web Pages Retrieval Technologies on the Internet.
Content Description for Face Animation.
Data Mining in Franchise Organizations.
Diffusion of E-Learning as an Educational Innovation.
Franchising and Information Technology.
Diffusion of Innovations in Organisations.
Neural Networks for Automobile Insurance Customers.
Project Management for IT Projects.
Incremental Expansion of a Distributed Database Systems.
Integrative Document and Content Management Solutions.
Managing Strategic IT Investment Decisions.
Innovative Thinking in Software Development.
Effective Learning Through Optimum Distance Among Team Members.
User Spreadsheet Systems Development.
History and Future Development of Group Support Systems.
Migrating Legacy Systems to the Web.
Evaluation of an Open Learning Environment.
Reasoning About User Preferences.
Portable Portals for M-Commerce.
Collective Intentional Action in Virtual Communities.
Communication Management for Large Modules.
Building Police/Community Relations Through Virtual Communities.
Critical Success Factors of ERP Implementation.
Executive Judgement in E-Business Strategy.
Mobile Transaction Models Framework.
Audience Response Systems and Face-to-Face Learning.
Isoluminance Contours for Animated Visualization.
Data Warehouse Development.
Web Initiatives and E-Commerce Strategy.
IS Implementation in the UK Health Sector.
Geographic Information Systems as Decision Tools.
Macromedia Flash on the Client and the Server.
Relationship Cardinality Constraints in Relational Database Design.
Outsourcing Information Technology in Australia.
Virtual Teams as Sociotechnical Systems.
Web Access by Older Adult Users.
Market of Resources for Agile/Virtual Enterprise Integration.
Web Usability.
Challenges in Quality of Service for Tomorrow's Networks.
Artificial Intelligence Techniques in Medicine and Healthcare.
Methods for Understanding IS Failures.
Women in the IT Profession.
Exception Rules in Data Mining.
Virtual Work Research Agenda.
Web Caching.
Governance in IT Outsourcing Partnerships.
Context-Aware Framework for ERP.
Usability Evaluation of Online Learning Programs.
Hypothetical Reasoning Over Databases.
Computer Attitude and Anxiety.
Tertiary Education and the Internet.
Trust in Technology Partnerships.
Object Database Benchmarks.
Hyper Video for Distance Learning.
Rotating Banner Advertisements on the World Wide Web.
New Perspectives on Rewards and Knowledge Sharing.
Formal Methods in Software Engineering.
Design Levels for Distance and Online Learning.
Concepts of Emergence Index in Image Databases.
Interactivity and Amusement in Electronic Commerce.
Media and Personal Involvement in the Perceptions of Data Quality.
Principles of Advanced Database Integrity Checking.
eCRM in a Manufacturing Environment.
Alignment of Information Technology and Human Resources Strategies.
Integrating Security in the Development Process with UML.
Principles to Guide the Integration and Implementation of Educational Technology.
Strategic Vision for Information Technology.
The CRM-KDD Nexus.
Balancing Risks and Rewards of ERP.
From Digital Divide to Digital Dividend.
Business Model Application of UML Stereotypes.
Motivation for Using Microcomputers.
Globalization of Consumer E-Commerce.
Journalism Online in Peru.
Object-Oriented Software Reuse in Business Systems.
Global Implications of E-Commerce Tool and Artifact Creation.
Impediments for Knowledge Sharing in Professional Service Firms.
Issues and Challenges in IT in Small Business.
Audio Analysis Applications for Music.
Web-Based Distance Learning and the Second Digital Divide.
Web-based distance learning programs promise learning options anywhere, anytime, to anyone. However, some individuals with disabilities are locked out of these opportunities when courses are designed in such a way that they are inaccessible to individuals using assistive technology. This chapter provides an overview of access challenges for people with disabilities; suggestions for course developers on creating accessible courses; and suggestions for administrators on developing accessiblity policies, guidelines, and procedures.
Data Communications and E-Learning.
Online Academic Libraries and Distance Learning.
Use Cases and the UML.
U.S. Disabilities Legislation Affecting Information Technology.
Critical Realism as an Underlying Philosophy for IS Research.
Engineering Emotionally Intelligent Agents.
Managing the Organisational Impacts of Information Systems.
Learning Portals as New Academic Spaces.
Database Integrity.
Critical Realism in IS Research.
Informationbase - A New Information System Layer.
Building Educational Technology Partnerships Through Participatory Design.
Self-Organizing Networked Learning Environments.
Recursive Nature of the Market for Enterprise Applications.
E-Commerce Training for SMEs.
Decision Support Systems in Small Businesses.
Learnability.
Contingency Theory, Agent-Based Systems and a Virtual Advisor.
Process-Aware Information Systems for Virtual Teamwork.
Assessing the Value of Information Systems Investments.
Business Processes and Knowledge Management.
Moderator in Governmen t-Initiated Online Discussions.
Wireless Ad Hoc Networking.
Knowledge Management Systems Acceptance.
This chapter introduces a framework of knowledge management systems acceptance labeled Requirements of Acceptance Model (RAM). It argues that acceptance of knowledge management systems is dependent on perceived relevance, systems accessibility, and management support. Together these components constitute the RAM. Further, it argues that implementation of systems is at large a process of acceptance where the requirements of acceptance are attained. Finally, it argues that to achieve the requirements of acceptance, implementation should be iterative and cooperative between users and developers by continually developing, implementing, and testing prototypes.
An ERP Life-Cycle Costs Model.
Socio-Cognitive Model of Trust.
Video Content-Based Retrieval Techniques.
Forward Engineering of UML Static Models.
Reuse of Formal Specifications.
Trust Placement Process in Metateam Projects.
Issues of E-Learning in Third World Countries.
Teams and Electronic Technologies.
Unified Modeling Language.
Functional Integration of Decision Making Support.
Stickiness and Web-Based Customer Loyalty.
Decision-Making Support Systems.
Collaborative Learning On-Demand.
Enterprise Resource Planning for Intelligent Enterprises.
E-Mail Usage in South Pacific Distance Education.
Privacy-Dangers and Protections.
Usability of Online Learning Systems and Course Materials.
Software Contracts for Component-Based Web Engineering.
Translation of Natural Language Patterns to Object and Process Modeling.
Client/Server and the Knowledge Directory.
Multilingual Electronic Commerce in a Global Economy.
Census Data for Health Preparedness and Response.
Using Geographic Information Systems to Solve Community Problems.
Agent- and Web-Based Employment Marketspaces in the U.S. Department of Defense.
Parallel and Distributed Multimedia Databases.
Knowledge, IT, and the Firm.
Information Systems and Systems Theory.
Heuristics in Medical Data Mining.
Optical Music Recognition with Wavelets.
Marketplace Architecture for Enterprise Integration.
Mobile Location Services.
Software and Systems Engineering Integration.
Ethics of New Technologies.
Supporting Assurnace and Compliance Monitoring.
Infocratic Perspective on the Delivery of Personal Financial Services.
Benefits and Challenges of Blended Learning Environments.
Computer-Mediated Learning Groups.
Quality Assurance Issues for Online Universities.
Consistent Queries Over Databases with Integrity Constraints.
User Perceptions and Groupware Use.
The Changing Library Education Curriculum.
Diffusion Patterns of the Internet Technology Cluster in Irish SMEs.
Internet Abuse and Addiction in the Workplace.
Application Service Provision for Intelligent Enterprises.
Agents and Payment Systems in E-Commerce.
Mobile Agent Authentication and Authorization in E-Commerce.
Ontology-Based Query Formation and Information Retrieval.
Theoretical Framework for CRM Outsourcing.
Packet Inter-Arrival Distributions in Computer Network Workloads.
Information Modeling in UML and ORM.
Web-Based Supply Chain Strategy.
Metadata for Electronic Documents Using the Dublin Core.
Overcoming Barriers in the Planning of a Virtual Library.
Library Management and Organizational Change.
Technology of Formal Education.
Security Issues in Distributed Transaction Processing Systems.
Offshore Software Development Outsourcing.
Successful Health Information System Implementation.
Space Opera- GIS Basics.
Chief Knowledge Officers.
Intelligent Software Agents.
VRML-Based System for a 3D Virtual Museum.
Dot-Comming SMEs in Singapore for a New Economy.
Electronic Government Strategies and Research in the U.S..
Process-Based Data Mining.
Automation of American Criminal Justice.
Measurement Issues in Decision Support Systems.
After a brief discussion on the history of decision-making, this chapter focuses on metrics for justifying investment in information systems and technology and for measuring business and management performance. The discussion of metrics is linked to current practices in decision support systems and focuses on the needs for future systems. With several examples drawn from contemporary practice, we introduce implementation guidelines for DSS development incorporating new metrics that go beyond ROI and Balanced Scorecard-like measures. Suggested guidelines include simplicity, selectivity, a focus on research and learning, and benchmarking. These guidelines suggest that future metrics to support decision support systems should be grouped into meaningful categories and tied more closely to system architecture.
ICTs as Participatory Vehicles.
Content Management in Organizations.
Current Multicast Technology.
Technology's Role in Distance Education.
Dimensions of Database Quality.
World Wide Web Search Technologies.
Information Systems and Small Business.
Trust in Knowledge-Based Organizations.
Enterprise Resource Planning and Systems Integration.
Instant Messaging Moves From the Home to the Office.
Electronic Commerce Policies for Dutch SMEs.
Measuring Collaboration in Online Communications.
Electronic/Digital Government Innovation, and Publishing Trends with IT.
E-Government, E-Democracy and the Politicians.
Simulation for Business Engineering of Electronic Markets.
The Past, Present, and Future of End-User Performance.
Adoption of E-Commerce in the Value Chain by SMEs.
Internet Support for Knowledge Management Systems.
Case-Based Learning in Computer Information Systems.
A Language/Action Based Approach to Information Modeling.
Harmonizing IT and Business Strategies.
This chapter proposes that all business strategies should be harmonized into a single strategy, rather than attempt to align IT strategy with business strategy. It focuses on two hypotheses: firstly, that IT strategy is not widely aligned with business strategy; and secondly, that IT is still thought of as "something different" in businesses. The chapter proposes that rather than attempt to align IT strategy with business strategy, the strategies should be harmonized into a single strategy. The chapter attempts to use lessons from geese to outline the process of strategic development.
Intelligent Metabusiness.
Small Busines Transformation Through Knowledge Management.
Fundamentals of Multirate Systems.
Simple Methods for Design of Narrowband Highpass FIR Filters.
One Method for Design of Narrowband Lowpass Filters.
This chapter describes a design of a narrowband lowpass finite impulse response (FIR) filter using a small number of multipliers per output sample (MPS). The method is based on the use of a frequency-improved recursive running sum (RRS), called the sharpening RRS filter, and the interpolated finite impulse response (IFIR) structure. The filter sharpening technique uses multiple copies of the same filter according to an amplitude change function (ACF), which maps a transfer function before sharpening to a desired form after sharpening. Three ACFs are used in the design, as illustrated in the accompanying examples.
Flexible Job-Shop Scheduling Problems.
The Software Industry in Egypt.
On the Relativity of Ontological Domains and Their Specifications.
E-Collaboration Support Systems Issues to be Addressed.
Cross-Cultural Research in MIS.
Standards for Web-Based Integration Adapters.
Interoperability in Geospatial Information Systems.
Usability and Learnability Evaluation of Web-Based ODL Programs.
Facial and Body Feature Extraction for Emotionally-Rich HCI.
Gender and Computer Anxiety.
Cache Management for Web-Powered Databases.
E-Learning Environment.
Formal Development of Reactive Agent-Based Systems.
Contracting Mechanisms in IS/IT Outsourcing Phenomenon.
Archival Issues Related to Digital Creations.
Curriculum Development in Web-Based Education.
Online Communities and Community Building.
Information Resources Development Challenges in a Cross-Cultural Environment.
Organizations in the Western world devote much attention to the development of information systems as strategic resources. The transfer of this management practice to a different cultural environment generates new challenges as reported in the experience of a Pacific Basin public institution with Western affiliation, while under strong influence of local cultures and practices. The value of the investment in information resources is judged not only on technical merits, but also on the ability of the organization to properly address the cultural and organizational issues related to information usage.
Real Options Analysis in Strategic Information Technology Adoption.
Virtualization and Its Role in Business.
This chapter presents some of the aspects of virtualisation and its role in modern society. In today's world, a controlled virtualisation process creates enormous opportunity for economic growth of those countries and organisations, which, so far, due to various restrictions, have had no chance to become competitive in the global market. Those people and organisations that know how to make use of the opportunities presented by virtualisation may become more effective in business. Moreover, virtualisation creates the best options for intellectual enterprise development. Virtualisation is a very complex process. The author would like to discusses both the positive impact virtualisation can have on society and also some dangers or problems.
Information Technology Strategic Alignment.
Promotion of e-Government in Japan and Its Operation.
Knowledge Discovery Using Heuristics.
Artificial Neural Networks Used in Automobile Insurance Underwriting.
Modeling ERP Acedemic Deployment via AST.
Scenarios for Web-Enhanced Learning.
Business Model Innovation in the Digital Economy.
Enterprise Resource Planning Maintenance Concepts.
Multimedia Content Adaption.
Triangular Strategic Analysis for Hybrid E-Retailers.
E-Commerce Curriculum.
Virtual Organization in the Human Mind.
Experiential Perspective on Knowledge Management.
Mobile Commerce Technology.
Bridging the Industry-University Gap through Action Research.
Challenges in M-Commerce.
Contemporary IT-Assisted Retail Management.
Security and Trust of Online Auction Systems.
Quality of UML.
MESH Object-Oriented Hypermedia Framework.
Usable M-Commerce Systems.
Application of Fuzzy Logic Fraud Detection.
Structural Text Mining.
The Impact of Sound Relationships on Achieving Alignment.
Internet Data Mining Using Statistical Techniques.
Business Modelling with Client-Oriented Requirements Strategy.
Adaptive Mobile Applications.
Technology and Work in the Virtual Organization.
Enterprise Resource Planning and Integration.
Database Support for M-Commerce.
Minorities and the Digital Divide.
Eight Key Elements of Successful Self-Funding E-Learning Programs.
DRM Technology for Mobile Multimedia.
Empirical Study of E-Commerce Adoption SMEs in Thailand.
Information Resources Development in China.
Certifying Software Product and Processes.
Comparing Conventional and Non-Parametric Option Pricing.
Contextual Metadata for Document Databases.
Bayesian Machine Learning.
Intelligent Business Portals.
Implementation Management of an E-Commerce-Enabled Enterprise Information System.
Culture and Anonymity in GSS Meetings.
Public Sector Case Study on the Benefits of IS/IT.
Faculty Perceptions and Participation in Distance Education.
Spatial Analysis in a Public Health Setting.
User Experiences of the E-commerce Site with the Standard User Interface.
Newcomer Assimilation in Virtual Team Socialization.
Softening the MIS Curriculum for a Technology-Based Profession.
Strategic Alignment of Organizational Strategies.
Introducing Java to the IT Master's Curriculum.
Inexperienced and Global Software Teams.
Simulation and Gaming in IT Education.
Systems Thinking and the Internet.
Speech and Audio Signal Applications.
Wireless Middleware.
Trust in B2C E-Commerce for the New Zealand Maori.
Technology Planning in Schools.
Multimedia Information Filtering.
Designing OMIS-Based Collaboration for Learning Organizations.
Personal Internet Usage and Quality of Work Life.
Trends in Information Technology Governance.
Bridging the Digital Divide in Scotland.
Critical Strategies for IS Projects.
Behavioral Factors in Strategic Alliances.
Multiple Internet Technologies in In-Class Education.
Business Process and Workflow Modeling in Web Services.
Signature-Based Indexing Techniques for Web Access Logs.
Transfering Technology to the Developing World.
Qos-Oriented MAC Protocols for Future Mobile Applications.
Learning Systems Engineering.
Delivering Web-Based Education.
Object-Oriented Software Metrics.
Critical Success Factors for Distance Education Programs.
Designing Agents with Negotiation Capabilities.
INFOSEC Policy - The Foundation for an Effective Security Program.
Decision-Making Support Systems and Representation Levels.
Integrating Requirements Engineering Techniques and Formal Methods.
Future of Small Business E-Commerce.
Staying Up-to-Date with Changes in IT.
Complex Adaptive Enterprises.
Success Surrogates in Representational Decision Support Systems.
Cooperation of Geographic and Multidimensional Databases.
Managing Value-Creation In The Digital Economy.
Hierarchies in Multidimensional Databases.
E-Mail as a Strategic Tool in Organizations.
Decision Support Systems Concept.
IS Project Management Contemporary Research Challenges.
Classification-Rule Discovery with an Ant Colony Algorithm.
Internet Diffusion in the Hospitality Industry.
A Web-Geographical Information System to Support Territorial Data Integration.
Educating the Business Information Technologist.
Text Mining in the Context of Business Intelligence.
Web Search via Learning from Relevance Feedback.
Animated Characters within the MPEG-4 Standard.
Bayesian Modelling for Machine Learning.
Current Network Security Software.
Building and Management of Trust in Information Systems.
Sharing Organizational Knowledge through Knowledge Repositories.
Efficient Multirate Filtering.
Basic Notions on Multidimensional Aggregate Data.
Managing Hierarchies and Taxonomies in Relational Databases.
Intelligent Software Agents in E-Commerce.
Innovation Link Between Organization Knowledge and Customer Knowledge.
Intelligent Agents For Competitive Advantage.
Technology and Knowledge Management.
E-Commerce Taxation Issues.
Policy Frameworks for Secure Electronic Business.
A Socio-Technical Case Study of Bangladesh.
Strategically-Focused Enterprise Knowledge Management.
Data Mining for Supply Chain Management Complex Networks.
Functionalities and Position of Manufacturing Execution Systems.
Constructionist Perspective of Organizational Data Mining.
Next-Generation ERP.
Evolution of ERP Systems.
Database Technologies on the Web.
Modeling for E-Learning Systems.
Limited-Perspective Bias in Technology Projects.
XML Schema Integration and E-Commerce.
Web Accessibility and the Law.
Geospatial Information Systems and Enterprise Collaboration.
Discovery of Classification Rules from Databases.
Technological Collaboration and Trust in Virtual Teams.
Change Process Drivers for E-Business.
Relating Cognitive Problem-Solving Style to User Resistance.
Antecedents of Trust in Online Communities.
Digital Government and Individual Privacy.
The growth of the Internet and digital government has dramatically increased the Federal government's ability to collect, analyze, and disclose personal information about many private aspects of citizens' lives. Personal information once available only on paper to a limited number of people is now instantly retrievable anywhere in the world by anyone with a computer and an Internet connection. Over time, there has also been a declining level of trust by Americans in government, and currently, many perceive the government as a potential threat to their privacy. Given these forces at work in our society, one should not be surprised to read the results of surveys that show privacy as a top concern of citizens in the 21st century. If citizens do not believe that the government is adequately protecting the privacy of their individual information, they may be less willing to provide this information. Such reluctance could compromise the ability of government to collect important information necessary to develop, administer and evaluate the impact of various policies and programs. Privacy issues discussed in this chapter include challenges regarding (1) protecting personal privacy; (2) ensuring confidentiality of data collected; and (3) implementing appropriate security controls. Perspectives on privacy and stewardship responsibilities of agencies are also discussed.
Dynamic Multidimensional Data Cubes for Interactive Analysis of Massive Datasets.
Virtual Work, Trust and Rationality.
Extensions to UML Using Stereotypes.
Agent-Based Negotiation in E-Marketing.
Risk Management in the Digital Economy.
Contract-Based Workflow Design Patterns in M-Commerce.
Modeling Information Systems in UML.
E-Business Transaction in Web Integrated Network Environment.
Inclusion Dependencies.
Multimedia Computing Environment for Telemedical Applications.
Conducting Ethical Research in Virtual Environments.
Technology-Mediated Progressive Inquiry in Higher Education.
Adaptive Playout Control Schemes for Speech over the Internet.
Issues in Delivering Course Material via the Web.
How Teachers Use Instructional Design in Real Classrooms.
Strategic Experimentation and Knowledge Management.
Trends and Perspectives in Online Education.
Evaluating Computer-Supported Learning Initiatives.
Designing Web Applications.
Innovations for Online Collaborative Learning in Mathematics.
Shaping the Evolution of Mobile Commerce.
Basics of the Triune Continuum Paradigm.
A University/Community Partnership to Bridge the Digital Divide.
Data Mining and Mobile Business Data.
Virtual Schools.
Wireless Technologies to Enable Electronic Business.
Developing Dynamic Balanced Scorecards.
Music Score Watermarking.
Non-Functional Requirements and UML Stereotypes.
Knowledge-Based Support Environment.
Combining Local and Global Expertise in Services.
Bibliomining for Library Decision-Making.
Defining and Understanding ERP Systems.
Interoperability of Information Systems.
Survey of 3D Human Body Representations.
Classroom Communication on a Different Blackboard.
Implementing an Online Academic Evaluation System.
Delineating Knowledge Flows for Enterprise Agility.
The Organizational Context in the Use of a Workflow System.
Critical Trends in Telecommunications.
Social Responsibility and the Technology Paradigm in Latin America.
Distributed Recommender Systems for Internet Commerce.
Model-Supported Alignment of IS Architecture.
A Primer on E-Government.
Enhancing Workplaces with Constructine Online Recreation.
Digitization of Library Information and its Accessibilty for People with Disabilities.
Progammed Instruction, Programmed Branching, and Learning Outcomes.
Software Requirements Risk and Maintainability.
Human Body Part Classification and Activity Recognition for Real-Time Systems.
Functional Dependency and Other Related Dependancies.
Software Reuse in Hypermedia Applications.
Computing Curriculum Analysis and Development.
Storage and Access Control Issues for XML Documents.
Strategies of E-Commerce Business Value Optimization.
Developing Trust in Virtual Teams.
Software Agents in E-Commerce Systems.
New SQL Standard in Database Modeling.
Leader-Facilitated Relationship Building in Virtual Teams.
Program Execution and Visualization on the Web.
Programming is a demanding task with an education program that requires the assistance of complex tools such as programming environments, algorithm animators, problem graders, etc. In this chapter, we give a comprehensive presentation of tools for program execution and visualization on the Web. We summarize the technical evolution of these tools, describe educational uses, report lessons learned, and look at formal evaluations of their educational effectiveness. We also deal with a closely related matter, namely, collections of Web documents containing programming exercises. Finally, we outline our view of future trends in the use of the Web for programming education, and we give our personal conclusions. This chapter is of interest to educators and researchers, because it gives a comprehensive presentation of the main issues and results of a field where most of the contributions are sparse in the literature.
Metrics for Data Warehouse Quality.
Institutional Dimensions of Information Systems Evaluation.
IT Productivity Impacts in Manufacturing Contexts.
Obstacles for SMEs for E-Adoption in the Asia Pacific Region.
SMEs in Knowledge-Based Economies.
End-User Computing Success Measurement.
Essentials of Functional and Object-Oriented Methodology.
Students' Perceptions of Online Courses.
Information Systems and Technology in South Africa.
Departure of the Expert Systems Project Champion.
The Social Contract Revised.
Observations on Implementing Specializations Within an IT Program.
Citizenship and New Technologies.
Changing Trends in the Preparation of Print Media.
Generic Framework for Defining Domain-Specific Models.
Management of Cognitive and Affective Trust to Support Collaboration.
Life Cycle of ERP Systems.
Liability for System and Data Quality.
Uses and Gratifications for the World Wide Web.
Tailorable Information Systems.
Best Practices for Effective Virtual Teams.
System Development for E-Business.
Public-Key Cryptography.
Component-Oriented Approach for Designing Enterprise Architecture.
Digital Asset Management Concepts.
Organizational Hypermedia Document Management Through Metadata.
Governance Structures for IT in the Health Care Industry.
Histogram Generation from the HSV Color Space.
Legal Expert Systems in Administrative Organizations.
Technology in the Foreign Language Classroom.
Integration Framework for Complex Systems.
Personal Information Privacy and Internet Technology.
Cognitive Research in Information Systems.
Mobile Telecommunications and M-Commerce Applications.
Agent-Based Intelligence Infrastructure.
ERP Adoption by Indian Organizations.
Information Laws.
Cross-Culture Communication.
Bridging the Growing Digital Divide.
Actor-Network Theory in Information Systems Research.
Modelling Technological Change in Small Business.
Actor-Network Theory and Adoption of E-Commerce in SMEs.
Ecological Models and Information Systems Curriculum.
Knowledge Exchange in Electronic Networks of Practice.
Designing Hypertext and the Web.
Business Strategy, Structure and IT Alignment.
Building Local Capacity via Scaleable Web-Based Services.
Querying Multidimensional Data.
A powerful and easy-to-use querying environment is certainly one of the most important components in a multidimensional database, and its effectiveness is influenced by many other aspects, both logical (data model, integration, policy of view materialization, etc.) and physical (multidimensional or relational storage, indexes, etc.). As is evident, multidimensional querying is often based on the metaphor of the data cube and on the concepts of facts, measures, and dimensions. In contrast to conventional transactional environments, multidimensional querying is often an exploratory process, performed by navigating along the dimensions and measures, increasing/decreasing the level of detail and focusing on specific subparts of the cube that appear to be "promising" for the required information.In this chapter we focus on the main languages proposed in the literature to express multidimensional queries, particularly those based on: (i) an algebraic approach, (ii) a declarative paradigm (calculus), and (iii) visual constructs and syntax. We analyze the problem of evaluation, i.e., the issues related to the efficient data retrieval and calculation, possibly (often necessarily) using some pre-computed data, a problem known in the literature as the problem of rewriting a query using views. We also illustrate the use of particular index structures to speed up the query evaluation process.
Improving Virtual Teams Through Creativity.
Leapfrogging an IT Sector.
Qualitative Methods in IS Research.
E-Business Systems Security for Intelligent Enterprise.
Implementing the Shared Event Paradigm.
Audience-Driven Web Site Design.
Web Tools for Molecular Biological Data Analysis.
Monitoring Strategies for Internet Technologies.
Digital Literacy and the Position of the End-User.
Intranet Use and Emergence of Networks of Practice.
Metrics for the Evaluation of Test Delivery Systems.
Face Expression and Motion Analysis over Monocular Images.
Artificial Neural Networks in Financial Trading.
Surveying Mobile Commerce Environments.
Content-Based Image Retrieval Query Paradigms.
Analyzing the Quality of Virtual Teams.
Bayesian Analysis of Geographical Variation in Disease Risk.
Simulation in Information Systems Research.
Cluster Analysis Using Rough Clustering and k-Means Clustering.
Networks and Electronic Commerce Adoption in Small Businesses.
Information and Communication Technology Tools for Competitive Intelligence.
Supporting the Evaluation of Intelligent Sources.
Data Dissemination in Mobile Databases.
Trust in B2C E-Commerce Interface.
GIS-Based Accessibility Measures and Application.
IT Industry Success in Finland and New Zealand.
Enhanced Knowledge Warehouse.
Learning 3D Face Animation Model.
E-Governement Interoperability.
Data Collection Methodologies for Web-Based Experiments.
Ethical Implications of Investigating Internet Relationships.
"Anytime, Anywhere" in the Context of Mobile Work.
Knowledge Discovery Solutions for Intelligent Enterprises.
Evaluating IS Quality as a Measure of IS Effectiveness.
E-Mail and Communication.
IT Implementation in Small Business.
Trust in Virtual Enterprises.
Designing High Performance Virtual Teams.
Improving Public Sector Service Delivery through Knowledge Sharing.
Traversal Pattern Mining in Web Usage Data.
Applying a Metadata Framework to Improve Data Quality.
Java 2 Micro Edition for Wireless Enterprose.
Distributed Construction through Participatory Design.
Tactic Knowledge and Discourse Analysis.
Advanced Techniques for Object-Based Image Retrieval.
Knowledge Management on the Web.
Neural Networks for Retail Sales Forecasting.
New Advancements in Image Segmentation for CBIR.
The Impact of IT on Business Partnerships and Organizational Structures.
Open Source Software Development Model.
Fault Tolerance for Distributed and Networked Systems.
Isochronus Distributed Multimedia Synchronization.
Integrated-Services Architecture for Building Internet Multimedia Applications.
Knowledge Discovery from Databases.
Discovering Association Rules in Temporal Databases.
Kernelized Database Systems Security.
Collective Knowledge Composition in a P2P Network.
E-Government Databases.
Converting a Legacy Database to Object-Oriented Database.
E-Mail Data Stores.
Proper Placement of Derived Classes in the Class Hierarchy.
Replication Mechanisms Over a Set of Distributed UDDI Registries.
Logic Databases and Inconsistency Handling.
High Quality Conceptual Schemes.
A Rhetorical Perspective on Localization and International Outsourcing.
Hierarchical Architecture of Expert Systems for Database Management.
Ontological Assumptions in Information Modeling.
Intension Mining.
Integrative Document and Content Management Systems Architecture.
Dataveillance and Panoptic Marketspaces.
Advanced Query Optimization.
Main Memory Databases.
Business Rules in Databases.
Database Replication Protocols.
Data Warehouses.
Text Databases.
Relational, Object-Oriented and Object-Relational Data Models.
Online Data Mining.
Extended Entity Relationship Modeling.
Symbolic Objects and Symbolic Data Analysis.
Integration of Data Semantics in Heterogeneous Database Federations.
Query Operators in Temporal XML Databases.
Raster Databases.
Since the launch of Google Earth at the latest it is clear that online services for multi-Terabyte satellite imagery are becoming integral part of our Internet experience. Actually, 2-D imagery is but the tip of the iceberg - the general concept of multi-dimensional spatio-temporal raster data covers 1-D sensor time series, 2-D imagery, 3-D image time series (x/y/t) and exploration data (x/y/z), 4-D climate models (x/y/z/t), and many more.
Ensuring Serializability for Mobile-Client Data Caching.
Databases for Mobile Applications.
Rough Sets.
Data Model Versioning and Database Evolution.
Vertical Database Design for Scalable Data Mining.
Ontology-Based Data Integration.
Database Support for Workflow Management Systems.
Real-Time Databases.
Object Modeling of RDBMS Based Applications.
Business-to-Business Integration.
Knowledge Mining.
Enterprise Application Integration.
Moving Objects Databases.
Database Engineering Focusing on Modern Dynamism Crises.
Multimedia Databases.
Using Views to Query XML Documents.
Open Source Software and Information Systems on the Web.
Set Valued Attributes.
Normalizing Multimedia Databases.
Consistency in Spatial Databases.
Metric Databases.
Storing XML Documents in Databases.
Path-Oriented Queries and Tree Inclusion Problems.
Ubiquitous Computing and Databases.
Signature Files and Signature File Construction.
Semantic Information Management.
Sensors, Uncertainty Models, and Probabilistic Queries.
Data Warehousing and OLAP.
The Information Quality of Databases.
Text Categorization.
Active Database Management Systems.
Querical Data Networks.
Query Processing in Spatial Databases.
Extraction-Transformation-Loading Processes.
Free Software and Open Source Databases.
Open Source Database Management Systems.
Set Comparison in Relational Query Languages.
Biological Data Mining.
Deriving Spatial Integrity Constraints from Geographic Application Schemas.
Query Processing for RDF Data.
Data Warehousing, Multi-Dimensional Data Models, and OLAP.
Modeling and Querying Temporal Data.
Temporal Databases.
Benchmarking and Data Generation in Moving Objects Databases.
Mathematics of Generic Specifications for Model Management, I.
Spatio-Temporal Indexing Techniques.
Mathematics of Generic Specifications for Model Management, II.
Active Federated Database Systems.
Generic Model Management.
Digital Media Warehouses.
Semantically Modeled Enterprise Databases.
Biometric Databases.
Knowledge Discovery and Geographical Databases.
Multiparticipant Decision Making and Balanced Scorecard Collaborative.
Semantic Enrichment of Geographical Databases.
Knowledge Management in Tourism.
Syntactical and Semantical Correctness of Pictorial Queries for GIS.
Ontologies and Their Practical Implementation.
Data Dissemination.
Using Semantic Web Tools for Ontologies Construction.
Repairing Inconsistent XML Data with Functional Dependencies.
Geometric Quality in Geographic Information.
Managing Inconsistent Databases Using Active Integrity Constraints.
Object-Relational Modeling in the UML.
Replication Methods and Their Properties.
Service Mechanism Quality for Enhanced Mobile Multimedia Database Query Processing.
Transaction Concurrency Methods.
Common Information Model.
Document Versioning in Digital Libraries.
Multilevel Databases.
Component-Based Generalized Database Index Model.
An XML Multi-Tier Pattern Dissemination System.
Bioinformatics Data Management and Data Mining.
Natural Language Front-End for a Database.
Applying Database Techniques to the Semantic Web.
Preferred Repairs for Inconsistent Databases.
Rewriting and Efficient Computation of Bound Disjunctive Datalog Queries.
Rewriting and Efficient Computation of Bound Disjunctive Datalog Queries.
Transformation-Based Database Engineering.
CASE Tools for Database Engineering.
Checking Integrity Constraints in a Distributed Database.
Optimization of Continual Queries.
Security Controls for Database Technology and Applications.
Similarity Search in Time Series Databases.
Database Query Personalization.
Data Quality Assessment.
A Development Environment for Customer-Oriented Web Business.
Transactional Support for Mobile Databases.
Engineering Information Modeling in Databases.
Fuzzy Database Modeling.
Semistructured Data and its Conceptual Models.
Management of Large Moving Objects Datasets: Indexing, Benchmarking and Uncertainty in Movement Representation.
Approximate Computation of Distance-Based Queries.
Spatiotemporal Prediction using Data Mining Tools.
Object-Relational Spatial Indexing.
Integrating Web Data and Geographic Knowledge into Spatial Databases.
Spatial Joins: Algorithms, Cost Models and Optimization Techniques.
Quadtree-Based Image Representation and Retrieval.
Similarity Learning in GIS: An Overview of Definitions, Prerequisites and Challenges.
Survey on Spatial Data Modelling Approaches.
Indexing Multi-Dimensional Trajectories for Similarity Queries.
Mining in Spatiotemporal Databases.
Applications of Moving Objects Databases.
Simple and Incremental Nearest-Neighbor Search for Spatiotemporal Databases.
Source Integration for Data Warehousing.
While the main goal of a data warehouse is to provide support for data analysis and management's decisions, a fundamental aspect in design of a data warehouse system is the process of acquiring the raw data from a set of relevant information sources. We will call source integration system the component of a data warehouse system dealing with this process. The main goal of a source integration system is to deal with the transfer of data from the set of sources constituting the application-oriented operational environment, to the data warehouse. Since sources are typically autonomous, distributed, and heterogeneous, this task has to deal with the problem of cleaning, reconciling, and integrating data coming from the sources. The design of a source integration system is a very complex task, which comprises several different issues. The purpose of this chapter is to discuss the most important problems arising in the design of a source integration system, with special emphasis on schema integration, processing queries for data integration, and data cleaning and reconciliation.
Incomplete Information in Multidimensional Databases.
While incomplete information is endemic to real-world data, current multidimensional data models are not engineered to manage incomplete information in base data, derived data, and dimensions. This chapter presents several strategies for managing incomplete information in multidimensional databases. Which strategy to use is dependent on the kind of incomplete information present, and also on where it occurs in the multidimensional database. A relatively simple strategy is to replace incomplete information with appropriate, complete information. The advantage of this strategy is that all multidimensional databases can manage complete information. Other strategies require more substantial changes to the multidimensional database. One strategy is to reflect the incompleteness in computed aggregates, which is possible only if the multidimensional database allows incomplete values in its hierarchies. Another strategy is to measure the amount of incompleteness in aggregated values by tallying how much uncertain information went into their production.
Privacy in Multidimensional Databases.
When answering queries that ask for, summary statistics, the query-system of a multidimensional database should guard confidential data, that is, it should avoid revealing (directly or indirectly) individual data, which could be exactly calculated or accurately estimated from the values of answered queries. In order to prevent the disclosure of confidential data, the query-system should be provided with an auditing procedure which, each time a new query is processed, checks that its answer does not allow a (knowledgeable) user to disclose any sensitive data. A promising approach consists in keeping track of (or auditing) answered queries by means a dynamic graphical data structure, here called the answer map, whose size increases with the number of answered queries and with the number of dimensions of the database, so that the problem of the existence of an efficient auditing procedure naturally arises. This chapter reviews recent results on this problem for "additive" queries (such as COUNT and SUM queries) by listing some polynomially solvable problems as well as some hard problems, and suggests directions for future work.
Time in Multidimensional Databases.
In spite of the obvious importance of time in data warehousing and OLAP, current commercial systems do not support tracking the history of a data warehouse, either at the schema or instance level. In this chapter we address this issue, introducing the Temporal Multidimensional Model and a query language, denoted TOLAP, allowing expressing temporal OLAP queries at a high level of abstraction. Further, we show that previous work in temporal databases needs to be extended in order to handle evolution and versioning in OLAP. Finally, we present an implementation, along with preliminary experimental results.
Materialized Viewsin Multidimensional Databases.
Cooperation with Geographic Databases.
The purpose of this chapter is to create cooperation between geographic databases (GDBs) and multidimensional databases (MDDBs), which are considered as the most promising and efficient information technologies for supporting decision making. We focus on the common key elements between geographic and multidimensional data which allow effective support in data cooperating. These elements are basically time and space, which are present implicitly or explicitly in MDDB and are modeled on the dimensions, Time and Location. Thus, because GDBs are primarily concerned with geographic data, we will focus on space as a bridge element for cooperating MDDBs and GDBs. We propose an approach that extends the geographic data structure through special attributes, called binding attributes, in order to describe all phenomena represented by MDDBs. This extension will make it possible to answer more specific "OLAP-based" queries within GDBs without modifying the physical organization of data in both environments.
Hierarchies.
In this chapter we will focus on the rules of aggregation hierarchies in analysis dimensions of a cube. We give an overview of the related works on the basic concepts of the different types of aggregation hierarchies. We then discuss the hierarchies from two different points of view: mapping between domain values and hierarchical structures. In relation to them, we introduce the characterization of some OLAP operators on hierarchies and give a set of operators that concern the change in the hierarchy structure. Finally, we propose an enlargement of the operator set concerning hierarchies.
Basic Notions.
This chapter presents the basic notions regarding multidimensional (aggregate) databases by referring to different definitions given for them in the literature. It illustrates the important concepts of micro, macro, and metadata; presents a formal definition of the aggregation process, discussing the concepts of dimension and dimension hierarchies; describes the multidimensional aggregate data structure, distinguishing between simple, complex, and composite structure; illustrates the different types of null values; and discusses differences and similarities which exist between multidimensional aggregate data (generally called statistical data because they are used mainly by statisticians) and the On-Line-Analytic Processing (OLAP) of multidimensional data represented by different data cubes, also discussing the different (symmetric and nonsymmetric) treatment of dimensions and measures required by OLAP and aggregate multidimensional databases. Finally it discusses a graph model and a tabular model for this kind of data, and gives a set of definitions regarding the OLAP terminology.
Operators for Multidimensional Aggregate Data.
In this chapter the author proposes the different approaches for defining operators able to manipulate this multidimensional structure. In particular, he initially considers operators for multidimensional aggregate data which extend relational algebra and relational calculus (the so-called enlarged relational model). Then he discusses operators for multidimensional aggregate data defined in a tabular environment. In both the cases the author defines such data as statistical (aggregate) data. Subsequently he introduces the operators for OLAP applications, giving a terminology correspondence between the multidimensional aggregate (statistical) databases and OLAP areas. Then he defines the fundamental operators deduced from the previous ones, which form the basic algebra for the manipulation of multidimensional aggregate data, giving their formal definitions and some explanatory examples.
Dynamic Multidimensional Data Cubes.
Data cubes are ubiquitous tools in data warehousing, online analytical processing, and decision support applications. Based on a selection of pre-computed and materialized aggregate values, they can dramatically speed up aggregation and summarization over large data collections. Traditionally, the emphasis has been on lowering query costs with little regard to maintenance, i.e., update cost issues. We argue that current trends require data cubes to be not only query-efficient, but also dynamic at the same time, and we also show how this can be achieved. Several array-based techniques with different tradeoffs between query and update cost are discussed in detail. We also survey selected approaches for sparse data and the popular data cube operator, CUBE. Moreover, this work includes an overview of future trends and their impact on data cubes.
Multidimensionality in Statistical, OLAP, and Scientific Databases.
The term "multidimensional databses" refers to data that can be viewed conceptually in a multidimensional space, where each dimension represents some attributes of the data. Viewing data in this form is natural for many applications, yet the concepts are not treated in a uniform way in the database literature. In this chapter, we show the commonality of concepts between three database areas: statistical, OLAP, and scientific databases. We show that these domains have two main structural concepts: the cross-product space of the dimensions, and the classification hierarchy structure associated with each dimension. In the first part of this chapter we describe how these structures are sed to represent data in statistical and OLAP databases and how summarization operators can be applied to them. Further, we discuss how these structures can be extended to represent related information using federated database concepts. In the second part of the chapter we show that these concepts are common to many scientific database application. In particular, we discuss the importance of supporting classification structures and the difficulty in representing them as tables in relational databases. We also discuss data structures to support multidimensional databases, emphasizing space-time representation, clustering in multidimensional space, indexing in multidimensional space, and supporting classification structures. We conclude by arguing that the concepts of multidimensionality and classification structures as well as the operation over them should be elevated to "first class" object types. These object types should be visible by the application user explicitly in the conceptual schemas as well as exposing them in the user interfaces.
Querying Multidimensional Data.
A powerful and easy-to-use querying environment is certainly one of the most important components in a multidimensional database, and its effectiveness is influenced by many other aspects, both logical (data model, integration, policy of view materialization, etc.) and physical (multidimensional or relational storage, indexes, etc.). As is evident, multidimensional querying is often based on the metaphor of the data cube and on the concepts of facts, measures, and dimensions. In contrast to conventional transactional environments, multidimensional querying is often an exploratory process, performed by navigating along the dimensions and measures, increasing/decreasing the level of detail and focusing on specific subparts of the cube that appear to be "promising" for the required information.In this chapter we focus on the main languages proposed in the literature to express multidimensional queries, particularly those based on: (i) an algebraic approach, (ii) a declarative paradigm (calculus), and (iii) visual constructs and syntax. We analyze the problem of evaluation, i.e., the issues related to the efficient data retrieval and calculation, possibly (often necessarily) using some pre-computed data, a problem known in the literature as the problem of rewriting a query using views. We also illustrate the use of particular index structures to speed up the query evaluation process.
Conceptual Multidimensional Models.
A variety of multidimensional data models have recently been proposed by both academic and industry communities. but consensus on formalism or even a common terminology has not yet emerged. In this chapter, we first discuss the requirements that an ideal conceptual multidimensional model should fulfill. These requirements are suggested by general information system modeling principles and the specific characteristics of OLAP applications. Building on these requirements, we then present a general conceptual multidimensional data model and show how it can be used to describe the basic aspects of a business application in a way that is easy to understand and independent of the criteria for actual data organization in the various systems. Starting from the characteristics of the model proposed, we summarize the general features that a multidimensional conceptual model should support. We then survey various multidimensional models proposed and relate their characteristics to these general features. Finally, we discuss the main points raised in the chapter and some problems that remain to be solved in this context.
Towards an Autopoietic Approach for Information Systems Development.
Business Action and Information Modeling - the Task of the Next Millennium.
Event Modeling.
Information System Design Based on Reuse of Conceptual Components.
An Environment for Managing Enterprise Domain Ontology.
Spatial and Topological Data Models.
Coherent, Consistent, and Comprehensive Modeling of Communication, Information, Action, and Organization.
A Unifying Translation of Natural Language Patterns to Object and Process Modeling.
An Information Management Environment Based on the Model of Object Primitives.
Designing Model-Based Intelligent Dialogue Systems.
Conceptual Modeling Process and the Notion of a Concept.
Integrating Fact-oriented Modeling with Object-oriented Modeling.
A Language/action Based Approach to Information Modeling.
From Information Model to Controllable Implementation.
Modeling of Customers' Interactive Control of Service Processes.
On the Convergence of Analysis and Design Methods for Multi-agent, Component-based and Object-oriented systems.
Requirements for Web Engineering Methodologies.
A Genre-Based Method for Information Systems Planning.
Metrics for Managing Quality in Information Modeling.
Preface.
Object-Oriented Web Applications Modeling.
Information Modeling in the Internet Age - Challenges, Issues, and Research Directions.
Conceptual Web Site Modeling.
Information Models for Document Engineering.
Audience-driven Web Design.
A Systematic Relationship Analysis for Modeling Information Domains.
HMT: Modeling Interactive and Adaptive Hypermedia Applications.
Mapping UML Techniques to Design Activities.
Seamless Formalizing the UML Semantics through Metamodels.
The Whole-Part Relationship in the Unified Modeling Language: A New Approach.
Temporal OCL Meeting Specification Demands for Business Components.
A Systematic Approach to Transform UML Static Models to Object-Oriented Code.
An Interactive Viewpoint on the Role of UML.
Supplementing UML with concepts from ORM.
Systematic Design of Web Applications with UML.
RUP - A process model for working with UML.
Extension of the Unified Modeling Language for Mobile Agents.
Using a Semiotic Framework to Evaluate UML for the Development of Models of High Quality.
Rendering Distributed Systems in UML.
Linking UML with Integrated Formal Techniques.
Data Modeling And UML.
Rational Unified Process and Unified Modeling Language - A GOMS Analysis.
Preface.
UML Modeling Support for Early Reuse Decisions in Component-Based Development.
Modeling of Business Rules for Active Database Application Specification.
Active database applications require the classic cycle of analysis, design, prototyping and implementation. During analysis and design steps of the information system engineering process, modeling behavior is an important task. This task is both essential and crucial when information system is centered on active databases, which allow the replacement of parts of application programs with active rules. For that reason, the specification of business rules during analysis and design steps becomes an actual requirement. Business rules ensure the well-functioning of information system. They are descriptive (integrity constraints) or functional (derivation rules and active rules). To relieve programmers from using either traditional or ad hoc techniques to design active databases, it is necessary to develop new techniques to model business rules. These techniques have to enhance the specification of dynamic aspect through a high-level description language able to express precisely and completely rule semantic. In this chapter, we propose a uniform approach to model business rules (active rules, integrity constraints, etc.). To improve the behavior specification we extend the state diagrams that are widely used for dynamic modeling. This extension is a transformation of state transitions according to rule semantics. In addition, we outline new functionalities of Computer-Aided System Engineering (CASE) to take into consideration the active database specificities. In this way, the designer can be assisted to control, maintain and reuse a set of rules.
CMU-WEB: A Conceptual Model with Metrics for Testing and Designing Usability in Web Applications.
With the ubiquitous availability of browsers and Internet access, the last few years have seen a tremendous growth in the number of applications being developed on the World Wide Web (WWW). Models for analyzing and designing these applications are only just beginning to emerge. In this work, we propose a three-dimensional classification space for WWW applications, consisting of a degree of structure of pages dimension, a degree of support for interrelated events dimension and a location of processing dimension. Next, we propose usability design metrics for WWW applications along the structure of pages dimension. To measure these, we propose CMU-WEB-a conceptual model that can be used to design WWW applications, such that its schema provide values for the design metrics. This work represents the first effort, to the best of our knowledge, to provide a conceptual model that measures quantifiable metrics that can be used for the design of more usable Web applications, and that can also be used to compare the usability of existing Web applications, without empirical testing.
Enforcing Cardinality Constraints in the ER Model with Integrity Methods.
Entity-Relationship (ER) schemas include cardinality constraints that restrict the dependencies among entities within a relationship type. The cardinality constraints have direct impact on application transactions, since insertions or deletions of entities or relationships might affect related entities. Application transactions can be strengthened to preserve the consistency of a database with respect to the cardinality constraints in a schema. Yet, once an ER schema is translated into a logical database schema, the direct correlation between the cardinality constraints and application transaction is lost, since the components of the ER schema might be decomposed among those of the logical database schema.We suggest extending the Enhanced-ER (EER) data model with integrity methods that can enforce the cardinality constraints. The integrity methods can be fully defined by the cardinality constraints, using a small number of primitive update methods, and are automatically created for a given EER diagram. A translation of an EER schema into a logical database schema can create integrity routines by translating the primitive update methods alone. These integrity routines may be implemented as database procedures, if a relational DBMS is utilized, or as class methods, if an object-oriented DBMS is utilized.
Algorithm Development, Simulation Analysis, and Parametric Studies for Data Allocation in Distributed Database Systems.
In a distributed database system, an increase in workload typically necessitates the installation of additional database servers followed by the implementation of expensive data reorganization strategies. We present the Partial REALLOCATE and Full REALLOCATE heuristics for efficient data reallocation. Complexity is controlled and cost minimized by allowing only incremental introduction of servers into the distributed database system. Using first simple examples and then, a simulator, our framework for incremental growth and data reallocation in distributed database systems is shown to produce near optimal solutions when compared with exhaustive methods.
Object-Oriented Database Benchmarks.
The Role of Use Cases in the UML: A Review and Research Agenda.
A use case is a description of a sequence of actions constituting a complete task or transaction in an application. Use cases were first proposed by Jacobson (1987) and have since been incorporated as one of the key modeling constructs in the UML(Booch, Jacobson, & Rumbaugh, 1999) and the Unified Software Development Process(Jacobson, Booch, & Rumbaugh, 1999). This chapter traces the development of use cases, and identifies a number of problems with both their application and theoretical underpinnings. From an application perspective, the use-case concept is marked by a high degree of variety in the level of abstraction versus implementation detail advocated by various authors. In addition, use cases are promoted as a primary mechanism for identifying objects in an application, even though they focus on processes rather than objects. Moreover, there is an apparent inconsistency between the so-called naturalness of object models and the commonly held view that use cases should be the primary means of communicating and verifying requirements with users. From a theoretical standpoint, the introduction of implementation issues in use cases can be seen as prematurely anchoring the analysis to particular implementation decisions. In addition, the fragmentation of objects across use cases creates conceptual difficulties in developing a comprehensive class diagram from a set of use cases. Moreover, the role of categorization in human thinking suggests that class diagrams may serve directly as a good mechanism for communicating and verifying application requirements with users. We conclude by outlining a framework for further empirical research to resolve issues raised in our analysis.
Object-Process Methodology Applied to Modeling Credit Card Transactions.
Object-Process Methodology (OPM) is a system development and specification approach that combines the major system aspects-function, structure and behavior-within a single graphic and textual model. Having applied OPM in a variety of domains, this chapter specifies an electronic commerce system in a hierarchical manner, at the top of which are the processes of managing a generic product supply chain before and after the product is manufactured. Focusing on the post-product supply chain management, we gradually refine the details of the fundamental, almost "classical" electronic commerce interaction between the retailer and the end-customer, namely payment over the Internet using the customer's credit card. The specification results in a set of Object-Process Diagrams and a corresponding equivalent set of Object-Process Language sentences. The synergy of combining structure and behavior within a single formal model, expressed both graphically and textually, yields a highly expressive system modeling and specification tool. The comprehensive, unambiguous treatment of this basic electronic commerce process is formal, yet intuitive and clear, suggesting that OPM is a prime candidate for becoming a common standard vehicle for defining, specifying, and analyzing electronic commerce and supply chain management systems.
Information Analysis in UML and ORM: A Comparison.
Since its adoption by the Object Management Group as a language for object-oriented analysis and design, the Unified Modeling Language (UML) has become widely used for designing object-oriented code. However, UML has had only minimal adoption among practitioners for the purposes of information analysis and database design. One main reason for this is that the class diagrams used in UML for data modeling provide only weak, and awkward, support for the kinds of business rules found in data-intensive applications. Moreover, UML's graphical language does not lend itself readily to verbalization and multiple instantiation for validating data models with domain experts. These defects can be remedied by using a fact-oriented approach for information analysis, from which UML class diagrams may be derived. Object-Role Modeling (ORM) is currently the most popular fact-oriented modeling approach. This chapter examines the relative strengths and weaknesses of UML and ORM for conceptual data modeling, and indicates how models in one notation can be translated into the other.
Cooperative Query Processing via Knowledge Abstraction and Query Relaxation.
As database users adopt a query language to obtain information from a database, a more intelligent query answering system is increasingly needed that cooperates with the users to provide informative responses by understanding the intent behind a query. The effectiveness of decision support would improve significantly if the query answering system returned approximate answers rather than a null information response when there is no matching data available. Even when exact answers are found, neighboring information is still useful to users if the query is intended to explore some hypothetical information or abstract general fact. This chapter proposes an abstraction hierarchy as a framework to practically derive such approximate answers from ordinary everyday databases. It provides a knowledge abstraction database to facilitate the approximate query answering. The knowledge abstraction database specifically adopts an abstraction approach to extract semantic data relationships from the underlying database, and uses a multi-level hierarchy for coupling multiple levels of abstraction knowledge and data values. In cooperation with the underlying database, the knowledge abstraction database allows the relaxation of query conditions so that the original query scope can be broadened and thus information approximate to exact answers can be obtained. Conceptually abstract queries can also be posed to provide a less rigid query interface. A prototype system has been implemented at KAIST and is being tested with a personnel database system to demonstrate the usefulness and practicality of the knowledge abstraction database in ordinary database systems.
Ternary Relationships: Semantic Requirements and Logically Correct Alternatives.
A Case Study of the Use of the Viable System Model in the Organization of Software Development.
This chapter considers the usefulness of the Viable System Model (VSM) in the study of organizational adaptation. The VSM is a rigorous organizational model that was developed from the study of cybernetics and has been given considerable attention by management science research. The chapter presents a longitudinal case study that focuses upon a software development team. The VSM was useful in diagnosing the likely consequences of different organizational designs and in prescribing an alternative solution.
Using Weakly Structured Documents at the User-Interface Level to Fill in a Classical Database.
Electronic documents have become a universal way of communication due to Web expansion. But using structured information stored in databases is still essential for data coherence management, querying facilities, etc. We thus face a classical problem-known as "impedance mismatch" in the database world; two antagonist approaches have to collaborate. Using documents at the end-user interface level provides simplicity and flexibility. But it is possible to take documents as data sources only if helped by a human being; automatic document analysis systems have a significant error rate. Databases are an alternative as semantics and format of information are strict; queries via SQL provide 100% correct responses. The aim of this work is to provide a system that associates document capture freedom with database storage structure.The system we propose does not intend to be universal. It can be used in specific cases where people usually work with technical documents dedicated to a particular domain. Our examples concern medicine and more explicitly medical records. Computerization has very often been rejected by physicians because it necessitates too much standardization and form-based user interfaces are not easily adapted to their daily practice. In this domain, we think that this study provides a viable alternative approach. This system offers freedom to doctors; they would fill in documents with the information they want to store, in a convenient order and in a freer way. We have developed a system that allows a database to fill in quasi-automatically from documents paragraphs.The database used is an already existing database that can be queried in a classical way for statistical studies or epidemiological purposes. In this system, the document fund and the database containing extractions from dccuments coexist. Queries are sent to the database, answers include data from the database and references to source documents.
Extending UML for Space- and Time-Dependent Applications.
Changing the Face of War through Telemedicine and Mobile E-commerce.
FOOM - Functional and Object-Oriented Methodology for Analysis and Design of Information Systems.
FOOM is an integrated methodology for analysis and design of information systems, which combines the two essential software-engineering paradigms: the functional- (or process-) oriented approach and the object-oriented (OO) approach. In FOOM, system analysis includes both functional and data modeling activities, thereby producing both a functional model and a data model. These activities can be performed either by starting with functional analysis and continuing with data modeling, or vice versa. FOOM products of the analysis phase include:a)a hierarchy of OO-DFDs (object-oriented data flow diagrams), and b) an initial object schema, which can be created directly from the user requirements specification or from an entity-relationship diagram (ERD) that is mapped to that object schema. System design is performed according to the OO approach. The products of the design phase include: a) a complete object schema, consisting of the classes and their relationships, attributes, and method interfaces; b) object classes for the menus, forms and reports; and c) a behavior schema, which consists of detailed descriptions of the methods and the application transactions, expressed in pseudo-code and message diagrams. The seamless transition from analysis to design is attributed to ADISSA methodology, which facilitates the design of the menus, forms and reports classes, and the system behavior schema, from DFDs and the application transactions.
The Psychology of Information Modeling.
Information modeling is the cornerstone of information systems analysis and design. Information models, the products of information modeling, not only provide the abstractions required to facilitate communication between the analysts and end-users, but they also provide a formal basis for developing tools and techniques used in information systems development. The process of designing, constructing, and adapting information modeling methods for information systems development is known as method engineering. Despite the pivotal role of modeling methods in successful information systems development, most modeling methods are designed based on common sense and intuition of the method designers with little or no theoretical foundation or empirical evidence. Systematic scientific approach is missing! This chapter proposes the use of cognitive psychology as a reference discipline for information modeling and method engineering. Theories in cognitive psychology are reviewed in this chapter and their application to information modeling and method engineering is discussed.
How Complex Is the Unified Modeling Language?
Unified Modeling Language (UML)has emerged as the software industry's dominant modeling language. It is the de facto modeling language standard for specifying, visualizing, constructing, and documenting the components of software systems. Despite its prominence and status as the standard modeling language, UML has its critics. Opponents argue that it is complex and difficult to learn. Some question the rationale of having nine diagramming techniques in UML and the raison d'être of those nine techniques in UML. Others point out that UML lacks a comprehensive methodology to guide its users, which makes the language even more convoluted. A few studies on UML can be found in the literature. However, no study exists to provide a quantitative measure of UML complexity or to compare UML with other object-oriented techniques. In this research, we evaluate the complexity of UML using complexity metrics. The objective is to provide a reliable and accurate quantitative measure of UML complexity. A comparison of the complexity metrical values of UML with other object-oriented techniques was also carried out. Our findings suggest that each diagram in UML is not distinctly more complex than techniques in other modeling methods. But as a whole, UML is very complex-2-11 times more complex than other modeling methods.
Managing Organizational Hypermedia Documents: A Meta-information System.
Recently, many organizations have attempted to build hypermedia systems to expand their working areas into Internet-based virtual work places. Increasingly, it becomes more important than ever to manage organizational hypermedia documents (OHDs); metadata plays a critical role for managing these documents. This chapter redefines metadata roles and proposes a metadata classification and the corresponding metadata schema for OHDs. Furthermore, a meta-information system, HyDoMiS (Hyperdocument Meta-information System) built on the basis of this schema is proposed. HyDoMiS performs three functions: metadata management, search, and reporting. The metadata management function is concerned with workflow, documents, and databases. The system is more likely to help implement and maintain hypermedia information systems effectively.
Formal Approaches to Systems Analysis Using UML: An Overview.
Implementation Techniques For Extensible Object Storage Systems.
A Review of Experiments on Natural Language Interfaces.
A Framework for Analyzing Mobile Transaction Models.
Methodology Evaluation Framework for Component-Based System Development.
Considering Mobility in Query Processing for Mobile Commerce Systems.
A Run-Time Based Technique to Optimize Queries in Distributed Internet Databases.
Towards Flexible Specification, Composition, and Coordination of Workflow Activities.
On The Representation Of Temporal Dynamics.
Software Agents for Mobile Commerce Services Support.
The Development of Ordered SQL Packages in Peer-to-Peer Warehousing Environment.
Performance Implication of Knowledge Discovery Techniques in Databases.
Applying UML For Designing Multidimensional Databases And OLAP Applications.
Meta-model Based Information Mediation.
Federated Process Framework for Transparent Process Monitoring in Business Process Outsourcing.
Fuzzy Aggregations and Fuzzy Specializations in Eindhoven Fuzzy EER Model.
Using DEMO and ORM in Concert: A Case Study.
Improving the Understandability of Dynamic Semantics: An Enhanced Metamodel for UML State Machines.
Online Analytic Mining for Web Access Patterns.
Modeling Motion: Building Blocks of a Motion Database.
Comparing Metamodels for ER, ORM and UML Data Models.
Regression Test Selection for Database Applications.
Framework for the Rapid Development of Modeling Environments.
Revisiting Workflow Modeling with Statecharts.
Normalization of Relations with Nulls in Candidate Keys: Traditional and Domain Key Normal Forms.
Metrics for Workflow Design: How an Information Processing View on Business Processes Helps to Make Good Designs.
Preface.
Agile Development Methods and Component-Orientation: A Review and Analysis.
An Evaluation Framework for Component-Based and Service-Oriented System Development Methodologies.
Toward an Extended Framework for Human Factors Research on Data Modeling.
An Attempt to Establish a Correspondence between Development Methods and Problem Domains.
Evaluation of Component-Based Development Methods.
Two Meta-Models for Object-Role Modeling.
Comprehension of Hierarchical ER Diagrams Compared to Flat ER Diagrams.
Analyzing and Comparing Ontologies with Meta-Methods.
Participatory Development of Enterprise Process Models.
Constraints on Conceptual Join Paths.
Evaluating Conceptual Coherence in Multi-Modeling Techniques.
A Comparison of the FOOM and OPM Methodologies for User Comprehension of Analysis Specifications.
An Empirical Investigation of Requirements Specification Languages: Detecting Defects While Formalizing Requirements.
Goal Modeling in Requirements Engineering: Analysis and Critique of Current Methods.
Assessing Enterprise Modeling Languages Using a Generic Quality Framework.
Preface.
Validating an Evaluation Framework for Requirements Engineering Tools.
An Approach for Evolution-Driven Method Engineering.
A Taxonomic Class Modeling Methodology for Object-Oriented Analysis.
A Service-Oriented Component Modeling Approach.
Using a Semiotic Framework for a Comparative Study of Ontology Languages and Tools.
Using Logic for Querying XML Data.
Ubiquitous Access to Web Databases.
Protecting Datasources over the Web: Policies, Models, and Mechanisms.
Practical Case Study of a Web-based Tutor Payment System.
Web Content Management and Dynamic Web Pages - A Tutorial.
Cache Management for Web-Powered Databases.
Database-Driven Product Catalog System.
The Development of On-line Tests Based on Multiple Choice Questions.
Web Mining to Create a Domain Specific Web Portal Database.
CODAR - A POA-based CORBA Database Adaptor for Web Service Infrastructures.
Web-Powered Databases: The Low Level in C++.
Effective Databases for Text & Document Management.
Database Integrity: Challenges and Solutions
Encyclopedia of Information Science and Technology (5 Volumes)
Encyclopedia of Database Technologies and Applications
Multidimensional Databases: Problems and Solutions
Information Modeling in the New Millennium
Unified Modeling Language: Systems Analysis, Design and Development Issues
Advanced Topics in Database Research, Vol. 1
Advanced Topics in Database Research, Vol. 2
Advanced Topics in Database Research, Vol. 3
Information Modeling Methods and Methodologies
Spatial Databases: Technologies, Techniques and Trends
Web-Powered Databases
Advances in Real-Time Systems.
Ein Datenbankkern zur Speicherung variabel strukturierter Feature-Terme. Implementierungstechniken.
Eine fallbasierte Lernkomponente als integraler Bestandteil der MOLTKE-Werkbank zur Diagnose technischer Systeme.
Oberflächenbasierte Segmentierung von Tiefenbildern.
Ein planbasierter Ansatz zur Generierung multimedialer Präsentationen.
Effiziente Pufferverwaltung in parallelen relationalen Datenbanksystemen.
Operationalisierung des Modells der Expertise mit KARL.
Integration of Active and Deductive Database Rules.
Indexierung und Retrieval von Feature-Bäumen am Beispiel der linguistischen Analyse von Textkorpora.
Inkrementelle wörterbuchbasierte Wortschatzerweiterungen in sprachverarbeitenden Systemen. Entwurf einer konstruktiven Lexikonkonzeption.
Parallele Modelle für Deduktionssysteme.
HAMVIS: Generierung von Visualisierungen in einem Rahmensystem zur systematischen Entwicklung von Benutzungsschnittstellen.
Ein modellgestütztes Analysesystem zum Bildverstehen strukturierter Dokumente.
Verwendung von Bildauswertungsmethoden zur Erkennung und Lagebestimmung von generischen polyedrischen Objekten im Raum.
Implementierung einer effizienten Anfrageauswertung für ein deduktives Datenbanksystem.
Fehlertolerante und effiziente automatische Analyse digitalisierter Dokumente zur Gewinnung von Hypertextstrukturen.
Effizientes Problemlösen durch flexible Wiederverwendung von Fällen auf verschiedenen Abstraktionsebenen.
Modellkonstruktion in MIKE. Methoden und Werkzeuge.
Steuerungsansätze auf der Basis Neuronaler Netze für sechsbeinige Laufmaschinen.
Validierung konzeptueller Schemata.
Fuzzy-Techniken in objektorientierten Datenbanksystemen zur Unterstützung von Entwurfsprozessen.
Pragmatische Programmsynthese.
Dear KV, I hope you don't mind if I ask you about a non-work-related problem, though I guess if you do mind you just won't answer. I work on an open source project when I have the time, and we have some annoying nontechnical problems. The problems are really people, and I think you know the ones I mean: people who constantly fight with other members of the project over what seem to be the most trivial points, or who contribute very little to the project but seem to require a huge amount of help for their particular needs. I find myself thinking it would be nice if such people just went away, but I don't think starting a flame war on our mailing lists over these things would really help. Any thoughts on this nontechnical problem?
Ein Informationsmodell für Ableitungsprozesse und ihre Ergebnisse im Wissensgewinnungsprozeß.
Separierung und Resolution multipler Perspektiven in der konzeptuellen Modellierung.
Erweiterung der Wissensbasierten CAD-Konstruktion um Restriktionsnetztechniken.
Zur Darstellung und Verarbeitung von Wissen über Himmelsrichtungen.
Adaptive Neuronale Netze und ihre Anwendung als Modelle der Entwicklung kortikaler Karten.
Das 3-stufige Frame-Repräsentationsschema - eine mehrdimensional modulare Basis für die Entwicklung von Expertensystemkernen.
Extraktion von linienförmigen Merkmalen und Ermittlung des optischen Flusses mit seinen Ableitungen aus Voll- und Halbbildfolgen.
Verfeinerung in objektorientierten Spezifikationen.
Analyse und Optimierung von Indexstrukturen in Geo-Datenbanksystemen.
Externe Schemata in objektorientierten Datenbansystemen.
Erklärungsbasiertes Computer-Sehen von Bildfolgen.
Zur natürlichsprachlichen interaktiven Unterstützung im Datenbankentwurf.
Optimierung deklarativer Anfragen in Objektbanken.
Die Bildanalysesprache TRIAS.
Deduktion mit Shannongraphen für Prädikatenlogik erster Stufe.
Parallele Suche mit randomisiertem Wettbewerb in Inferenzsystemen.
Erklärungen für komplexe Wissensbasen.
Wissensbasierte Verfahren zur Synthese mathematischer Beweise: Eine kombinatorische Anwendung.
Definition of Behavior in Object-Oriented Databases by View Integration.
Schematransformationen in Datenbanken.
Ein Regelsystem zur Integrtätssicherung in aktiven relationalen Datenbanksystemen.
Verteiltes und kooperatives Planen in einer flexiblen Fertigungsumgebung.
Eine parallele Architektur zur inkrementellen Generierung multimodaler Dialogbeiträge.
View Integration für objektorientierte Datenbanken.
Grammatikentwicklung mit Constraint Logik Programmierung. Implementierung einer Grammatik für das Deutsche mit PROLOG III.
Replikationsmanagement in verteilten Informationssystemen.
A Tour on TriGS. Devolopment of an Active System and Application of Rule Patterns for Active Database Design.
Optimierung von Speicherzugriffskosten in Objektbanken: Clustering und Prefetching.
Transaction Services for Knowledge Base Management Systems. Modeling Aspects, Architectural Issues, and Realization Techniques.
Diagnosis and Repair of Constraint Violations in Database Systems.
Föderierte Datenbanktechnologie für die Molekularbiologie: Konzepte, Einsatz und Nutzen.
Das digital lecture board. Konzeption, Design und Entwicklung eines Whiteboards für synchrones Teleteaching.
DB-gestützte Kooperationsdienste für technische Entwurfsanwendungen.
Bedarfsorientierte Dienstvermittlung in vernetzten Systemen.
Materialisation and Parallelism in the Mapping of an Object Model to a Relational Multi-processor System.
Flexible Kontrolle in Expertensystemen zur Planung und Konfigurierung in technischen Domänen.
Wissensbasiertes Lösen von Ablaufplanungsproblemen durch explizite Heuristiken.
Ein Basisdienst für die zuverlässige Abwicklung langdauernder Aktivitäten.
Interoperabilität von Datenbanksystemen bei struktureller Heterogenität. Architektur, Beschreibungs- und Ausführungsmodell zur Unterstützung der Integration und Migration.
Konzeptionelle Modellierung von Informationssystemen als verteilte Objektsysteme.
Trabsaktionsverwaltung in heterogenen, föderierten Datenbanksystemen.
Entwurf einer Sprache für die verhaltensorientierte konzeptionelle Modellierung von Informationssystemen.
Schemaintegration für den Entwurf föderierter Datenbanken.
Kommunikationsoptimierung für mobile Agenten durch hierarchisches Klonen.
Erwartungsgestützte Analyse medizinischer Befundungstexte. Ein wissensbasiertes Modell zur Sprachverarbeitung.
Einheitliche Theorie für korrekte parallele und fehlertolerante Ausführung von Datenbanktransaktionen.
Architektur verteilter Workflow-Management-Systeme.
ALCP - Ein hybrider Ansatz zur Modellierung von Unsicherheit in terminologischen Logiken.
Das Konzept der Transaktionshülle zur konsistenten Spezifikation von Abhänigigkeiten in komplexen Anwendungen.
Kognitives Parsing: Repräsentation und Verarbeitung sprachlichen Wissens.
Dynamische, situationsbezogene Hypertext-Handbücher für komplexe Tätigkeiten.
Effiziente Konsistenzprüfunf in Datenbanksystemen.
MoKon - Ein Ansatz zur Wissensbasierten Konfiguration von Variantenerzeugnissen.
Skalierbare Multicast-Kommunikation in Weitverkehrsnetzen.
Source-to-Source Transformationen zur Erklärung des Programmverhaltens bei deduktiven Datenbanken.
Symbolisches Lösen mathematischer Probleme durch Kooperation algorithmischer und logischer Systeme.
Situationsanalyse bei Kontakten während der Ausführung von Roboterbewegungen in unsicheren Umgebungen.
Begleitende Montageablaufplanung für ein sensorgestütztes Zweiarm-Manipulatorsystem.
Sichtenmanagement in Client-Server-Systemen.
Dynamische Modularisierung lexikalischer Informationen in einem Wiederverwendungsszenario.
Wissensbasierte Analyse medizinischer Bilder - das Biotop-Verfahren.
Benutzeranpaßbare semantische Sprachanalyse und Begriffsrepräsentation für die medizinische Dokumentation.
Heuristic and Randomised Optimisation Techniques in Object-Oriented Database Systems.
Änderungskontrolle in deduktiven Datenbanken.
Verdeckungen und spezielle Sichten bei der Polyederrekonstruktion.
Access Control in Object-Oriented Federated Database Systems.
Generierung domänenspezifischer Wissensrepräsentationssysteme und Transformation von Wissensbasen mit einer Anwendung in der Rechtsinformatik.
Interaktive Akquisition elementarer Roboterfähigkeiten.
Modulare Problemlösungsarchitekturen für Konstruktionssysteme.
Toleranz- und Kongruenzrelationen in relationalen Datenbanken.
Modellierung von Expertise über Konfigurierungsaufgaben.
Reduktion zur effizienten Programmierung von Methoden in deduktiven objektorientierten Datenbanken.
An Approach to Query Processing in Advanced Database Systems.
Metadata-Based Middleware for Integrating Information Systems.
Semantic Integrity Constraints in Federated Database Schemata.
MODEL-K: Modellierung und Operationalisierung von Selbsteinschätzung und -steuerung durch Reflexion und Metawissen.
EMSY - Ein Modellierungskonzept für ökologische und biologische Systeme unter besonderer Berücksichtigung ihrer dynamischen Veränderung.
Akquisition von Integritätsbedingungen in Datenbanken.
Inkrementelle, domänenunabhängige Thesauruserstellung in dokumentbasierten Informationssystemen durch Kombination von Konstruktionsverfahren.
Unterstützung anwendungsspezifischer Zugriffsprofile in Objektbanksystemen.
Dynamische Beziehungen als Modell der Kooperation in verteilten Objektsystemen.
Eine mehrschichtige Architektur zur Fehlerdiagnose und Fehlerbehebung bei der Entwicklung logischer Programme.
Entwurf eines objektorientierten Datenbankmodells Für relationale Datenbanksysteme.
Vorgangsmodelle. Ein Ansatz zur Repräsentation und Analyse zeitabhängigen Verhaltens bei der Überwachung und Diagnose technischer Systeme.
Terminierung und Konfluenz in einer aktiven objektorientierten Datenbank.
Integration von Aktionsplanung und Konfigurierung.
Distributed Machine Learning.
Detektion, Verfolgung und Klassifikation bewegter Objekte in monokularen Bildfolgen am Beispiel von Straßenverkehrsszenen.
Produkorientierte automatische Planung von Prüfoperationen bei der robotergestützten Montage.
Ein Verfahren zur semi-automatischen Generierung von Mediatorspezifikationen.
Wissensbasierte Verkehrsszenenanalyse zur Fahrerunterstützung.
Replikation in vernetzten Systemen mit mobilen Teilnehmern.
Funktionsorientiertes Management heterogener ATM-Netzwerke.
Control Mechanisms in Distributed Object Bases. Synchronization, Deadlock Detection, Migration.
Zeitrepräsentation und merkmalsgesteuerte Suche zur Terminplanung.
Aktive Mechanismen zur Konsistenzsicherung in Föderationen heterogener und autonomer Datenbanken.
Modellbildung und Architektur von verteilten Workflow-Management-Systemen.
Flexible Steuerung eines sprachverstehenden Systems mit homogener Wissensbasis.
Wissensbasierte Greifplanung für Mehrfinger-Roboterhände.
Entwicklung verteilter Objektstrukturen für skalierbare und hochgradig verfügbare Informationssysteme.
Konzeption eines regelbasierten Systems für Produktdatenmanagement in CIM.
Digitale und Allgemeine Topologie in der bildhaften Wissensrepräsentation.
Einbettung von Konzepthierarchien in ein Deduktives Datenbanksystem.
Typsichere Objektbankmigration.
MISTRAL: Processing Relational Queries using a Multidimensional Access Technique.
Wissensintensives Lernen für zeitkritische technische Diagnoseaufgaben.
Selbstorganisierende neuronale Netzwerkmodelle zur Bewegungssteuerung.
Lernen durch Genetisch-Neuronale Evolution: Aktive Anpassung an unbekannte Umgebungen mit selbstentwickelten parallelen Netzwerken.
Schema-unabhängige Anfragesprachen für relationale Datenbanken.
Qualitative Analyse im Rahmen qualitativen und modellbasierten Schließens.
Ein Dialogmodul für ein Spracherkennungs- und Dialogsystem.
Cyclops - Wissensbasierte Bildanalyse in der Medizin.
Hypermediabasiertes Knowledge Engineering für verteilte wissensbasierte Systeme.
Anfrageoptimierung in objektrelationalen Datenbanken durch kostenbedingte Termersetzungen
Dynamische semantische Netze - Zur Kontextabhängigkeit von Wortbedeutungen.
Extending the Stratification Approach for Deductive Database Systems.
Wissensbasierte Synthese von Bildanalyseprogrammen.
Adaptive Roboterkontrolle mit konnektionistischen Systemen.
Design and Implementation of a Database Programming Language for XML-based Applications.
Towards the Intelligent On-line Home Office.
The IT Leap and the Sektornet.
Technology as the Catalyst of Users' Acceptance in Electronic Commerce.
Introduction by the Editors.
ACCORD Solutions for Flexible Working.
Flexible Quality-of-Service Technology for Supporting Voice/Data-Integrated Nomadic Networking.
The Telco's Goldmine - 50 Mb/s to the General Public over the Local Telephone Lines.
Support Services for Business Process Oriented Telework. The COBIP Project.
New Ways of Flexible Working: The Contribution of ACTS, and Issues for IST.
Interoperability for Multimedia Services: The Approach of the Standards Bodies - An Overview Based on ACTS Guideline SII G06.
Net Working in the Knowledge Economy.
Internet Service Architectures and ATM - The ELISA Approach.
Developing a Reference Model for Networked Flexible Work through Industry Trials.
New Business Paradigms.
UMPTIDUMPTI - Anytime, Anywhere, Anybody - UMTS for ALL.
Giving Mobile Users Access to Net-Based Services - A Mobile Agent Approach.
Gestalt - On-line Education and Training - State of the Art.
TECODIS. The Experience of Teleworking for Software Developers.
Flexible Working Technology for Sustainable Development and Social Inclusion.
On Modeling Conformance for Flexible Transformation over Data Models.
Knowledge Representation and Transformation in Ontology-based Data Integration.
The 'Family of Languages' Approach to Semantic Interoperability.
UML for the Semantic Web: Transformation-Based Approaches.
Tracing Data Lineage Using Schema Transformation Pathways.
Transforming UML Domain Descriptions into Configuration Knowledge Bases.
Transforming Data Models with UML.
Schema Conversion Methods between XML and Relational Models.
RDFT: A Mapping Meta-Ontology for Web Service Integration.
A Logic Programming Approach to RDF Document and Query Transformation.
Ontology Extraction for Distributed Environments.
Knowledge Transformation for the Semantic Web
Machine Reconstruction of Human Control Strategies
Flexible Working - New Network Technologies
Natural Language Processing for Online Applications: Text Retrieval, Extraction & Categorization
WFMS: The Next Generation of Distributed Processing Tools.
Semantic-Based Decomposition of Transactions.
An Extensible Approach to Realizing Advanced Transaction Models.
The Reflective Transaction Framework.
Customizable Concurrency Control for Persistent Java.
Inter- and Intra-Transaction Parallelism for Combined OLTP/OLAP Workloads.
Transaction Optimization Techniques.
Towards Distributed Real-Time Concurrency and Coordination Control.
Flexible Commit Protocols for Advanced Transaction Processing.
Toward Formalizing Recovery of (Advanced) Transactions.
ConTracts Revisited.
Transaction Processing in Broadcast Disk Environments.
Transactions in Transactional Workflows.
Logical Handling of Inconsistent and Default Information.
Approximate Reasoning Systems: Handling Uncertainty and Imprecision in Information Systems.
An Introduction to the Fuzzy Set and Possibility Theory-Based Treatment of Flexible Queries and Uncertain or Imprecise Databases.
Uncertainty in Intelligent Databases.
A Bibliography on Uncertainty Management in Information Systems.
Probabilistic and Bayesian Representations of Uncertainty in Information Systems: A Pragmatic Introduction.
Uncertain, Incomplete, and Inconsistent Data in Scientific and Statistical Databases.
On the Classification of Uncertainty Techniques in Relation to the Application Needs.
Introduction.
Sources of Uncertainty, Imprecision, and Inconsistency in Information Systems.
Knowledge Discovery and Acquisition From Imperfect Information.
Imperfect Information: Imprecision and Uncertainty.
The Transferable Belief Model for Belief Representation.
Uncertainty in Information Retrieval Systems.
Imperfect Information in Relational Databases.
STORM: An Object-Oriented Multimedia DBMS.
Synchronization and User Interaction in Distributed Multimedia Presentation Systems.
Database Approach for the Management of Multimedia Information.
A Visual Multimedia Query Language for Temporal Analysis of Video Data.
A Multimedia Query Specification Language.
Searching and Browsing a Shared Video Database.
Load-Balanced Data Placement for Variable-Rate Continuous Media Retrieval.
Multimedia DBMS - Reality of Hype?
Model for Interactive Retrieval of Videos and Still Images.
Playout Management in Multimedia Database Systems.
An Object-Oriented Modeling of Multimedia Database Objects and Applications.
Integration of Simulation and Multimedia in Automatically Generated Internet Courses.
A Visual Simulation Environment for MIPS Based on VHDL.
An Authoring Environment for the SimulNet Educational Platform.
BabelWin: An environment for learning and monitoring reading and writing skills.
An Internet Distance-Learning Operating Model.
Synchronous Drawing Actions in Environments of Collaborative Learning of Design.
Interconnecting Courseware Modules via WWW.
APRISA: A Tool for Teaching the Interconnection of Open Systems.
Adaptive Internet-based learning with the TANGOW system.
Interactive Mathematics Teaching with Mathedu.
Teaching Support Units.
The Interactive Physics Course on the Internet.
Task Based Training of Application Users.
SUMA Project (Open Murcia University Services).
Introducing Waves Using Simulations Controled From Html Files.
Using Bayesian Networks in Computerized Adaptive Tests.
XML-based Integration of Hypermedia Design and Component-Based Techniques in the Production of Educational Applications.
Computers in Education: the Near Future.
Learning Basque in a Distance-Adaptive way.
Collaborative planning for problem solution in distance learning.
High Level Design of Web-Based Environments for Distance Education.
M.A.C. A Hypermedia System for Learning Electronics.
Computer-Human Learning.
Natural Language Processing in Educational Computer Science.
Educational Web Sites: Some Issues for Evaluation.
Building a Virtual Learning Environment Using Agents.
Approach to Intelligent Adaptive Testing.
Evaluation Criteria for Hypermedia Educational Systems.
Using teachers as heuristics evaluators of educational software interfaces.
Creating Collaborative Environments for Web-based Training Scenarios.
The Next Step in Computer Based Education: the Learning Technologies Standardisation.
Advanced Learning Environments: Changed Views and Future Perspectives.
Development of Didactic Resources for Distance Learning based on Simulation.
TutorMap. Past, Present and Future of a Mathematics Tutoring System.
Learning Communities in the Web: Concepts and strategies.
Symbolic Calculus Training by Means of MathTrainer.
EJS: An Authoring Tool to Develop Java Applications.
ADAM CASE. Using upper CASE tools in Software Engineering Laboratory.
SICAS. Interactive system for algorithm development and simulation.
Simurob and JRF. Teaching tools for robot simulation and programming.
Collares Ortofónicos. System for the training of the suprasegmental parameters of intensity and rhythm of the articulated sound.
EXercita. A System for Archiving and Publishing Programming Exercises.
Interactive Design of Adaptive Courses.
Using Simulation and Virtual Reality for Distance Education.
ED68K. A design framework for the development of digital systems based on MC68000.
An Experience on Virtual Teaching: AulaNet.
Guided Collaborative Chess Tutoring through Game History Analysis.
Using Analysis, Design and Development of Hypermedia Applications in the Educational Domain.
Ubiquitous Computing and Collaboration. New interaction paradigms in the classroom for the 21st Century.
Shared Whiteboard Manager and Student Notebook for the PLAN-G Telematic Platform.
Creation of a Multimedia System for Learning about Oscillations.
Guiding the user. An element to aid knowledge construction in adaptive hypermedia systems.
Pedagogical Strategies With Hypermedia. Limiting access to hyperspace for educational purposes.
Artificial Intelligence in the HyperClass: Design Issues.
Improving the Language Mastery through Responsive Environments.
Test Construction and Management with Network Adaptive Control.
Foundations of Programming: a Teaching Improvement.
HCI Curricula in Spain. A cooperatively designed, free Web-access syllabus.
SQL-92 Compatibility Issues.
Temporal Data Types.
Aggregates
Modification.
A Second Example.
Schema Specification.
The From Clause.
Rationale for a Temporal Extension to SQL.
Supporting Multiple Calendars.
The Surrogate Data Type.
Vacuuming.
The Data Model for Time.
A Timestamp Representation.
Temporal Granularity.
Temporal Indeterminancy.
The Baseline Clock.
``Now''.
Valid-Time Selection and Projection.
Cursors.
The TSQL2 Data Model.
Transaction Time Support.
Schema Versioning.
Introduction to TSQL2.
TSQL2 Tutorial.
Event Tables.
An Algebra for TSQL2
An Architectural Framework.
Frontmatter.
References, Author Index, Syntax Index, Subject Index.
Language Specification.
Clustering in Metric Spaces with Applications to Information Retrieval.
Techniques for Textual Document Indexing and Retrieval Knowledge Sources and Data Mining.
A Science Data System Architecture for Information Retrieval.
Finding Topics in Collections of Documents: A Shared Nearest Neighbor Approach.
Techniques for Clustering Massive Data Sets.
On Quantitative Evaluation of Clustering Systems.
Document Clustering, Visualization, and Retrieval Link Mining.
Clustering Techniques for Large Database Cleansing.
Query Clustering in the Web Context.
Granular Computing for the Design of Information Retrieval Support Systems.
Indexing Techniques for Advanced Database Systems
Advanced Relational Programming: Mathematics and Its Applications.
Mining the World Wide Web: An Information Search Approach
Logics for Databases and Information Systems (the book grow out of the Dagstuhl Seminar 9529: Role of Logics in Information Systems, 1995)
The Discovery of the Artificial. Behavior, Mind and Machines Before and Beyond Cybernetics
Multimedia Mining: A Highway to Intelligent Multimedia Documents
Client Data Caching: A Foundation for High Performance Object Oriented Database Systems
Software Prototyping in Data and Knowledge Engineering
Advanced Transaction Models and Architectures
Advanced Database Indexing
Advanced Signature Indexing for Multimedia and Web Applications
:Advanced Signature Indexing for Multimedia and Web Applications is an excellent reference for professionals involved in the development of applications in multimedia databases or the Web and may also serve as a textbook for advanced level courses in database and information retrieval systems.
Ontology-Based Query Processing for Global Information Systems
Ontology-based query processing for global information systems describes an initiative for enhancing query processing in a global information system. The following are some of the relevant features: Providing semantic descriptions of data repositories using ontologies. Dealing with different vocabularies so that users are not forced to use a common one. Defining a strategy that permits the incremental enrichment of answers by visiting new ontologies. Managing imprecise answers and estimations of the incurred loss of information.
Compression and Coding Algorithms
Open Source GIS: A GRASS GIS Approach
Computers and Education in the 21st Century
Computers and Education. Towards an Interconnected Society
The Design and Implementation of a Log-Structured File-System.
This dissertation presents a new technique for disk storage management called a log-structured file system. The technique writes all file system changes in large sequential transfers to a log-like structure on disk. The key benefit is a high write performance that is independent of the workload. The large transfers also enable the efficient use of large disk arrays such as RAIDs. The technique minimizes the overhead of computing the redundancy information required by large RAIDs. A log-structured file system achieves high write rates without sacrificing file retrieval performance. Files are read back from the log efficiently due to the indexing information that is maintained The log structure also permits fast recovery from system crashes. Using a recovery system based on checkpoints and roll-forward the log-structured file system can quickly restore the disk to a consistent state. An important focus of this dissertation is the technique used for free space management in a log-structured file system. The approach taken was to divide the disk into large segments to which the log was written. A segment cleaner mechanism exists to compress the live information from heavily fragmented segments. The mechanism reads in the fragmented segments, compacts the live data, and writes the data back to segments on disk. The dissertation includes a series of simulations that demonstrate the efficiency of a simple segment cleaning policy based on cost and benefit. The segment cleaner decides which segments to clean based on a function of the fraction alive in the segment and the age of the data in the segment. I have implemented a prototype log-structured file system called Sprite LFS; it outperforms current Unix file systems by an order of magnitude for small-file writes and matches or exceeds Unix performance for reads and large writes. Even when the overhead for cleaning is included, Sprite LFS can use 70% of the disk bandwidth for writing. Unix file systems typically can use only 5-10%.
The TSQL2 Temporal Query Language
Data Quality
Clustering and Information Retrieval
Loop Tiling for Parallelism
A Framework for Meta-Information in Digital Libraries.
Metadata Handling in HyperStorM.
Overview on Using Metadata to manage Multimedia Data.
Metadata for Mixed-Media Access.
Content-Based Image Retrieval using Metadata and Relaxation Techniques.
Metadata Management for Geographic Information Discovery and Exchange.
Using Metadata for the Intelligent Browsing of Structured Media Objects.
Metadata in Geographic and Environmental Data Management.
Video Data Management Systems: Metadata and Architecture.
A Metadatabase System for Semantic Image Search by a Mathematical Model of Meaning.
In the design of multimedia database systems, one of the most important issues is to extract images dynamically according to the user's impression and the image's contents. In this paper, we present a metadatabase system which realizes the semantic associative search for images by giving keywords representing the user's impression and the image's contents.This metadatabase system provides several functions for performing the semantic associative search for images by using the metadata representing the features of images. These functions are realized by using our proposed mathematical model of meaning. The mathematical model of meaning is extended to compute specific meanings of keywords which are used for retrieving images unambiguously and dynamically. The main feature of this model is that the semantic associative search is performed in the orthogonal semantic space. This space is created for dynamically computing semantic equivalence or similarity between the metadata items of the images and keywords.
The Use of Metadata for the Rendering of Personalized Video Delivery.
Metadata for Content-based Retrieval of Speech Recording.
Microsoft SQL Server (Chapter 27)
Database Systems - Concepts, Languages and Architectures
SQL - The Standard Handbook
Distributed Databases: Principles and Systems
Introduction to Algorithms
Introduction to Algorithms, Second Edition
Oracle (Chapter 25)
IBM DB2 Universal Database (Chapter 26)
Database Management Systems.
Introduction to Modern Information Retrieval.
Multimedia Data Management: Using Metadata to Integrate and Apply Digital Media
Database System Concepts, 1st Edition.
Database System Concepts, 2nd Edition.
Database System Concepts, 4th Edition.
:This acclaimed revision of a classic database systems text offers a complete background in the basics of database design, languages, and system implementation. It provides the latest information combined with real-world examples to help readers master concepts. All concepts are presented in a technically complete yet easy-to-understand style with notations kept to a minimum. A running example of a bank enterprise illustrates concepts at work. To further optimize comprehension, figures and examples, rather than proofs, portray concepts and anticipate results.
Database System Concepts, 3rd Edition.
:This acclaimed revision of a classic database systems text offers a complete background in the basics of database design, languages, and system implementation. It provides the latest information combined with real-world examples to help readers master concepts. All concepts are presented in a technically complete yet easy-to-understand style with notations kept to a minimum. A running example of a bank enterprise illustrates concepts at work. To further optimize comprehension, figures and examples, rather than proofs, portray concepts and anticipate results.
Database Design, Revised 2nd Edition.
Database Design.
Database Design, Second Edition.
File Organisation for Database Design.
Extracting Reaction Information from Chemical Databases.
Integrated Learning in a Real Domain.
Attribute-Oriented Induction in Relational Databases.
Statistical Technique for Extracting Classificatory Knowledge from Databases.
Summary Data Estimation Using Decision Trees.
Information Discovery through Hierarchical Maximum Entropy Discretization and Synthesis.
Using Functions to Encode Domain and Contextual Knowledge in Statistical Induction.
Knowledge Discovery in Databases: An Overview.
The Trade-Off between Knowledge and Data in Knowledge Acquisition.
Automated Knowledge Generation From a CAD Database.
Incremental Discovery of Rules and Structure by Hierarchical and Parallel Clustering.
A Support System for Interpreting Statistical Data.
Mining for Knowledge in Databases: Goals and General Description of the INLEN System.
Automating the Discovery of Causal Relationships in a Medical Records Database: The POSCH AI Project.
Induction of Decision Trees from Complex Structured Data.
Discovery of Medical Diagnostic Information: An Overview of Methods and Results.
Knowledge Discovery as a Threat to Database Security.
Minimal-Length Encoding and Inductive Inference.
Discovery, Analysis, and Presentation of Strong Rules
On Evaluation of Domain-Independent Scientific Function-Finding Systems.
Justification-Based Refinement of Expert Knowledge.
Rule Discovery for Query Optimization.
Unsupervised Discovery in an Operational Control Setting.
Rule Induction Using Information Theory.
Learning Useful Rules from Inconclusive Data.
Integration of Heuristic and Bayesian Approaches in a Pattern-Classification System.
Discovering Functional Relationships from Observational Data.
On Linguistic Summaries of Data.
The Discovery, Analysis, and Representation of Data Dependencies in Databases.
Interactive Mining of Regularities in Databases.
Fast Discovery of Association Rules.
Predicting Equity Returns from Securities Data.
Finding Patterns in Time Series: A Dynamic Programming Approach.
The Process of Knowledge Discovery in Databases.
Graphical Models for Discivering Knowledge.
Bayesian Classification (AutoClass): Theory and Results.
Inductive Logic Programming and Knowledge Discovery in Databases.
A Statistical Perspective on Knowledge Discovery in Databases.
Automating the Analysis and Cataloging of Sky Surveys.
From Data Mining to Knowledge Discovery: An Overview.
Transformation Rules and Trees.
Discovering Informative Patterns and Data Cleaning.
Attribute-Oriented Induction in data Mining.
Bayesian Networks for Knowledge Discovery.
Data Surveyor: Searching the Nuggets in Parallel.
Using Inductive Learning To Generate Rules for Semantic Query Optimization.
Explora: A Multipattern and Multistrategy Discovery Assistant.
Knowledge Discovery in Database Terminology.
Selecting and reporting What Is Interesting.
Data Mining and Knowledge Discovery Internet Resources.
Metaqueries for Data Mining.
Integrating Inductive and Deductive Reasoning for Data Mining.
Modeling Subjective Uncertainty in Image Annotation.
From Data Mining to Knowledge Discovery: Current Challenges and Future Directions.
From Contingency Tables to Various Forms of Knowledge in Databases.
Conceptual Modeling of Workflows.
A Behaviorally Driven Approach to Object-Oriented Analysis and Design with Object Oriented Data Modeling.
Coordinated Collaboration of Objects.
Identifying Objects by Declarative Queries.
Temporally Enhanced Database Design.
An Active, Object-Oriented, Model-Equivalent Programming Language.
Mapping an Extended Entity-Relationship into a Schema of Complex Objects.
Advances in Object-Oriented Modeling.
Leveraging Relational Data Assets.
Modeling Object Dynamics.
Database Integration: The Key to Data Interoperability.
On the Design of Behavior Consistent Specializations of Object Life Cycles in OBD and UML.
Objects and Events as Modeling Drivers.
The Type System of LML.
A Regular Type Language for Logic Programs.
Logic Programming with Type Specifications.
Semantic Types for Logic Programs.
A Semantics for Typed Logic Programs.
A Pragmatic View of Types for Logic Programs.
The Type System of a Higher-Order Logic Programming Language.
The type system of the logic programming language $\lambda$Prolog is discussed. The incorporation of higher-order notions within this language requires the use of a typing scheme to distinguish between expressions of different function types. Thus $\lambda$Prolog is a (strongly) typed language, in contrast to the typeless language that underlies the idea of descriptive types in logic programming. The typing discipline that is employed in the language is based on the notion of simple types in the $\lambda$-calculus. This form of typing enforces arity restrictions on functions and predicates and provides a builtin functional hierarchy over terms. The language contains a facility for defining new primitive types and thus permits finer grained distinctions to be introduced by the user. Further, the use of type variables and type constructors provides a form of polymorphism that is similar in certain respects to that present in the language ML. The notion of type checking in $\lambda$Prolog is discussed and shown to be an operation that can be performed at the time of compilation. The value of typing distinctions in determining the clarity of programs and their usefulness in conjunction with type checking in preventing run-time errors due to type violations is also discussed. In addition to their function in type checking, types also have a major role in determining computations in the logic programming context. We discuss this aspect of types that is in contrast to their behavior in other programming paradigms and we show how this leads to a presence of types in the runtime environment. While typing has several advantages, it is sometimes to the programmer''s advantage to be able to omit their mention. Type reconstruction provides a means for filling missing type information in and we discuss issues pertinent to this process in the context of $\lambda$Prolog.
Types and the Intended Meaning of Logic Programs.
Dependent Types in Logic Programming.
Polymorphically Typed Logic Programs.
Using Moded Type Systems to Support Abstraction in Logic Programs.
Actors: A Conceptual Foundation for Concurrent Object-Oriented Programming.
Groundwork for an Object Database Model.
Definition Groups: Making Sources into First-Class Objects.
Object-Oriented Specifications.
Unifying Functional, Object-Oriented and Relational Programming with Logical Semantics.
A Model for Object-Based Inheritance.
Vulcan: Logical Concurrent Objects.
The BETA Programming Language.
Block-Structure and Object-Oriented Languages.
Development and Implementation of an Object-Oriented DBMS.
A Mechanism for Specifying the Structure of Large, Layered Systems.
An Object-Oriented Framework for Conceptual Programming.
Type Evolution in an Object-Oriented Database.
A Substrate for Object-Oriented Interface Design.
Inheritance and the Development of Encapsulated Software Systems.
The Object-Oriented Classification Paradigm.
Workflow Management: Models, Methods, and Systems
Structure and Interpretation of Computer Programs
Structure and Interpretation of Computer Programs, Second Edition
Warren's Abstract Machine: A Tutorial Reconstruction
Digital Library Use: Social Practice in Design and Evaluation
Visual Reconstruction
Vector Models for Data-Parallel Computing
From Gutenberg to the Global Information Infrastructure
Advances in Knowledge Discovery and Data Mining.
Introduction to Object-Oriented Databases
Advances in Object-Oriented Data Modeling
Knowledge Discovery in Databases
Concepts, Techniques, and Models of Computer Programming
The Art of Prolog - Advanced Programming Techniques
The Art of Prolog - Advanced Programming Techniques, 2nd Ed.
The Intellectual Foundation of Information Organization
Instant electronic access to digital information is the single most distinguishing attribute of the information age. The elaborate retrieval mechanisms that support such access are a product of technology. But technology is not enough. The effectiveness of a system for accessing information is a direct function of the intelligence put into organizing it. Just as the practical field of engineering has theoretical physics as its underlying base, the design of systems for organizing information rests on an intellectual foundation. The subject of this book is the systematized body of knowledge that constitutes this foundation.Integrating the disparate disciplines of descriptive cataloging, subject cataloging, indexing, and classification, the book adopts a conceptual framework that views the process of organizing information as the use of a special language of description called a bibliographic language. The book is divided into two parts. The first part is an analytic discussion of the intellectual foundation of information organization. The second part moves from generalities to particulars, presenting an overview of three bibliographic languages: work languages, document languages, and subject languages. It looks at these languages in terms of their vocabulary, semantics, and syntax. The book is written in an exceptionally clear style, at a level that makes it understandable to those outside the discipline of library and information science.Digital Libraries and Electronic Publishing series
Research Directions in Object-Oriented Programming
Journey to Data Quality
All organizations today confront data quality problems, both systemic and structural. Neither ad hoc approaches nor fixes at the systems levelinstalling the latest software or developing an expensive data warehousesolve the basic problem of bad data quality practices. Journey to Data Quality offers a roadmap that can be used by practitioners, executives, and students for planning and implementing a viable data and information quality management program. This practical guide, based on rigorous research and informed by real-world examples, describes the challenges of data management and provides the principles, strategies, tools, and techniques necessary to meet them. The authors, all leaders in the data quality field for many years, discuss how to make the economic case for data quality and the importance of getting an organization's leaders on board. They outline different approaches for assessing data, both subjectively (by users) and objectively (using sampling and other techniques). They describe real problems and solutions, including efforts to find the root causes of data quality problems at a healthcare organization and data quality initiatives taken by a large teaching hospital. They address setting company policy on data quality and, finally, they consider future challenges on the journey to data quality.
Self-Stabilization
Heterogenous Active Agents
Digital Libraries
Datenbanken: Konzepte und Sprachen
Datenbanken: Implementierungstechniken
Datenbanken: Konzepte und Sprachen, 3. Auflage
Object Identity as a Query Language Primitive.
We demonstrate the power of object identities (oid's) as a database query language primitive. We develop an object-based data model, whose structural part generalizes most of the known complex-object data models: cyclicity is allowed in both its schemas and instances. Our main contribution is the operational part of the data model, the query language IQL, which uses oid's for three critical purposes: (1) to represent data-structures with sharing and cycles, (2) to manipulate sets and (3) to express any computable database query. IQL can be statically type checked, can be evaluated bottom-up and naturally generalizes most popular rule-based database languages. The model can also be extended to incorporate type inheritance, without changes to IQL. Finally, we investigate an analogous value-based data model, whose structural part is founded on regular infinite trees and whose operational part is IQL.
Method Schemas.
The concept of method schemas is proposed as a simple model for object-oriented programming with features such as classes with methods and inheritance, method name overloading, and late binding. An important issue is to check whether a given method schema can possibly lead to inconsistencies in some interpretations. The consistency problem for method schemas is studied. The problem is shown to be undecidable in general. Decidability is obtained for monadic and/or recursion-free method schemas. The effect of covariance is considered. The issues of incremental consistency checking and of a sound algorithm for the general case are briefly discussed.
Self-explained Toolboxes.
The Object-Oriented Database System Manifesto.
A Query Language for O2.
Introduction to Languages.
LISP O2: a Persistent Object-Oriented Lisp.
Clustering Strategies in O2: An Overview.
Handling Distribution in the O2 System.
The O2 Programming Environment.
Integrating Concurrency Control into an Object-Oriented Database System.
Consistency of Versions in Object-Oriented Databases.
Reloop, an Algebra Based Query Language for an Object-Oriented Database System.
Three Alternative Workstation-Server Architectures.
Introduction to the Programming Environment.
Introduction to the System.
The Story of O2.
Using a Database System to Implement a Debugger.
Using Database Applications to Compare Programming Languages.
A Guided Tour of an O2 Applications.
Introduction to the Data Model.
Incremental Compilation in O2.
The O2 Database Programming Language.
O2, an Object-Oriented Data Model.
The Altair group is currently designing an object-oriented data base system called O2. This paper presents a formal description of the object-oriented data model of this system. It proposes a type system defined in the framework of a set-and-tuple data model. It models the well known inheritance mechanism and enforces strong typing.
Building User Interfaces with Looks.
Geographic Applications: An Experience with O2.
The O2 Object Manager: An Overview.
A Framework for Schema Updates In An Object-Oriented Database System.
All Your Data: The Oracle Extensibility Architecture.
Enabling Component Databases with OLE DB.
The Architecture of a Database System for Mobile and Embedded Devices.
Distributed Component Database Management Systems.
Extensible Indexing Support in DB2 Universal Database.
Component Database Systems: Introduction, Foundations, and Overview.
Conclusions and Perspectives.
Foreword.
Building Component Database Systems Using CORBA.
An Architecture for Transparent Access to Diverse Data Sources.
Transaction Management in Database Systems
A Transaction Model for Active Distributed Object Systems
ACTA: The SAGA Continues
ACTA is a comprehensive transaction framework that permits a transaction modeler to specify the effects of extended transactions on each other and on objects in the database. ACTA allows the specification of (1) the interactions between transactions in terms of relationships between significant (transaction manage- ment) events, such as begin, commit, abort, delegate, split, and join, pertaining to different transactions and (2) transactions'' effects on objects'' state and concur- rency status (i.e., synchronization state). Various extended traditional models have been proposed to deal with applica- tions that involve reactive (endless), open-ended (long-lived) and collaborative (interactive) activities. One such model is Sagas [GS87] independent (component) transactions T1, T2,..., Tn which can interleave in any way with component trans- actions of other Sagas. Components can commit even before the Saga commits. However, if the Saga subsequently aborts, effects of the committed components are nullified through the invocation of compensating transactions. After giving a brief introduction to the modeling primitives of ACTA, we illustrate their use by giving a complete formal characterization of Sagas. Sub- sequently, the reasoning power of ACTA is shown by proving properties of Sagas. Finally, the flexibility of ACTA is displayed through a series of variations to the original model of Sagas, each variation coming out of changes to the formal characterization of Sagas.
Introduction to Advanced Transaction Models
A Flexible Framework for Transaction Management in Engineering Environments
Dynamic Restructuring of Transactions
Multidatabase Transaction and Query Processing in Logic
A Transaction Model for an Open Publication Environment
A Cooperative Transaction Model for Design Databases
Using Polytransactions to Manage Interdependent Data
Facility for Non Standard Database Systems
The S-transaction Model
The ConTract Model
Concepts and Applications of Multilevel Transactions and Open Nested Transactions
An Analysis of a Dynamic Query Optimization Scheme for Different Data Distributions.
A Survey of Indexing Techniques for Object-Oriented Database Management Systems.
Towards a Unification of Rewrite-Based Optimization Techniques for Object-Oriented Queries.
Algebraic Query Optimization in the CoOMS Structurally Object-Oriented Database System.
Extensible Query Optimization and Parallel Execution in Volcano.
Tagging as an Alternative to Object Creation.
Query Optimization in Deductive Object Bases.
Query Optimization in Object Bases: Exploiting Relational Techniques.
ADT-based Type System for SQL.
Evaluation Aspects of an Object-oriented Deductive Database Language.
Recently, F-logic has been proposed as an attempt to extend deductive databases by typical concepts of object-oriented languages. Among these concepts are complex objects, (term-based) object identity, methods, classes, typing, inheritance and browsing. In Kifer et al. syntax and model-theoretic semantics is discussed; however many algorithmic aspects which arise when computing the corresponding models are left open. In this paper we start to bridge this gap. Several topics in the context of the evaluation of programs are discussed in detail; among these are weak recursion, global stratification and dynamic type-checking.
Challenges for Query Processing in Object-Oriented Databases.
Integration of Composite Objects into Relational Query Processing: The SQL/XNF Approach.
Physical Database Design for an Object-Oriented Database System.
Optimization of Complex-Object Queries in PRIMA - Statement of Problems.
Implementation of the Object-Oriented Data Model TM.
An Engineering Database benchmark.
The Wisconsin Benchmark: Past, Present, and Future.
Benchmark Software Distribution: Release 1.0.
Introduction.
Do we all look at homeless people in the same way? The innocence of kids can teach us how to look at the world in a different way.
The Neal Nelson Database Benchmark: A Benchmark Based on the Realities of Business.
The Set Query Benchmark.
Doing Your Own Benchmark.
The History of DebitCredit and the TPC.
TPC Benchmark A: Standard Specification.
TPC Benchmark B: Standard Specification.
ASAP: An ANSI SQL Standard Scaleable and Portable Benchmark for Relational Database Systems.
TPC Benchmark B: Standard Specification.
Doing Your Own Benchmark.
TPC-D: Benchmarking for Decision Support.
The Engineering Database Benchmark.
Overview of the Full-Text Document Retrieval Benchmark.
The Wisconsin Benchmark: Past, Present, and Future.
Overview of the SPEC Benchmarks.
Database and Transaction Processing Performance Handbook.
The Set Query Benchmark.
TPC-C - The Standard Benchmark for Online transaction Processing (OLTP).
The History of DebitCredit and the TPC.
ASAP - An ANSI SQL Standard Scaleable and Portable Benchmark for Relational Database Systems.
The Neal Nelson Database Benchmark: A Benchmark Based on the Realities of Business.
TPC Benchmark A: Standard Specification.
Towards a Theory of Declarative Knowledge.
Performance Evaluation of Data Intensive Logic Programs.
Foundations of Semantic Query Optimization for Deductive Databases
Converting AND-Control to OR-Control by Program Transformation.
Negation as Failure Using Tight Derivations for General Logic Programs.
Compiling the GCWA in Indefinite Deductive Databases.
Intelligent Query Answering in Rule Based Systems
Logic Programming and Parallel Complexity
Unification Revisited
On the Declarative Semantics of Logic Programs with Negation.
Equivalences of Logic Programs
A Logic-based Language for Database Updates.
Introduction
UML is at a crossroads. Which of the proposed revisions will bring it closer to meeting user needs and winning tool-vendor commitment? What if UML2 instead combined the best features of each proposal?
On the Declarative Semantics of Deductive Databases and Logic Programs.
A Theorem-Proving Approach to Database Integrity.
Optimizing Datalog Programs.
Datalog programs, i.e., Prolog programs without function symbols, are considered It is assumed that a variable appearing in the head of a rule must also appear in the body of the rule. The input of a program is a set of ground atoms (which are given in addition to the program's rules) and, therefore, can be viewed as an assignment of relations to some of the program's predicates. Two programs are equivalent if they produce the same result for all possible assignments of relations to the extensional predicates (i.e., the predicates that do not appear as heads of rules). Two programs are uniformly equivalent if they produce the same result for all possible assignments of initial relations to all the predicates (i.e., both extensional and intentional). The equivalence problem for Datalog programs is known to be undecidable. It is shown that uniform equivalence is decidable, and an algorithm is given for minimizing a Datalog program under uniform equivalence. A technique for removing parts of a program that are redundant under equivalence (but not under uniform equivalence) is developed. A procedure for testing uniform equivalence is also developed for the case in which the database satisfies some constraints.
Negation in Logic Programming.
A Superjoin Algorithm for Deductive Databases.
On Domain Independent Databases.
Middleware for Location-Based Services.
Database Aspects of Location-Based Services.
Data Transmission in Mobile Communication Systems.
LBS Interoperability Through Standards.
Data Collection.
Introduction.
Navigation Systems: A Spatial Database Perspective.
General Aspects of Location Based Services.
Case Study: The Development of the Find Friends Application.
Active Rule Management in Chimera
The HiPAC Project
Active Database Facilities in Ode
The Ariel Project
The POSTGRES Rules System
The A-RDL System
The Starburst Rule System
Introduction to Active Database Systems
Standards and Commercial Systems
Applications of Active Databases
Conclusions and Future Directions
Introduction to Spatial Databases: Applications to GIS
Optimizing Compilers for Modern Architectures: A Dependence-based Approach
Universal Database Management: A Guide to Object/Relational Technology
Building an Object-Oriented Database System, The Story of O2
Transactional Information Systems: Theory, Algorithms, and the Practice of Concurrency Control and Recovery
Principles of Transaction Processing for Systems Professionals.
Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning and Expert Systems
Migrating Legacy Systems: Gateways, Interfaces, and the Incremental Approach
Data Warehousing: Using the Wal-Mart Model
Data on the Web: From Relations to Semistructured Data and XML
Stochastic Local Search: Foundations & Applications
The Object Data Standard: ODMG 3.0
How to Build a Digital Libary
:Given modern society's need to control its ever-increasing body of information, digital libraries will be among the most important and influential institutions of this century. With their versatility, accessibility, and economy, these focused collections of everything digital are fast becoming the "banks" in which the world's wealth of information is stored. How to Build a Digital Library is the only book that offers all the knowledge and tools needed to construct and maintain a digital library-no matter how large or small. Two internationally recognized experts provide a fully developed, step-by-step method, as well as the software that makes it all possible. How to Build a Digital Library is the perfectly self-contained resource for individuals, agencies, and institutions wishing to put this powerful tool to work in their burgeoning information treasuries. Features Sketches the history of libraries-both traditional and digital-and their impact on present practices and future directions Offers in-depth coverage of today's practical standards used to represent and store information digitally Uses Greenstone, freely accessible open-source software-available with interfaces in the world's major languages (including Spanish, Chinese, and Arabic) Written for both technical and non-technical audiences Web-enhanced with software documentation, color illustrations, full-text index, source code, and more Author Biography: Ian H. Witten is a professor of computer science at the University of Waikato in New Zealand. He directs the New Zealand Digital Library research project. His research interests include information retrieval, machine learning, text compression, and programming by demonstration. He received an MA in Mathematics from Cambridge University, England; an MSc in Computer Science from the University of Calgary, Canada; and a PhD in Electrical Engineering from Essex University, England. He is a fellow of the ACM and of the Royal Society of New Zealand. He has published widely on digital libraries, machine learning, text compression, hypertext, speech synthesis and signal processing, and computer typography. He has written several books, the latest being Managing Gigabytes (1999) and Data Mining (2000), both from Morgan Kaufmann. David Bainbridge is a senior lecturer in Computer Science at the University of Waikato, New Zealand. He holds a PhD in Optical Music Recognition from the University of Canterbury, New Zealand where he studied as a Commonwealth Scholar. Since moving to Waikato in 1996 he has continued to broadened his interest in digital media, while retaining a particular emphasis on music. An active member of the New Zealand Digital Library project, he manages the group's digital music library, Meldex, and has collaborated with several United Nations Agencies, the BBC and various public libraries. David has also worked as a research engineer for Thorn EMI in the area of photo-realistic imaging and graduated from the University of Edinburgh in 1991 as the class medalist in Computer Science.
The Object Database Standard: ODMG-93
Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations
The Object Database Standard: ODMG-93 (Release 1.1)
Managing Gigabytes: Compressing and Indexing Documents and Images, Second Edition
The Object Database Standard: ODMG-93 (Release 1.2)
Priniples of Database Query Processing for Advanced Applications
The Object Database Standard: ODMG 2.0
Advanced Database Systems.
SQL for Smarties: Advanced SQL Programming
Readings in Object-Oriented Database Systems
Joe Celko's SQL Puzzles & Answers
Active Database Systems: Triggers and Rules For Advanced Database Processing.
SQL for Smarties: Advanced SQL Programming, Second Edition
Practical File System Design with the Be File System
Joe Celko's Data and Databases: Concepts in Practice
Unleashing Web 2.0: From Concepts to Creativity
Vossen is both an IS & CS Professor at the University of Muenster, and also served as European Editor-in-Chief for Elseviers international information systems journal. Hagemann is his PhD student whose area of research is Web technology.
Using the New DB2: IBM's Object-Relational Database System.
A Complete Guide to DB2 Universal Database
Engineering a Compiler
Fuzzy Modeling Tools for Data Mining and Knowledge Discovery: Knowledge Discovery, Fuzzy Rule Induction, and Autonomous Agents for Databases and Spreadsheets
Component Database Systems
Database Transaction Models for Advanced Applications.
Camelot and Avalon: A Distributed Transaction Facility
Information Visualization in Data Mining and Knowledge Discovery
Information Visualization in Data Mining and Knowledge Discovery
Database-Driven Web Sites
Query Processing for Advanced Database Systems, Selected Contributions from a Workshop on "Query Processing in Object-Oriented, Complex-Object and Nested Relation Databases", Interationales Begegnungs- und Forschungszentrum für Informatik, Schloss Dagstuhl, Germany, June 1991
Essentials of Artificial Intelligence
The Benchmark Handbook for Database and Transaction Systems (1st Edition).
The Benchmark Handbook for Database and Transaction Systems (2nd Edition).
Transaction Processing: Concepts and Techniques
Moving Objects Databases
Web Farming for the Data Warehouse
Data Mining: Concepts and Techniques
Computer Architecture: A Quantitative Approach, 2nd Edition
Recovery in Parallel Database Systems, Second Edition
Web Dragons: Inside the Myths of Search Engine Technology
A Guide to Developing Client/Server SQL Applications
The Jasmine Object Database: Multimedia Applications for the Web
Elements of Machine Learning.
Practical Digital Libraries: Books, Bytes, and Bucks
Distributed Algorithms
:In Distributed Algorithms, Nancy Lynch provides a blueprint for designing, implementing, and analyzing distributed algorithms. She directs her book at a wide audience, including students, programmers, system designers and researchers. Distributed Algorithms contains the most significant algorithms and impossibility results in the area, all in a simple automata-theoretic setting. The algorithms are proved correct, and their complexity is analyzed according to precisely defined complexity measures. The problems covered include resource allocation, communication, consensus among distributed processes, data consistency, deadlock detection, leader election, global snapshots, and many others. The material is organized according to the system model - first by the timing model and then by the interprocess communication mechanism. The material on system models is isolated in separate chapters for easy reference. The presentation is completely rigorous, yet is intuitive enough for immediate comprehension. This book familiarizes readers with important problems, algorithms, and impossibility results in the area: readers can then recognize the problems when they arise in practice, apply the algorithms to solve them, and use the impossibility results to determine whether problems are unsolvable. The book also provides readers with the basic mathematical tools for designing new algorithms and proving new impossibility results. In addition, it teaches readers how to reason carefully about distributed algorithms - to model them formally, devise precise specifications for their required behavior, prove their correctness, and evaluate their performance with realistic measures.
Atomic Transactions
Understanding SQL and Java Together: A Guide to SGLJ, JDBC, and Related Technologies
Understanding the New SQL: A Complete Guide, Second Edition, Volume I
Understanding SQL Stored Procedures: A Complete Guide to SQL/PSM
Understanding the New SQL: A Complete Guide
Foundations of Deductive Databases and Logic Programming
Advanced Compiler Design and Implementation
Commonsense Reasoning
Database Design for Smarties: Using UML for Data Modeling
Machine Learning: A Theoretical Approach.
Database Principles, Programming, Performance
Database: Principles, Programming, and Performance, Second Edition
Data Quality: The Accuracy Dimension
Computer Architecture: A Quantitative Approach.
Computer Organization & Design: The Hardware/Software Interface
Computer Organization & Design: The Hardware/Software Interface, Second Edition
MICO: An Open Source CORBA Implementation
Data Preparation for Data Mining
C4.5: Programs for Machine Learning
:Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.
Introduction to Data Compression
Location-Based Services
Developing Time-Oriented Database Applications in SQL
Readings in Database Systems, First Edition
Readings in Database Systems, Second Edition
Object-Relational DBMSs: The Next Great Wave
:Discover why object-relational technology is ideal for supporting a broad spectrum of data types and application areas, from financial services to multimedia data. In this completely revised and updated edition, database experts Michael Stonebraker and Paul Brown explore the object-relational paradigm and examine the most recent developments in the field. Specially written for database application programmers, database analysts, and IT managers, this book includes detailed information on how to classify DBMS applications, where object-relational DBMSs fit in the database world, and what mechanisms are required to support such an engine.
Readings in Database Systems, Third Edition
The Mathematics of Inheritance Systems
Object-Relational DBMSs, Second Edition
A Many-Sorted Calculus Based on Resolution and Paramodulation.
The first-order calculus whose well formed formulas are clauses and whose sole inference rules are factorization, resolution and paramodulation is extended to a many-sorted calculus. As a basis for Automated Theorem Proving, this many-sorted calculus leads to a remarkable reduction of the search space and also to simpler proofs. The soundness and completeness of the new calculus and the Sort-Theorem, which relates the many-sorted calculus to its one-sorted counterpart, are shown. In addition results about term rewriting and unification in a many-sorted calculus are obtained. Practical examples and a proof protocol of an automated theorem prover based on the many-sorted calculus are presented.
Principles of Multimedia Database Systems
Database Modeling & Design, Third Edition
Inside ODBC.
Inside COM.
The Data Compression Book, 2nd Edition
Managing Gigabytes: Compressing and Indexing Documents and Images.
Plattformen.
CSCL im Fernstudium.
Selbst organisierte Szenarien.
CSCL in Hochschulseminaren: Zwei Beispielszenarien aus der Praxis.
Virtuelle kooperative Lernräume.
Kooperation in größeren Lerngruppen.
CSCL als Herausforderung an die Lehrerbildung.
Kollaboratives Lernen Studierender mit Hilfe von Knowledge Forum.
Pädagogische und didaktische Grundlagen.
Einleitung und Begriffe.
Entwicklungsprozess.
Kooperative Lernräume.
Software- und Systementwicklung.
Informatikgrundlagen und Mensch-Computer-Kommunikation.
Problemorientiertes Lernen.
Kooperation in kleineren Lerngruppen.
Einführung und Bereitstellung.
Lern- und kommunikationspsychologische Grundlagen.
Projektorientierung.
Gruppen und Gruppenarbeit.
Mediendidaktische Konzeption.
Konzepte für die Lerngruppe.
Qualitätssicherung.
Spezifikationen, Normen und Standards für Lernmaterialien.
Kooperatives Lernen in Organisationen.
CSCL in der betrieblichen Weiterbildung.
Neue Lernformen in der Berufsausbildung: Eine Fallstudie.
CSCL in der Schule.
Forschungsmethoden.
Koordinationswerkzeuge zur Bildung von Lerngruppen.
Moderation.
Coaching.
Konzepte zur Administration.
Kommunikation.
Medienwahl.
The Software Abstract is intended to tell its reader enough about the software to know where to find all the other information that exists about it, anywhere. A copy of the Abstract opens every document about the software. It has a fairly rigid format, so a sequence of Abstracts can be scanned swiftly, the same information always being found at the same place. The format of the Software Abstract is defined in Figure 1. Although most Software Abstracts are written to define individual programs, the format is also an effective tool for describing any software entity: subroutine, program, software system or subsystem, data file, or library. It has even been used successfully to document books and articles about documenting (Schneider, French, Lucas 77). The Abstract is constrained to fit on one side of one sheet of paper. This is the reason that certain sections of the Abstract are considered optional.An Example of Software Abstract is shown in Figure 2. The example demonstrates the considerable informational latitude that can be incorporated into a Software Abstract without violating the strict format guidelines. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player
Adaptivität für individuelles Lernen.
Werkzeuge für spezielle Lernmethoden.
Kommunikationskonzepte.
CSCL für Lernbehinderte und Hochbegabte.
Motivation.
Lerngruppen.
Perspektiven.
Konzepte für den Lehrenden.
Bedarfsanalysen.
Sicherheit: Gewährleistung und Begrenzung des Informationsflusses.
Logikorientierte Datenbanken - eine Einführung.
Konzepte objektorientierter Datenmodelle.
Eine Einführung in Frame-Logik.
Natürlichsprachliche Interaktion mit Datenbanken.
Konzepte des Datenbank-Entwurfs.
Modellbildung für Datenbank-Transaktionen.
Datenbankkonzepte für wissensbasierte Systeme.
Datenstrukturen für Geodatenbanken.
Dieser Bericht gibt eine Uebersicht ueber den gegenwaertigen Stand des Entwurfs effizienter Zugriffsstrukturen fur geometrische Objekte in Datenbanken.
Operationen und Kalküle für komplex strukturierte Werte und Objekte.
Studien- und Forschungsführer Informatik der neuen Bundesländer (Fakultätentag Informatik, Arbeitskreis "Informatik an Deutschen Universitäten und Wissenschaftlichen Hochschulen"), 2. Aufl.
Theoretische Informatik: Eine anwendungsorientierte Einführung, 2. Auflage
Java 2, Von den Grundlagen bis zu Threads und Netzen, 2. Auflage
Einführung in die objektorientierte Programmierung mit Java, 2. Auflage
Einführung in die objektorientierte Programmierung mit Java, 1. Auflage
Einführung in die Theoretische Informatik: Formale Sprachen und Automatentheorie
Übersetzerbau
Datenbanksysteme - Eine Einführung, 4. Auflage
Datenbanksysteme - Eine Einführung, 5. Auflage
Datenbanksysteme - Eine Einführung, 6. Auflage
Algorithmen in Java
Rechneraufbau und Rechnerstrukturen, 9. Auflage
Datenbanken im Unternehmen: Analyse, Modellbildung und Einsatz
Ähnlichkeitssuche in Multimedia-Datenbanken - Retrieval, Suchalgorithmen und Anfragebehandlung
Ideen der Informatik: Grundlegende Modelle und Konzepte
Datenmodelle, Datenbanksprachen und Datenbankmanagementsysteme, 4. Auflage
CSCL-Kompendium. Lehr- und Handbuch zum computerunterstützten kooperativen Lernen
Datenmodelle, Datenbanksprachen und Datenbankmanagementsysteme, 5. Auflage
Datenbanksysteme - Eine Einführung, 7. Auflage
Übungsbuch Datenbanksysteme, 2. Auflage
JavaServer Pages
:JavaServer Pages (JSP) technology provides an easy way to create dynamic web pages. JSP uses a component-based approach that allows web developers to easily combine static HTML for look-and-feel with Java components for dynamic features. The simplicity of this component-based model, combined with the cross-platform power of Java, allows a web development environment with enormous potential. JavaServer Pages shows how to develop Java-based web applications without having to be a hardcore programmer. The author provides an overview of JSP concepts and discusses how JSP fits into the larger picture of web applications. Web page authors will benefit from the chapters on generating dynamic content, handling session information, accessing databases, authenticating users, and personalizing content. In the programming-oriented chapters, Java programmers learn how to create Java components and custom JSP tags for web authors to use in JSP pages. About the Author: Hans Bergsten is the founder of Gefion Software, whose main product is a servlet-based component suite for developing web applications. Hans is also an active participant in the development of the JSP specification.
Java Cookbook
Java Examples in a Nutshell
:This book is a companion volume to Java in a Nutshell. While Java in a Nutshell is a quick-reference at heart, it also includes an accelerated introduction to Java programming. Java Examples in a Nutshell picks up where that book leaves off, providing a suite of example programs for novice Java programmers and experts alike. This book doesn't hold your hand or supply detailed explanations of Java syntax or method calls; it simply delivers well-commented, working examples that explore the wide range of what is possible with Java 1.1. Each chapter concludes with programming exercises that suggest further avenues for building on what you have learned.
Java in a Nutshell - A Desktop Quick Reference for Java Programmers, Covers Java 1.0
Java Power Reference
Java Foundation Classes in a Nutshell
Java Fundamental Classes Reference
XML in a Nutshell
This authoritative new edition of XML in a Nutshell provides developers with a complete guide to the rapidly evolving XML space. Serious users of XML will find topics on just about everything they need, including fundamental syntax rules, details of DTD and XML Schema creation, XSLT transformations, and APIs used for processing XML documents. Simply put, this is the only reference of its kind among XML books.
Java 2D Graphics
One weakness of Java has been its graphics capabilities. Java 1.0 and 1.1 only included simple primitives for line drawing: lines could only be one pixel wide, they could only be solid, and there wasn't any good way to draw curves. Font management and color management were also weak. Java 2D (collectively called the "2D API") signals a major improvement in Java's graphics capabilities. It covers many of the classes in Java 1.2 that address graphics handling and improves on many weaknesses that were present in the previous versions of Java. The 2D API allows you to produce high-quality, professional images on a screen or printer. Java 2D Graphics describes the 2D API from top to bottom, demonstrating how to set line styles and pattern fills as well as more advanced techniques of image processing and font handling. You'll see how to create and manipulate the three types of graphics objects: shapes, text, and images. Other topics include image data storage, color management, font glyphs, and printing. Java 2D Graphics assumes no prior knowledge of graphics. Chock full of detailed explanations and examples, this book provides beginning Java programmers with a solid foundation in 2D graphics and helps more advanced programmers create and use high-quality images in their applications. Topics covered in the book include: The rendering pipelineShapes and pathsGeometryPainting with solid colors, gradients, and texturesStroking paths, including dashed linesTransformations: translation, rotation, shearing, and scalingAlpha compositingClippingRasterizing and antialiasingFonts and textFont metricsGlyphsColors and color spacessRGB and CIEXYZICC color profilesImages, image color models, and image dataImage processingImage data storageGraphics devicesPrinting
Java and XML
Java revolutionized the programming world by providing a platform-independent programming language. XML takes the revolution a step further with a platform-independent language for interchanging data. Java and XML shows how to put the two together, building real-world applications in which both the code and the data are truly portable.
Java Virtual Machine
Windows 98 in a Nutshell
Database Programming with JDBC and Java
Java Performance Tuning
CJKV Information Processing: Chinese, Japanese, Korean & Vietnamese Computing
CGI Programming on the World Wide Web, 1st Edition
Programming the Be Operating System: Writing Programs for the Be Operating System
Natural Language Processing with Thought Treasure
Tree Automata
Unification theory.
Classical vs non-classical logics (the universality of classical logic).
Higher order logic.
Meta-languages, reflection principles, and self-reference.
Mathematical induction.
Logical basis for the automation of reasoning: Case studies.
MEME, MAST, and Meta-MEME: New Tools for Motif Discovery in Protein Sequences.
Discovering Concepts in Structural Data.
Motif Discovery in Protein Structure Databases.
Assembling Blocks.
A Framework for Biological Pattern Discovery on Networks of Workstations.
Discovering Patterns in DNA Sequences by the Algorithmic Significance Method.
RNA Structure Analysis: A Multifaceted Approach.
Systematic Detection of Protein Structural Motifs.
Pattern Discovery and Classification in Biosequences.
Overview: A System for Tracking and Managing the Results from Sequence Comparison Programs.
Representation and Matching of Small Flexible Molecules in Large Databases of 3D Molecular Information.
Text Algorithms
Handbook of Logic in Artificial Intelligence and Logic Programming, Volume2, Deduction Methodologies
Pattern Discovery in Biomolecular Data: Tools, Techniques and Applications
The Vbase Object Database Environment.
CommonLoops: Merging Lisp and Object-Oriented Programming.
CommonLoops blends object-oriented programming smoothly and tightly with the procedure-oriented design of Lisp. Functions and methods are combined in a more general abstraction. Message passing is invoked via normal Lisp function call. Methods are viewed as partial descriptions of procedures. Lisp data types are integrated with object classes. With these integrations, it is easy to incrementally move a program between the procedure and object-oriented styles.One of the most important properties of CommonLoops is its extensive use of meta-objects. We discuss three kinds of meta-objects: objects for classes, objects for methods, and objects for discriminators. We argue that these meta-objects make practical both efficient implementation and experimentation with new ideas for object-oriented programming.CommonLoops' small kernel is powerful enough to implement the major object-oriented systems in use today.
Complex Entities for Engineering Applications.
Overview of the Iris DBMS.
Database Description with SDM: A Semantic Database Model
A Tutorial on Semantic Database Modeling
SIM: A Database System Based on the Semantic Data Model.
PICQUERY: A High Level Query Language for Pictorial Database Management.
A reasonably comprehensive set of data accessing and manipulation operations that should be supported by a generalized pictorial database management system (PDBMS) is proposed. A corresponding high-level query language, PICQUERY, is presented and illustrated through examples. PICQUERY has been designed with a flavor similar to QBE as the highly nonprocedural and conservational language for the pictorial database management system PICDMS. PICQUERY and a relational QBE-like language would form the language by which a user could access conventional relational databases and at the same time pictorial databases managed by PICDMS or other robust PDBMS. This languageinterface is part of an architecture aimed toward data heterogeneity transparency over pictorial and nonpictorial databases.
Managing Change in a Computer-Aided Design Database.
Object-oriented concepts can make a design database more reactive to changes in its contents. By embedding change semantics in the database model, the design engineer can be relieved of managing the detailed effects of changes. However, mechanisms are needed to limit the scope of change propagation and to unambiguously identify the objects to which propagated changes should apply. We propose new mechanisms based on group check-in/check-out, browser contexts and paths, configuration constraints, and rules, to support a powerful automatic change capability within a design database.
Integrating an Object-Oriented Programming System with a Database System.
There are two major issues to address to achieve integration of an object-oriented programming system with a database system. One is the language issue: an object-oriented programming language must be augmented with semantic data modeling concepts to provide a robust set of data modeling concepts to allow modeling of entities for important real-world applications. Another is the computational-model issue: application programmers should be able to access and manipulate objects as though the objects are in an infinite virtual memory; in other words, they should not have to be aware of the existence of a database system in their computations with the data structures the programming language allows. This paper discusses these issues and presents the solutions which we have incorporated into the ORION object-oriented database system at MCC.
Object Management in Distributed Information Systems.
Development of an Object-Oriented DBMS.
We describe the results of developing the GemStone object-oriented database server, which supports a model of objects similar to that of Smalltalk-80. We begin with a summary of the goals and requirements for the system: an extensible data model that captures behavioral semantics, no artificial bounds on the number or size of database objects, database amenities (concurrency, transactions, recovery, associative access, authorization) and an interactive development environment. Object-oriented languages, Smalltalk in particular, answer some of these requirements. We discuss satisfying the remaining requirements in an object oriented context, and report briefly on the status of the development efforts. This paper is directed at an audience familiar with object-oriented languages and their implementation, but perhaps unacquainted with the difficulties and techniques of database system development. It updates the original report on the project [CM], and expands upon a more recent article [MDP].
The POSTGRES Data Model.
Type Evolution in an Object-Oriented Database.
A Prological Definition of HASL: A Purely Functional Language with Unification-Based Conditional Binding Expressions.
Constraining-Unification and the Programming Language UNICORN.
Up to this point direct implementations of axiomatic or equational specifications have been limited because the implementation mechanisms used are incapable of capturing the full semantics of the specifications. The programming language Unicorn was designed and implemented with the intention of exploring the full potential of programming with equations. Unicorn introduces a new language mechanism, called constraining-unification. When coupled with semantic unification, constraining-unification closely models the semantics of equational specifications thereby allowing for the implementation of a wider class of specifications. Unlike the language mechanisms of rewrite-rule and logic programming, constraining-unification is free of order dependencies. The same results are produced regardless of the order in which the axioms are stated. The use of viewpoints contributes to the flexibility of the Unicorn language. Preconditions for partial operations can be specified without added machinery.
LEAF: A Language which Integrates Logic, Equations and Functions.
The APPLOG Language.
The Unification of Functional and Logic Languages.
EQLOG: Equality, Types, and Generic Modules For Logic Programming.
Logic Programming Language Scheme.
UNIFORM - A Language Based Upon Unification Which Unifies (much of) LISP, PROLOG, and ACT 1.
Uniform is an AI programming language under development based upon augmented unification. It is an attempt to combine, in a simple coherent-framework, the most important features of Lisp, actor languages such as Act 1 and SmallTalk, and logic programming languages such as Prolog. Among the unusual abilities of the language is its ability to use the same program as a function, an inverse function, a predicate, a pattern, or a generator. All of these uses can be performed upon concrete, symbolic, and partially instantiated data. Uniform features automatic inheritance from multiple super classes, facilities for manipulation of programs, a limited ability to determine program equivalence, and a unification-oriented database.
Equality For Prolog.
The language Prolog has been extended by allowing the inclusion of assertions about equality. When a unification of two terms that do not unify syntactically is attempted, an equality theorem may be used to prove the two terms equal. If it is possible to prove that the two terms are equal the unification succeeds with the variable bindings introduced by the equality proof. It is shown that this mechanism significantly improves the power of Prolog. Sophisticated data abstraction with all the advantages of object-oriented programming is available. Techniques for passing partially instantiated data are described that extends the "multiuse" capabilities of the language, improve the efficiency of some programs, and allow the implementation of arithmetic relations that are both general and efficient. The modifications to standard Prolog are simple and straightforward and in addition the computational overhead for the extra linguistic power is not significant. Equality theorems will probably play an important role in future logic programming systems.
TABLOG: A New Approach To Logic Programming.
On the Relationship Between Logic and Functional Languages.
QUTE: A Functional Language Based on Unification.
FRESH: A Higher-Order Language With Unification and Multiple Results.
FUNLOG: A Computational Model Integrating Logic Programming and Functional Programming.
Introduction to Data Structures and Algorithms Related to Information Retrieval.
String Searching Algorithms.
Signature Files.
The Student Forum will provide an opportunity for students working in the area of dependable computing to present and discuss their research objectives, approaches and preliminary results. The Forum is centered around a conference track during which the selected "student research papers" are presented.
File Organizations for Optical Disks.
Lexical Analysis and Stoplists.
Extended Boolean Models.
Introduction to Information Storage and Retrieval Systems.
Stemming Algorithms.
New Indices for Text: Pat Trees and Pat Arrays.
Relevance Feedback and Other Query Modification Techniques.
Ranking Algorithms.
Inverted Files.
Special-Purpose Hardware for Information Retrieval.
Clustering Algorithms.
Thesaurus Construction.
Parallel Information Retrieval Algorithms.
Boolean Operations.
Hashing Algorithms.
Multimedia Communications - Synchronization.
Design of Large-Scale Multimedia-on-Demand Storage Servers and Storage Hierarchies.
Third-Generation Distributed Hypermedia Systems.
Visual Interfaces to Multimedia Databases.
Video and Image Content Representation and Retrieval.
Modeling Time-Based Media.
Image Database Prototypes.
Composite Models.
Document Model Issues for Hypermedia.
Content-Based Indexing and Retrieval.
Video Database Systems - Recent Trends in Research and Development Activities.
Video Segmentation for Video Data Management.
Multimedia Interfaces - Multimedia Content Indication.
Memory Management: Codecs.
Concurrency Control Performance Modeling: Alternatives and Implications.
A number of recent studies have examined the performance of concurrency control algorithms for database management systems. The results reported to date, rather than being definitive, have tended to be contradictory. In this paper, rather than presenting &ldquo;yet another algorithm performance study,&rdquo; we critically investigate the assumptions made in the models used in past studies and their implications. We employ a fairly complete model of a database environment for studying the relative performance of three different approaches to the concurrency control problem under a variety of modeling assumptions. The three approaches studied represent different extremes in how transaction conflicts are dealt with, and the assumptions addressed pertain to the nature of the database system's resources, how transaction restarts are modeled, and the amount of information available to the concurrency control algorithm about transactions' reference strings. We show that differences in the underlying assumptions explain the seemingly contradictory performance results. We also address the question of how realistic the various assumptions are for actual database systems.
On Mixing Queries and Transactions via Multiversion Locking.
Conflict Detection Tradeoffs for Replicated Data.
Serializability-Based Correctness Criteria.
Database Concurrency Control Using Data Flow Graphs.
A specialized data flow graph, Database Flow Graph (DBFG) is introduced. DBFGs may be used for scheduling database operations, particularly in an MIMD database machine environment. A DBFG explicitly maintains intertransaction and intratransaction dependencies, and is constructed from the Transaction Flow Graphs (TFG) of active transactions. A TFG, in turn, is the generalization of a query tree used, for example, in DIRECT [15]. All DBFG schedules are serializable and deadlock free. Operations needed to create and maintain the DBFG structure as transactions are added or removed from the system are discussed. Simulation results show that DBFG scheduling performs as well as two-phase locking.
Firm Real-Time Concurrency Control.
Reduction in Transaction Conflicts Using Semantics-Based Concurrency Control.
In this chapter, we consider algorithms that permit every site in a partitioned database system to perform new updates. Since this may result in independent updates to items in different partitions, conflicts among transactions are bound to occur. Commutative transactions can be used to reduce the number of these conflicts. We develop a probabilistic model to estimate the possible reduction in conflicts if we incorporate this notion of commutativity. The results show that the additional efforts to recognize this commutativity is not beneficial unless the number of transactions that commute with each other is significantly large.
Concurrency Control Mechanisms and Their Taxonomy.
Transactions and Database Processing.
Extensibility and Asynchrony in the Brown-Object Storage System.
Performance of Concurrency Control Algorithms for Real-Time Database Systems.
The Design and Performance Evaluation of a Lock Manager for a Memory-Resident Database System.
Concurrency Control and Recovery Methods for B+-Tree Indexes: ARIES/KVL and ARIES/IM.
Commit_LSN: A Novel and Simple Method for Reducing Locking and Latching in Transaction Processing Systems.
A Two-Phase Approach to Predictably Scheduling Real-Time Transactions.
An Analytic Model of Transaction Interference.
Synchronizing Long-Lived Computations.
Two-Phase Locking Performance and Its Thrashing Behavior.
Implementation Considerations and Performance Evaluation of Object-Based Concurrency Control Protocols.
Modeling and Analysis of Concurrency Control Schemes.
Modeling Performance Impact of Hot Spots.
Database Design Based on Entity and Realtionship.
Data Models and the ANSI/SPARC Architecture.
Network Database Design Methods.
Compter-Assisted Hierarchical Database Design.
Requirement Specification Techniques.
Semantic Data Models.
Relational Database Design.
Schema Implemetation and Restructuring.
Processing-Requirement Modeling and Its Applications in Logical Database Design.
An Interactive System for Database Design and Integration.
The Design of the UNIX Operating System.
Syntax of Programming Languages: Theory and Practice
Graph Drawing: Algorithms for the Visualization of Graphs
Linear Programming: Active Set Analysis and Computer Programs.
Object-Oriented Multidatabase Systems: A Solution for Advanced Applications
Research Foundations in Object-Oriented and Semantic Database System
Operating Systems Theory
Internetworking with TCP/IP - Principles, Protocols, and Architectures, Fourth Edition
How to Program Advanced Java
A Discipline of Programming.
Data Mining: Introductory and Advanced Topics
Information Retrieval: Data Structures & Algorithms
Database System Implementation
Graphic Java 2, Mastering the JFC: Volume II: Swing, 3rd Edition
Handbook of Multimedia Information Management.
Brinch Hansen on Pascal Compilers
Communicating Sequential Processes
This paper suggests that input and output are basic primitives of programming and that parallel composition of communicating sequential processes is a fundamental program structuring method. When combined with a development of Dijkstra's guarded command, these concepts are surprisingly versatile. Their use is illustrated by sample solutions of a variety of familiar programming exercises.
Algorithms for Clustering Data
The Implementation of Functional Programming Languages.
Object-Oriented Database Management: Applications in Engineering and Computer Science
The C Programming Language
The C Programming Language, Second Edition
Elements of the Theory of Computation
Object Databases in Practice.
The .NET Training Course
Object-Oriented Software Construction, 1st editon
:Object-Oriented Software Construction, second edition is the comprehensive reference on all aspects of object technology, from design principles to Object-Oriented techniques, Design by Contract, Object-Oriented analysis, concurrency, persistence, abstract data types and many more. Written by a pioneer in the field, contains an in-depth analysis of both methodological and technical issues.Two-color printing provides for clear figures and readable software extracts. Comes with a CD-ROM containing: the complete hyperlinked text, for easy reference; software to read the text on major industry platforms; supplementary material (reusable components, mathematical complements); and a complete graphical Object-Oriented development environment supporting the concepts of the book.
Introduction to the Theory of Programming Languages
Eiffel: The Language
An Object-Oriented Environment: Principles and Applications
Reusable Software: The Base Object-Oriented Component Libraries
Object Success
Object-Oriented Software Construction, 2nd Edition
Object-Oriented Applications
Principles of Distributed Database Systems.
Principles of Distributed Database Systems, Second Edition
Combinatorial Optimization: Algorithms and Complexity
The Art of Compiler Design: Theory and Practice
UNIX Database Management Systems
Object-Oriented Modeling and Design
File Structures: An Analytic Approach.
Database Tuning - A Principled Approach
Computer Networks
From the Book:This book is now in its third edition. Each edition has corresponded to a different phase in the way computer networks were used. When the first edition appeared in 1980, networks were an academic curiosity. When the second edition appeared in 1988, networks were used by universities and large businesses. When the third edition appeared in 1996, computer networks, especially the worldwide Internet, had become a daily reality for millions of people. Furthermore, the networking hardware and software have completely changed since the second edition appeared In 1988, nearly all networks were based on copper wire. Now, many are based on fiber optics or wireless communication. Proprietary networks, such as SNA have become far less important than public networks, especially the Internet. The OSI protocols have quietly vanished,, and the TCP/IP protocol suite has become dominant. In fact, so much has changed, the book has almost been rewritten from scratch. Although Chap. 1 has the same introductory function as it did in the second edition, the contents have been completely revised and brought up to date. For example, instead of basing the hook on the seven-layer OSI model. a five-layer hybrid model (shown in Fig. 1-21) is now used and introduced in Chap. 1. While not exactly identical to the TCP/IP model, it is much closer to the TCP/IP model in spirit than it is to the OSI model used in the second edition. Also, the new running examples used throughout the book - the Internet and Al M networks are introduced here, along with some gigabit networks and other popular networks. In Chap. 2, the focus has moved from copper wire to fiber optics and wireless communication,since these arc the technologies of the future. The telephone system has become almost entirely digital in the past decade, so the material on it has been largely rewritten, with new material on broadband ISDN added. The material on cellular radio has been greatly expanded, and new material on low-orbit satellites has been added to the chapter. The order of discussion of the data link layer and the MAC sublayer has been reversed, since experience with students shows that they understand the MAC sublayer better after they have studied the data link layer. The example protocols there have been kept, as they have proven very popular, but they have been rewritten in C. New material on the Internet and ATM data link layers has been added. The MAC sublayer principles of Chap. 4. have been revised to reflect new protocols, including wavelength division multiplexing, wireless LANs, and digital radio. The discussion of bridges has been revised, and new material has been added on high-speed LANs. Most of the routing algorithms of Chap. 5 have been replaced by more modern ones, including distance vector and link state routing. The sections on congestion control have been completely redone, and material on the running examples, the Internet and ATM is all new. Chap. 6 is still about the transport layer, but here, too, major changes have occurred, primarily, the addition of a large amount of new material about the Internet, ATM, and network performance. Chap. 7, on the application layer, is now the longest chapter in the book. The material on network security has been doubled in length, and new material has been added on DNS, SNMP, email, USENET, the World Wide Web, HTML, Java, multimedia, video on demand, and the MBone. Of the 395 figures in the third edition, 276 (70 percent) are completely new and some of the others have been revised. Of the 371 references to the literature, 282 (76 percent) are to books and papers that have appeared since the second edition was published. Of these, over 100 are to works published in 1995 and 1996 alone. All in all, probably 75 percent of the entire book is brand new, and parts of the remaining 25 percent have been heavily revised. Since this is effectively a new book, the cover was redesigned to avoid confusion with the second edition. Computer books are full of acronyms. This one is no exception. By the time you are finished reading this one, all of the following should ring a bell: AAL, AMPS, ARP, ASN, ATM, BGP, CDMA, CDPD, CSMA, DQDB, DNS, FAQ, FDM, FTP, FTTC, FTTH, GSM, HDLC, HEC, HlPPl, TAB, lCMP, IDEA, IETF, 1Pv6, ISO, ITU, LATA, MAC, MACA, MAN, MIB, MIME, NAP, NNTP, NSA, NSAP, OSI, OSPF, PCM, PCN, PCS, PEM, PGP, PPP, PSTN, PTT, PVC, QAM, RARP, RFC, RSA, SABME, SAP, SAR, SDH, SDLC, SHA, SMI, SNA, SNMP, SNRME, SPX, TCP, UDP, VHF, VLF, VSAT, WARC, WDM, WWV, and WWW. But don't worry. Each one will be carefully defined before it is used. To help instructors using this book as a text for course, the author has prepared three teaching aids: A problem solutions manual. PostScript files containing all the figures (for making overhead sheets). A simulator (written in C) for the example protocols of Chap. 3. The solutions manual is available from Prentice Hall (but only to instructors). The file with the figures and the simulator are available via the World Wide Web. To get them, please see the author's home page: http://www.cs.vu.nl/~ast/. The book was typeset in Times Roman using Troff, which, after all these years, is still the only way to go. While Troff is not as trendy as WYSIWYG systems, the reader is invited to compare the typesetting quality of this book with books produced by WYSIWYG systems. My only concession to PCs and desktop publishing is that for the first time, the art was produced using Adobe Illustrator, instead of being drawn on paper. Also for the first time, the book was produced entirely electronically. The PostScript output from Troff was sent over the Internet to the printer, where the film for making the offset plates was produced. No intermediate paper copy was printed and photographed, as is normally done. Andrew S. Tanenbaum
Modern Operating Systems
Design of Database Structures
Geographic Information Systems and Cartographic Modelling.
Core Swing: Advanced Programming
A First Course in Database Systems.
PCTE - The Standard for Open Repositories.
Seamless Object-Oriented Software Architecture - Analysis and Design of Reliable Systems
A Model Implementation of Standard Pascal
Principles of Database Design, Volume I: Logical Organizations
Konstruktion von Anfrageoptimierern für Objektbanken.
On the Complexity of Broadcast and Gossip in Different Communication Modes.
Branching Programs and Binary Decision Diagrams
Topological Concepts for Hierarchies of variables, Types and Controls.
Modifications of the Oettli-Prager Theorem with Application to the Eigenvalue Problem.
Introduction: Symbolic Algebraic Methods and Verification Methods.
Symbolic-Numeric Algorithms for Polynomials: Some recent Results.
On the Isoefficiency of the Parallel Descartes Method.
Matrix Methods for Solving Algebraic Systems.
A Feasibility Result for Interval Gaussian Elimination Relying on Graph Structure.
Solution of Systems of Polynomial Equation by Using Bernstein Expansion.
Symbolic-Algebraic Computations in Modeling Language for Mathematical Programming.
Translation of Taylor Series into LFT Expansions.
Quasi Convex-Concave Extension.
Rewriting, Induction and Decision Procedures: A Case Study of Presburger Arithmetic.
Derivative-Based Subdivision in Multi-dimensional Verified Gaussian Quadrature.
On the Shape of the Fixed Points of [f]([c])=[A][x]+[b].
Exact Computation with leda_real - Theory and geometric Applications.
Numerical Verification Method for Solutions of Nonlinear Hyperbolic Equations.
Geometric Series Bounds for the Local errors of Taylor Methods for Linear n-th-Order ODEs.
Safe Numerical Error Bounds for Solutions of Nonlinear Elliptic Boundary Value Problems.
Fast Verification Algorithms in MATLAB.
The Linear Complementarity Problem with Interval Data.
Some Numerical Methods for Nonlinear Least Squares Problems.
A New Insight of the Shortley-Weller Approximation for Dirichlet Problems.
How Orthogonality is Lost in Krylov Methods.
Symbolic-Numeric QD-Algorithms with Application in Function Theory and Linear Algebra.
Content-Based Querying.
Searching Distributed Hypermedia.
Query Processing.
Introduction [Requirements for a Multimedia Database].
User Interaction in a Virtual World Environment.
Operating System Support [for Multimedia Databases].
Communication Support [for Multimedia Databases].
Indexing of Multimedia Data.
Critical Success Factors [for Multimedia Databases].
Multimedia and its Impact on Database System Architectures.
Current and Emerging Applications [Requirements for a Multimedia Database].
The SQL3 Server Interface.
The SGML/HyTime Server Interface.
An Expert Interface for Effective Man-Machine Interaction.
An Interactive Customization Program for a Natural Language Database Query System.
Talking it Over: The Natural Language Dialog System HAM-ANS.
The Semantic-Based Natural Language Interface to Relational Databases.
Studies in the Evaluation of a Domain-Independent Natural Language Query System.
DIAGRAM: A Grammar for Dialogues.
An explanatory overview is given of DIAGRAM, a large and complex grammar used in an artificial intelligence system for interpreting English dialogue. DIAGRAM is an augmented phrase-structure grammar with rule procedures that allow phrases to inherit attributes from their constituents and to acquire attributes from the larger phrases in which they themselves are constituents. These attributes are used to set context-sensitive constraints on the acceptance of an analysis. Constraints can be imposed by conditions on dominance as well as by conditions on constituency. Rule procedures can also assign scores to an analysis to rate it as probable or unlikely. Less likely analyses can be ignored by the procedures that interpret the utterance. For every expression it analyzes, DIAGRAM provides an annotated description of the structure. The annotations supply important information for other parts of the system that interpret the expression in the context of a dialogue. Major design decisions are explained and illustrated. Some contrasts with transformational grammars are pointed out and problems that motivate a plan to use metarules in the future are discussed. (Metarules derive new rules from a set of base rules to achieve the kind of generality previously captured by transformational grammars but without having to perform transformations on syntactic analyses.)
Considerations for the Development of Natural-Language Interfaces to Database Management Systems.
An Engine for Intelligent Graphics.
Algorithmic Number Theory and Its Relationship to Computational Complexity.
Algorithmic Techniques for Geometric Optimization.
Programming Satan's Computer.
Differential BDDs.
Templates for Linear Algebra Problems.
The ART behind IDEAS.
A Quantum Jump in Computer Science.
Mathematical System Models as a Basis of Software Engineering.
Abstracting Unification: A Key Step in the Design of Logic Program Analyses.
Multimedia Authoring Tools: State of the Art and Research Challenges.
Symmetry and Induction in Model Checking.
Trends in Active Vision.
Avoiding the Undefined by Underspecification.
We use the appeal of simplicity and an aversion to complexity in selecting a method for handling partial functions in logic. We conclude that avoiding the undefined by using underspecification is the preferred choice.
Towards a Computational Theory of Genome Rearrangements.
Towards a theory of Recursive Structures.
In computer science, one is interested mainly in finite objects. Insofar as infinite objects are of interest, they must be computable, i.e., recursive, thus admitting an effective finite representation. This leads to the notion of a recursive graph, or, more generally, a recursive structure or data base. In this paper we summarize our recent work on recursive structures and data bases, including (1) the high undecidability of many problems on recursive graphs, (ii) somewhat surprising ways of deducting results on the classification of NP optimization problems from results on the degree of undecidability of their infinitary analogues, and (iii) completeness results for query languages on recursive data bases. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player
Algebraic Topology and Distributed Computing: A Primer.
Computational Models for Distributed Multimedia Applications.
Computational Machine Learning in Theory and Praxis.
Hypermedia Systems as Internet Tools.
Scalable Computing.
Efficient Use of Parallel & Distributed Systems: From Theory to Practice.
Edge-Coloring Algorithms.
All the Needles in a Haystack: Can Exhaustive Search Overcome Combinatorial Chaos?
Chu Spaces and Their Interpretation as Concurrent Objects.
Fundamental Limitations on Search Algorithms: Evolutionary Computing in Perspective.
Petri Net Models of Distributed Algorithms.
Information Retrieval and Information Reasoning.
Reasoning about Actions and Change with Ramification.
Formulations and Formalisms in Software Architecture.
Recurrent Neural Networks.
The Oz Programming Model.
Experimental Validation of Models of Parallel Computation.
Artificial Life and Real World Computing.
Alternating Automata and Program Verification.
Database Transaction Models.
Quo Vadetis, Parallel Machine Models?
Standard Generalized Markup Language: Mathematical and Philosophical Issues.
Fuzzy Sets as a Tool for Modeling.
Data Mining for Selection of Manufacturing Processes.
Fractal Mining - Self Similarity-based Clustering and its Applications.
Text Mining and Information Extraction.
Outlier Detection.
Statistical Methods for Data Mining.
Data Mining within a Regression Framework.
Constraint-Based Data Mining.
Data Mining Query Languages.
Geometric Methods for Feature Extraction and Dimensional Reduction - A Guided Tour.
Data Mining for Imbalanced Datasets: An Overview.
Dimension Reduction and Feature Selection.
Reinforcement-Learning: An Overview from a Data Mining Perspective.
Parallel and Grid-Based Data Mining - Algorithms, Models and Systems for High-Performance KDD.
Link Analysis.
Relational Data Mining.
WEKA - A Machine Learning Workbench for Data Mining.
Evolutionary Algorithms for Data Mining.
Web Mining.
Bias vs. Variance Decomposition for Regression and Classification.
Data Mining Model Comparison.
Frequent Set Mining.
Meta-Learning.
Rule Induction.
Mining High-Dimensional Data.
LERS - A Data Mining System.
Mining Data Streams.
Handling Missing Attribute Values.
Logics for Data Mining.
Quality Assessment Approaches in Data Mining.
Association Rules.
Visualization and Data Mining for High Dimensional Datasets.
On the Use of Fuzzy Logic in Data Mining.
DataEngine - Tools for Intelligent Data Analysis and Control.
Mining with Rare Cases.
Data Mining for Financial Applications.
Data Mining for Software Testing.
Data Mining in Medicine.
Data Mining for Target Marketing.
GainSmarts Data Mining System for Marketing.
Wavelet Methods in Data Mining.
Granular Computing and Rough Sets.
Introduction to Knowledge Discovery in Databases.
Introduction to Supervised Methods.
Decomposition Methodology for Knowledge Discovery and Data Mining.
Data Cleansing - A Prelude to Knowledge Discovery.
Wizsoft's WizWhy.
Collaborative Data Mining.
Organizational Data Mining.
A Review of Web Document Clustering Approaches.
Mining Time Series Data.
Data Mining of Design Products and Processes.
Ensemble Methods for Classifiers.
Decision Trees.
Clustering Methods.
Interestingness Measures - On Determining What Is Interesting.
Spatial Data Mining.
Support Vector Machines.
Learning Information Patterns in Biological Databases.
Data Mining for Intrusion Detection.
Oracle Data Mining - Data Mining in the Database Environment.
Building Data Mining Solutions with OLE DB for DM and XML for Analysis.
A data mining component is included in Microsoft SQL Server 2000 and SQL Server 2005, one of the most popular DBMSs. This gives a push for data mining technologies to move from a niche towards the mainstream. Apart from a few algorithms, the main contribution of SQL Server Data Mining is the implementation of OLE DB for Data Mining. OLE DB for Data mining is an industrial standard led by Microsoft and supported by a number of ISVs. It leverages two existing relational technologies: SQL and OLE DB. It defines a SQL language for data mining based on a relational concept. More recently, Microsoft, Hyperion, SAS and a few other BI vendors formed the XML for Analysis Council. XML for Analysis covers both OLAP and Data Mining. The goal is to allow consumer applications to query various BI packages from different platforms. This paper gives an overview of OLE DB for Data Mining and XML for Analysis. It also shows how to build data mining application using these APIs.
Data Mining for CRM.
Information Fusion - Methods and Aggregation Operators.
Data Mining in Telecommunications.
Discretization Methods.
Causal Discovery.
Neural Networks.
Bayesian Networks.
Object-Oriented Galileo.
Proteus: The DBMS User Interface as an Object.
Handling Constraints and their Exceptions: An Attached Constraint Handler for Object-Oriented CAD Databases.
The Architecture of the EXODUS Extensible DBMS.
Summary.
Managing Complex Objects in the Darmstadt Database Kernel System.
Object-Oriented Database Systems: The Notion and the Issues.
The Efficient Support of Functionally-Defined Data in Cactis.
Inheritance Issues in Computer-Aided Design Databases.
Godal: An Object-Centered Database Language.
An Object-Oriented Interface to a Relational Database.
A Data Modeling Methodology for the Design and Implementation of Information Systems.
Formal specifications that precisely and correctly define the semantics of software systems become increasingly important as the complexity of such systems increase. The emerging set of semantic data models which support both structural and operational abstractions are excellent tools for formal specifications. In this paper we introduce a methodology, based on an object-oriented data model, for the design and development of large software systems. The methodology is demonstrated by applying the object-oriented data model to the specification of a database system which implements the given model. The specification serves several purposes: it formally defines the precise semantics of the operations of the data model, it provides a basis from which the corresponding database system software can be systematically derived, and it tests and demonstrates the adequacy of such a model for defining software systems in general. The design methodology introduced combines techniques from data modeling, formal specifications, and software engineering.
An Overview of PDM: An Object-Oriented Data Model.
Generating Object-Oriented Database Systems with the Data Model Compiler.
An Object-Oriented Database for Trellis.
Design Issues for Object-Oriented Database Systems.
A Shared Object Hierarchy.
This paper describes the design and proposed implementation of a shared object hierarchy. The object hierarchy is stored in a relational database and objects referenced by an application program are cached in the program's address space. The paper describes the database representation for the object hierarchy and the use of POSTGRES, a next-generation relational database management system, to implement object referencing efficiently. The shared object hierarchy system will be used to implement Object FADS, an object-oriented programming environment for interactive database applications that will be the main programming interface to POSTGRES.
ObServer: An Object Server for an Object-Oriented Database System.
Towards on Object-Oriented Data Model for a Mechanical CAD Database System.
Associate Access Support in GemStone.
Object Management in Postgres using Procedures.
This paper presents the object management facilities being designed into a next-generation data manager, POSTGRES. This system is unique in that it does not invent a new data model for support of objects but chooses instead to extend the relational model with a powerful abstract data typing capability and procedures as full-fledged data base objects. The reasons to remain with the relational model are indicated in this paper along with the POSTGRES relational extensions.
Persistent Memory: A Storage System for Object-Oriented Databases.
Views, Objects, and Databases.
Using Adaptive Information Extraction for Effective Human-Centred Document Annotation.
XML Information Retrieval and Information Extraction.
On Knowledgeable Unsupervised Text Mining.
Towards Collaborative Information Retrieval: Three Approaches.
Evaluating Retrieval Performance Using Clickthrough Data.
Concept Drift and the Importance of Example.
Text Mining.
The XDOC Document Suite - a Workbench for Document Mining.
Feature-Rich Memory-Based Classification for Shallow NLP and Information Extraction.
Visual Interfaces for Semantic Information Retrieval and Browsing.
Information Visualization Versus the Semantic Web.
Ontology-based Information Visualization.
The XML Revolution and the Semantic Web.
Concluding Remarks: Today's Vision of Envisioning the Semantic Future.
SVG and X3D: New XML Technologies for 2D and 3D Visualization.
Interactive Interfaces for Mapping E-commerce Ontologies.
Using Scalable Vector Graphics and Georgraphical Information Systems in a Back Pain Study.
Topic Maps Visualization.
Web Rendering Systems: Techniques, Classification Criteria and Challenges.
Recommender Systems for the Web.
Web Services: Description, Interfaces and Ontology.
Interactive Visualization of Paragraph-level Metadata to Support Corporate Document Use.
Informatikbetrachtungen.
Was ist Informatik?
Informatik und Wirtschaftsinformatik.
Informatik - Allgemeinbildung für die Informationsgesellschaft.
Computerunterstütztes problemorientiertes Lernen.
Was lehren wir eigentlich, wenn wir Informatik lehren?
Software-Entwicklung im industriellen Maßstab.
Wissen und Lernen.
Sind Informatiker auch gute Software-Ingenieuere?
Softwaresystemtechnik - eine Informatik-Ingenieurdisziplin.
Progress Toward Automating The Development of Database System Software.
Query Processing in a Multidatabase System
Updating Relational Views.
Common Subexpression Isolation in Multiple Query Optimization
Introduction to Query Processing
Processing Cyclic Queries.
Query Processing Using the Concecutive Retrieval Property.
Global Optimization of Relational Queries: A First Step
Query Processing in R*
Supporting Complex Objects in a Relational System for Engineering Databases.
Physical Database Design: Techniques for Improved Database Performance.
A Query Language for Statistical Databases
Querying Relational Views of Networks
Relational Query Processing on the Non-Von Supercomputer.
The Intelligent Database Machine (IDM).
Database Access Requirements of Knowledge-Based Systems.
The Property of Separability And Its Application to Physical Database Design.
Distributed Database Query Processing.
Datenbanksprachen und Datenbankbenutzung.
Realisierung von operationalen Schnittstellen.
Architektur von Datenbanksystemen.
Datenbankentwurf.
Maßnahmen zur Wahrung von Sicherheits- und Integritäsbedingungen.
Datenmodelle.
JADE - A Java Agent Development Framework.
Jason and the Golden Fleece of Agent-Oriented Programming.
Programming Multi-Agent Systems in 3APL.
IMPACT: A Multi-Agent Framework with Declarative Semantics.
Jadex: A BDI Reasoning Engine.
ARTIMIS Rational Dialogue Agent Technology: An Overview.
The DEFACTO System: Coordinating Human-Agent Teams for the Future of Disaster Response.
CLAIM and SyMPA: A Programming Environment for Intelligent and Mobile Agents.
JACK Intelligent Agents: An Industrial Strength Platform.
Implementation-Aware Embedded Control Systems.
Network Fundamentals.
Microcontrollers.
Bluetooth in Control.
Introduction to Hybrid Systems.
Real-Time Scheduling for Embedded Systems.
From Control Loops to Real-Time Programs.
Discrete-Event Systems.
Basics of Data Acquisition and Control.
Temporal Logic Model Checking.
The Cornell RoboCup Robot Soccer Team: 1999-2003.
Control of Autonomous Mobile Robots.
SOPCs: Systems on Programmable Chips.
Embedded Sensor Networks.
Feedback Control with Communication Constraints.
Control of Single-Input Single-Output Systems.
Vehicle Applications of Controller Area Network.
Finite Automata.
Digital Signal Processors.
Fundamentals of Dynamical Systems.
Fundamentals of RTOS-Based Digital Controller Implementation.
Network Protocols for Networked Control Systems.
Switched Systems.
LabVIEW Real-Time for Networked/Embedded Control.
An Overview of Hybrid Systems Control.
Control Issues in Systems with Loop Delays.
Networked Control Systems: A Model-Based Approach.
Embedded Real-Time Control via MATLAB, Simulink, and xPC Target.
Programmable Logic Controllers.
An Introduction to Hybrid Automata.
Basics of Sampling and Quantization.
Basics of Computer Architecture.
Control Using Feedback over Wireless Ethernet and Bluetooth.
Wireless Control with Bluetooth.
Control Loops in RTLinux.
Abduction and Analogy in Chance Discovery.
The PreparedMind: the Role of Representational Change in Chance Discovery.
Application to Understanding Consumers Latent Desires.
Discovering Deep Building Blocks for Competent Genetic Algorithms Using Chance Discovery via KeyGraphs.
Anatomy of Rare Events in a Complex Adaptive System.
Self-organizing Complex Systems.
Topic Diffusion in a Community.
Chance Discoveries from the WWW.
Prediction, Forecasting, and Chance Discovery.
Dimensional Representations of Knowledge in an Online Community.
Agent Communications for Chance Discovery.
Chance Discovery for Consumers.
Application to Questionnaire Analysis.
Modeling the Process of Chance Discovery.
KeyGraph: Visualized Structure Among Event Clusters.
Detection of Earthquake Risks with KeyGraph.
Logics of Argumentation for Chance Discovery.
The Storification of Chances.
Human-to-Human Communication for Chance Discovery in Business.
Enhancing Daily Conversations.
Active Mining with Visual Human Interface.
Awareness and Imagination of Hidden Factors and Rare Events.
Effects of Scenic Information.
Decisions by Chance and on Chance: Meanings of Chance in Recent News Stories.
Agent Naming and Coordination: Actor Based Models and Infrastructures.
Middleware Technologies: CORBA and Mobile Agents.
A Market-Based Model for Resource Allocation in Agent Systems.
Coordination and Security on the Internet.
Coordination Models: A Guided Tour.
Coordinating Agents using Agent Communication Languages Conversations.
Reusable Patterns for Agent Coordination.
Inter-Organizational Workflows for Enterprise Coordination.
Coordination and Control in Computational Ecosystems: A Vision of the Future.
High-Level Enabling Coordination Technologies, Introduction.
Brokering and Matchmaking for Coordination of Agent Societies: A Survey.
Scalability in Linda-like Coordination Systems.
Constraints Solving as the Coordination of Inference Engines.
Coordination Models and Languages: State of the Art, Introduction.
Basic Enabling Technologies, Introduction.
Preface: Coordination of Internet Agents.
Models and Technologies for the Coordination of Internet Agents: A Survey.
Coordination and Mobility.
Tuple-based Technologies for Coordination.
Run-Time Systems for Coordination.
Agent Coordination via Scripting Languages.
Foreword: Coordination and the Internet.
Applications of Coordination Technology, Introduction.
Emerging Issues of Coordination, Introduction.
Visions, Introduction.
Agent-Oriented Software Engineering for Internet Applications.
Grid Computing-Ansätze für verteiltes virtuelles Prototyping.
Bedeutung von Peer-to-Peer Technologien für die Distribution von Medienprodukten im Internet.
Peer-to-Peer-Computing - Wettbewerbsvorteil für Intel.
Sicherheitsaspekte von P2P Anwendungen in Unternehmen.
Vertrauen und Reputation in P2P-Netzwerken.
Die Anatomie des Grid.
Project JXTA.
Napster in der Videobranche?
Urheberrecht und Peer-to-Peer-Dienste.
Instant Messaging - Nutzenpotentiale und Herausforderungen.
Gnutella.
Moneybee - Vernetzung künstlicher Intelligenz.
Peer-to-Peer Anwendungsbereiche und Herausforderungen.
ZetaGrid.
Web Services und Peer-to-Peer-Netzwerke.
Rule Analysis
Architecture of Active Database Systems
ECA Functionality in a Distributed Environment.
NAOS.
Tool Support.
EXACT: An Approach to Coping with Heterogeneous Rule Execution Models.
RAP: The ROCK & ROLL Active Programming System.
Active Database Systems: Expectations, Commercial Experience, and Beyond.
Database Internal Applications.
Comparing Deductive and Active Databases.
Chimera: A Language for Designing Rule Applications.
SAMOS.
Performance Assessment.
Ariel.
Active Real-Time Database Systems.
Active Database Features in SQL3.
Summary.
Optimization
Introduction
PFL: An Active Functional DBPL.
Monitoring Complex Rule Conditions
REACH.
Ontologies for Knowledge Management.
Ontologies in F-logic.
Web Ontology Language: OWL.
Description Logics.
Ontologies and Metadata for eLearning.
Ontologies and Hypertext.
Knowledge Patterns.
Ontologies in Support of Problem Solving.
The Role of Ontologies in eCommerce.
Ontology Matching: A Machine Learning Approach.
Semantic Layering with Magpie.
Retrieving and Exploring Ontology-based Information.
Supporting User Tasks through Visualisation of Light-weight Ontologies.
Ontology Evaluation.
Ontology of the Process Specification Language.
An Overview of OntoClean.
Building a Very Large Ontology from Medical Thesauri.
Ontology Reconciliation.
Ontology and the Lexicon.
Ontology Learning.
The Resource Description Framework (RDF) and its Vocabulary Description Language RDFS.
Ontology-based Recommender Systems.
Ontology-based Content Management in a Virtual Organization.
An Ontology-based Platform for Semantic Interoperability.
An Ontology-Composition Algebra.
Ontology Engineering Environments.
Tools for Mapping and Merging Ontologies.
The Knowledge Portal "OntoWeb".
An Extensible Ontology Software Environment.
Ontologies in Bioinformatics.
On-To-Knowledge Methodology (OTKM).
Ontologies in Agent Architectures.
Ink as a First-Class Datatype in Multimedia Databases.
A Data Access Structure for Filtering Distance Queries in Image Retrieval.
Multimedia Athoring Systems.
Stream-based Versus Structured Video Objects: Issues, Solutions, and Challenges.
A Unified Approach to Data Modeling and Retrieval for a Class of Image Database Applications.
Indexing for Retrieval by Similarity.
Metadata for Building the MultiMedia Patch Quilt.
Querying Multimedia Databases in SQL.
Towards a Theory of Multimedia Database Systems.
The Storage and Retrieval of Continuous Media Data.
Retrieval of Pictures Using Approximate Matching.
Design and Implementation of QBISM, a 3D Medical Image Database System.
Dataflow and Education: Data-driven and Demand-driven Distributed Computation.
Contrasting Themes in the Semantics of Imperative Concurrency.
Functional Programming and the Language TALE.
Design, Specification and Validation of Hierarchies of Protocols in Distributed Systems.
Infinitary Languages: Basic Theory an Applications to Concurrent Systems.
The Quest Goes on: A Survey of Proofsystems for Partial Correctness of CSP.
Logic Programming: The Foundations, the Approach and the Role of Concurrency.
Process Theory: Semantics, Specification and Verification.
Applications of Temporal Logic to the Specification and Verification of Reactive Systems: A Survey of Current Trends.
Petri Nets: Basic Notions, Structure, Behaviour.
Concepts for Concurrent Programming.
Survey of Biodata Analysis from a Data Mining Perspective.
Mining Chemical Compounds.
Data Mining Methods for a Systematics of Protein Subcellular Location.
Phyloinformatics: Toward a Phylogenetic Database.
Declarative and Efficient Querying on Protein Secondary Structures.
AntiClustAl: Multiple Sequence Alignment by Antipole Clustering.
Piecewise Constant Modeling of Sequential Data Using Reversible Jump Markov Chain Monte Carlo.
Gene Mapping by Pattern Discovery.
Scalable Index Structures for Biological Data.
Introduction to Data Mining in Bioinformatics.
Predicting Protein Folding Pathways.
Summary: A structured folding pathway, which is a time ordered sequence of folding events, plays an important role in the protein folding process and hence, in the conformational search. Pathway prediction, thus gives more insight into the folding process and is a valuable guiding tool to search the conformation space. In this paper, we propose a novel 'unfolding' approach to predict the folding pathway. We apply graph-based methods on a weighted secondary structure graph of a protein to predict the sequence of unfolding events. When viewed in reverse this yields the folding pathway. We demonstrate the success of our approach on several proteins whose pathway is partially known.
RNA Structure Comparison and Alignment.
Active XML: A Data-Centric Perspective on Web Services.
Web Dynamics, Structure, and Page Quality.
An Event-Condition-Action Language for XML.
Search Engine Ability to Cope With the Changing Web.
Active XQuery.
Adaptive Web-Based Educational Hypermedia.
DREAM: Distributed Reliable Event-Based Application Management.
A Survey of Architectures for Adaptive Hypermedia.
Learning Web Request Patterns.
How Large Is the WorldWide Web?
Methods for MiningWeb Communities: Bibliometric, Spectral, and Flow.
WebVigiL: An Approach to Just-In-Time Information Propagation in Large Network-Centric Environments.
Web Dynamics - Setting the Scene.
Navigating the World Wide Web.
Theory of Random Networks and Their Role in Communications Networks.
Crawling the Web.
Combining Link and Content Information in Web Search.
MP - Mobile Portals, Profiles and Personalization.
Evolution of Web Structure and Content - Introduction.
Searching and Navigating the Web - Introduction.
Events and Change on the Web - Introduction.
Personalized Access to the Web - Introduction.
Behavioural Virtual Agents.
We discuss the application of behavioural architectures, in the robotic sense, to virtual agents. 'Virtual Teletubbies' are used as an example of the issues involved. we conclude that the use of such architectures has implications for the whole style in which a virtual world is modelled.
Logic-Based Knowledge Representation.
After a short analysis of the requirements that a knowledge representation language must satisfy, we introduce Description Logics, Modal Logics, and Nonmonotonic Logics as formalisms for representing terminological knowledge, time-dependent or subjective knowledge, and incomplete knowledge respectively. At the end of each section, we briefly comment on the connection to Logic Programming.
An Overview of Planning Under Certainty.
A Taxonomy of Theorem-Proving Strategies.
This article presents a taxonomy of strategies for fully-automated general-purpose first-order theorem proving. It covers forward-reasoning ordering-based strategies and backward-reasoning subgoal-reduction strategies, which do not appear together often. Unlike traditional presentations that emphasize logical inferences, this classification strives to give equal weight to the inference and search components of theorem proving, which are equally important in practice. For this purpose, a formal notion of search plan is given and shown to apply to all classes of strategies. For each class, the form of derivation is specified, and it is shown how inference system and search plan cooperate to generate it.
Knowledge Representation for Stochastic Decision Process.
Reasoning about stochastic dynamical systems and planning under uncertainty has come to play a fundamental role in AI research and applications. The representation of such systems, in particular, of actions with stochastic effects, has accordingly been given increasing attention in recent years. In this article, we survey a number of techniques for representing stochastic processes and actions with stochastic effects using dynamic Bayesian networks and influence diagrams, and briefly describe how these support effective inference for tasks such as monitoring, forecasting, explanation and decision making. We also compare these techniques to several action representations adopted in the classical reasoning about action and planning communities, describing how traditional problems such as the frame and ramification problems are dealt with in stochastic settings, and how these solutions compare to recent approaches to this problem in the classical (deterministic) literature. We argue that while stochastic dynamics introduce certain complications when it comes to such issues, for the most part, intuitions underlying classical models can be extended to the stochastic setting.
A Survey of Automated Deduction.
We survey research in the automation of deductive inference, from its beginnings in the early history of computing to the present day. We identify and describe the major areas of research interest and their applications. The area is characterised by its wide variety of proof methods, forms of automated deduction and applications.
The World Wide Web as a Place for Agents.
The Word Wide Web was born as an Internet service supporting a simple distributed hypertext management system. Since its start a number of technologies have been proposed to enhance its capabilities. In this paper we describe our concept of an active Web, namely how we design the software architecture of interactive cooperative applications based on the Word Wide web. An active Web includes agents able to use the services offered by Word Wide web clients and servers. In an active Web both users and agents can interoperate using a set of basic mechanisms for communication and synchronization. The active Web we describe here is based on coordination technology: we explore two alternative implementations, both based on Java enriched with alternative coordination kernels.
Lifelike Pedagogical Agents and Affective Computing: An Exploratory Synthesis.
OBDD-based Universal Planning: Specifying and Solving Planning Problems for Synchronized Agents in Non-deterministic Domains.
Recently model checking representation and search techniques were shown to be efficiently applicable to planning, in particular to non-deterministic planning. Such planning approaches use Ordered Binary Decision Diagrams (OBDDS) to encode a planning domain as a non-deterministic finite automaton (NFA) and then apply fast algorithms from model checking to search for a solution. OBDDS can effectively scale and can provide universal plans for complex planning domains. We are particularly interested in addressing the complexities arising in non-deterministic, multi-agent domains. In this chapter, we present UMOP,1 a new universal OBDD-based planning framework for non-deterministic, multi-agent domains, which is also applicable to deterministic singleagent domains as a special case. We introduce a new planning domain description language, NADL,2 to specify non-deterministic multi-agent domains. The language contributes the explicit definition of controllable agents and uncontrollable environment agents. We describe the syntax and semantics of NADL and show how to build an efficient OBDD-based representation of an NADL description. The UMOP planning system uses NADL and different OBDD-based universal planning algorithms. It includes the previously developed strong and strong cyclic planning algorithms [9,10]. In addition, we introduce our new optimistic planning algorithm, which relaxes optimality guarantees and generates plausible universal plans in some domains where no strong or strong cyclic solution exist. We present empirical results from domains ranging from deterministic and single-agent with no environment actions to nondeterministic and multi-agent with complex environment actions. UMOP is shown to be a rich and efficient planning system.
Combibining Artificial Intelligence and Databases for Data Integration.
Data integration is a problem at the intersection of the fields of Artificial Intelligence and Database Systems. The goal of a data integration system is to provide a uniform interfacc to a multitude of data sources, whether they are within one enterprise or on the World-Wide Web. The key challenges in data integration arise because the data sources being integrated have been designed independently for autonomous applications, and their contents are related in subtle ways. As a result, a data integration system requires rich formalisms for describing contents of data sources and relating between contents of different sources. This paper discusses works aimed at applying techniques from Artificial Intelligence to the problem of data integration. In addition to employing Knowledge Representation techniques for describing contents of information sources, projects have also made use of Machine Learning techniques for extracting data from sources and planning techniques for query optimization. The paper also outlines future opportunities for applying AI techniques in the context of data integration.
``Underwater Love'': Building Tristão and Isolda`s Personalities.
An Oz-Centric Review of Interactive Drama and Believable Agents.
Believable agents are autonomous agents that exhibit rich personalities. Interactive dramas take place in virtual worlds inhabited by believable agents with whom an audience interacts. In the course of this interaction, the audience experiences a story. This paper presents the research philosophy behind the Oz Project, a research group at CMU that has spent the last ten years studying believable agents and interactive drama. The paper then surveys current work from an Oz perspective.
Robots with the Best of Intentions.
Intelligent mobile robots need the ability to integrate robust navigation facilities with higher level reasoning. This paper is an attempt at combining results and techniques from the areas of robot navigation and of intelligent agency. We propose to integrate an existing navigation system based on fuzzy logic with a deliberator based on the so-called BDI model. We discuss some of the subtleties involved in this integration, and illustrate it on a simulated example. Experiments on a real mobile robot are under way.
Agent-Based Project Management.
Integrated project management means that design and construction planning are interleaved with plan execution, allowing both the design and plan to be changed as necessary. This requires that the right effects of change need to be propagated through the plan and design. When this is distributed among designers and planners, no one may have all of the information to perform such propagation and it is important to identify what effects should be propagated to whom, and when. We describe a set of dependencies among plan and design elements that allow such notification by a set of message-passing software agents. The result is to provide a novel level of computer support for complex projects.
A System for Defeasible Argumentation, with Defeasible Priorities.
Inspired by legal reasoning, this paper presents an argument-based system for defeasible reasoning, with a logic-programming-like language, and based on Dung's argumentation-theoretic approach to the semantics of logic programming. The language of the system has both weak and explicit negation, and conflicts between arguments are decided with the help of priorities on the rules. These priorities are not fixed, but are themselves defeasibly derived as conclusions within the system.
Handling Uncertainty in Control of Autonomous Robots.
Autonomous robots need the ability to move purposefully and without human intervention in real-world environments that have not been specifically engineered for them. These environments are characterized by the pervasive presence of uncertainty: the need to cope with this uncertainty constitutes a major challenge for autonomous robots. In this note, we discuss this challenge, and present some specific solutions based on our experience on the use of fuzzy logic in mobile robots. We focus on three issues: how to realize robust motion control; how to flexibly execute navigation plans; and how to approximately estimate the robot's location.
The Event Calculus Explained.
This article presents the event calculus, a logic-based formalism for representing actions and their effects. A circumscriptive solution to the frame problem is deployed which reduces to monotonic predicate completion. Using a number of benchmark examples from the literature, the formalism is shown to apply to a variety of domains, including those featuring actions with indirect effects, actions with nondeterministic effects, concurrent actions, and continuous change.
Towards a Logic Programming Infrastructure for Internet Programming.
After reviewing a number of Internet tools and technologies originating in the field of logic programming and discussing promissing directions of ongoing research, we describe a logic programming based networking infrastructure which combines reasoning and knowledge processing with flexible coordination of dynamic state changes and computation mobility, as well as and its use for the design of intelligent mobile agent programs. A lightweight logic programming language, Jinni, implemented in Java is introduced as a flexible scripting tool for gluing together knowledge processing components and Java objects in networked client/server applications and thin client environments as well as through applets over the Web. Mobile threads, implemented by capturing first order continuations in a compact data structure sent over the network, allow Jinni to interoperate with remote high performance BinProlog servers for CPU-intensive knowledge processing. A Controlled Natural Language to Prolog translator with support of third party speech recognition and text-to-speech translation allows interaction with users not familiar with logic programming.
Towards Autonomous, Perceptive, and Intelligent Virtual Actors.
This paper explains methods to provide autonomous virtual humans with the skills necessary to perform stand-alone role in films, games and interictive television. We present current research developments in the Virtual Life of autonomous synthetic actors. After a brief description of our geometric. physical, and auditory Virtual Environments, we introduce the perception action principles with a few simple examples. We emphasize the concept of virtual sensors for virtual humans. In particular, we describe our experiences in implementing virtual sensors such as vision sensors, tactile sensors, and hearing sensors. We then describe knowledge-based navigation. knowledge-based locomotion and in more details sensor-based tennis.
Temporally Invariant Junction Tree for Interference in Dynamic Bayesian Network.
Dynamic Bayesian networks (DBNs) extend Bayesian networks from static domains to dynamic domains. The only known generic method for exact inference in DBNs is based on dynamic expansion and reduction of active slices. It is effective when the domain evolves relatively slowly, but is reported to be "too expensive" for fast evolving domain where inference is under time pressure. This study explores the stationary feature of problem domains to improve the efficiency of exact inference in DBNs. We propose the construction of a temporally invariant template of a DBN directly supporting exact inference and discuss issues in the construction. This method eliminates the need for the computation associated with dynamic expansion and reduction of the existing method. The method is demonstrated by experimental result.
Termersetzungssysteme : Grundlagen der Prototyp-Generierung algebraischer Spezifikationen
The Programming Language Ada Reference Manual, Proposed Standard Document, United States Department of Defense
PREMO: A Framework for Multimedia Middleware - Specification, Rationale, and Java Binding
The Programming Language Ada Reference Manual, American National Standards Institute, Inc., ANSI/MIL-STD-1815A-1983
Software-Architekturen für verteilte Systeme
Least Squares Orthogonal Distance Fitting of Curves and Surfaces in Space
Architecture of Distributed Computer Systems
Unobstructed Shortest Paths in Polyhedral Environments
Exploitation of Fine-Grain Parallelism.
Relational Database Technology
Mikroarchitekturen und Mikroprogrammierung: Formale Beschreibung und Optimierung
Object-Oriented Database Programming
A Tight, Practical Integration of Relations and Functions.
The Design of Well-Structured and Correct Programs
Cooperative Interfaces to Information Systems
Symbolic Algebraic Methods and Verification Methods
The Classical Decision Problem
Reasoning with Logic Programming
Abstract State Machines. A Method for High-Level System Design and Analysis
Incremental Speech Translation.
A Resolution Principle for a Logic with Restricted Quantifiers
Multimedia Databases in Perspective
Co-oP, A Group Decision Support System for Cooperative Multiple Criteria Group Decision Making
Propositional, Probabilistic and Evidential Reasoning: Integrating Numerical and Symbolic Approaches
Von Datenbanken zu Expertensystemen
Automatic Verification of Sequential Infinite-State Processes
R/3 Einführung: Methoden und Werkzeuge
A Review of Ada Tasking
SAP R/3 Implementation: Methods and Tools
Generierung natürlicher Sprache mit Generalisierten Phrasenstruktur-Grammatiken
This dissertation describes NL generation with generalized phrase structure grammars (GPSG). It thoroughly discusses the theory of GPSG and argues that it can, in its 1985 version, not efficiently be implemented. Therefore, some modifications are suggested that overcome this problem. Using the modified formalism, two different approaches to GPSG-based generation are presented. The grammar-driven approach is shown to suffer from indeterminism, whereas the structure-driven approach can be efficiently implemented. The latter method has been explored within the Berlin machine translation system, where the generator starts from sentence-semantic input structures designed for transfer. Since the semantics is not integrated into the grammar, an explicit mapping from partial semantic onto syntactic structures is necessary, which is accomplished by using techniques known from AI production systems. Both generators allow for multilingual generation, which is demonstrated for English and German.
Querying Databases Privately: A New Approach to Private Information Retrieval
PORTAL Language Description - Second, Extended Edition
LOGIDATA+: Deductive Databases with Complex Objects
B2B Integration: Concepts and Architecture
Ada 95 Quality and Style, Guideline for Professional Programmers
Fundamental Algorithms for Permutation Groups
Interactive Relational Database design - A Logic Programming Implementation
Computer Science Today: Recent Trends and Developments
The MOSIX Distributed Operating System - Load Balancing for UNIX
Tractable Reasoning in Artificial Intelligence
Mental Representation and Processing of Geographic Knowledge - A Computational Approach
The Concurrency Control Problem for Database Systems
Ada 95 Rationale, The Language, The Standard Libraries
Logic Programming and Databases
A Survey of Verification Techniques for Parallel Programs
Time Series Package (TSPACK)
Entzifferte Geheimnisse: Methoden und Maximen der Kryptologie, 3. Auflage
Parallel Execution of Parlog
Entzifferte Geheimnisse: Methoden und Maximen der Kryptologie, 2. Auflage
Mapping Scientific Frontiers: The Quest for Knowledge Visualization
The Munich Project CIP, Volume I: The Wide Spectrum Language CIP-L
Information Visualisation and Virtual Environments
The Munich Project CIP, Volume II: The Program Transformation System CIP-S
A Guide to Modula-2
Mobile Agents: Control Algorithms
Modelling in Molecular Biology
Software-Bewertung: Ein semantischer Ansatz für Infomationsmaße
Active Visual Inference of Surface Shape
Theory Reasoning in Connection Calculi
Programming in Prolog
Zur Logik der Logik-Programmierung: Ein konstruktiver Ansatz
Programming in Prolog, 2nd Edition
Concepts in User Interfaces: A Reference Model for the Command and Response Languages (By Members of IFIP WG2.7)
Programming in Prolog, 3rd Edition
Concepts, Design, and Performance Analysis of a Parallel Prolog Machine
Prolog by Example: How to Learn, Teach and Use It
Concurrent Reactive Plans, Anticipation and Forestalling Execution Failures
Autonomous robots that accomplish their jobs in partly unknown and changing environments often learn important information while carrying out their jobs. To be reliable and effcient, they have to act appropriately in novel situations and respond immediately to unpredicted events. They also have to reconsider their intended course of action when it is likely to have flaws. For example, whenever a robot detects another robot, it should predict that robot's effect on its plan and -- if necessary -- revise its plan to make it more robust. To accomplish these patterns of activity we equip robots with structured reactive plans (SRPs), concurrent control programs that can not only be interpreted but also reasoned about and manipulated. These plans specify how the robot is to respond to sensory input in order to accomplish its jobs. In this book we describe a computational model of forestalling common flaws in autonomous robot behavior. To this end, we develop a representation for SRPs in which declarative statements for goals, perceptions, and beliefs make the structure and purpose of SRPs explicit and thereby simplify and speed up reasoning about SRPs and their projections. We have also developed a notation for transforming SRPs, which does not only make the physical effects of plan execution explicit, but also the process of plan interpretation, as well as temporal, causal, and teleological relationships among plan interpretation, the world, and the physical behavior of the robot. Using this notation a planning system can diagnose and forestall common flaws in robot plans that cannot be dealt with in other planning representations. Finally, we have extended the language for writing SRPs with constructs that allow for a flexible integration of planning and execution and thereby turned it into a single high-level language that can handle both planning and execution actions. Experiments in a simulated world show that by simultaneously forestalling flaws and executing SRPs, the robot can perform its jobs more reliably than, and almost as effciently as, it could using fixed control programs.
Large Sparse Numerical Optimization
Plan-Based Control of Robotic Agents: Improving the Capabilities of Autonomous Robots
An Introduction to the PL/CV2 Programming Logic
Hierarchical Neural Networks for Image Interpretation
Erfahrung und Berechnung: Kritik der Expertensystemtechnik
Korrekte Zugriffe zu verteilten Daten
The Data Mining and Knowledge Discovery Handbook.
Dynamische nicht-normalisierte Relationen und symbolische Bildbeschreibung
Indexstrukturen in Datenbanksystemen
Automatic Generation of Computer Animation: Using AI for Movie Animation
Systems of Reductions
Queueing Networks with Discrete Time Scale - Explicit Expressions for the Steady State Behavior of Discrete Time Stochastic Networks
Ray Shooting, Depth Orders and Hidden Surface Removal
The Design of Rijndael: AES - The Advanced Encryption Standard
Statistical Decision Theory and Bayesian Analysis, 2nd Edition.
Entwurf und Verifikation mikroprogrammierter Rechnerarchitekturen
Experience Management: Foundations, Development Methodology, and Internet-Based Applications
MetaSoft Primer, Towards a Metalanguage for Applied Denotational Semantics
Efficient Query Processing in Geographic Information Systems
Developing Industrial Case-Based Reasoning Applications: The INRECA-Methodology
The Logic System of Concept Graphs with Negation And Its Relationship to Predicate Logic
How to Multiply Matrices Faster
Managing Information Highways - The PRISM Book: Principles, Methods, and Case Studies for Designing Telecommunications Management Systems
On the Integration of Algebraic Functions
Intelligent Data Analysis, An Introduction, 2nd editon
The Adaptation of Virtual Man-Computer Interfaces to User Requirements in Dialogs
Public-Key Cryptography: State of the Art and Future Directions, E.I.S.S. Workshop, Oberwolfach, Germany, July 3-6, 1991, Final Report
Introduction to Cryptography: Principles and Applications
Due to the rapid growth of digital communication and electronic data exchange, information security has become a crucial issue in industry, business, and administration. Modern cryptography provides essential techniques for securing information and protecting data. In the first part, this book covers the key concepts of cryptography on an undergraduate level, from encryption and digital signatures to cryptographic protocols. Essential techniques are demonstrated in protocols for key exchange, user identification, electronic elections and digital cash. In the second part, more advanced topics are addressed, such as the bit security of one-way functions and computationally perfect pseudo random bit generators. The security of cryptographic schemes is a central topic. Typical examples of provably secure encryption and signature schemes and their security proofs are given. Though particular attention is given to the mathematical foundations, no special background in mathematics is presumed. The necessary algebra, number theory and probability theory are included in the appendix. Each chapter closes with a collection of exercises. Answers to the exercises are provided on the Web page for this book.
Qualität und Testbarkeit hochintegrierter Schaltungen: Qualitätssicherung durch regelbasierte Systeme
Incomplete Information: Structure, Inference, Complexity
:This monograph presents a systematic, exhaustive and up-to-date overview of formal methods and theories for data analysis and inference inspired by the concept of rough set. The book studies structures with incomplete information from the logical, algebraic and computational perspective. The formalisms developed are non-invasive in that only the actual information is needed in the process of analysis without external sources of information being required. The book is intended for researchers, lecturers and graduate students who wish to get acquainted with the rough set style approach to information systems with incomplete information.
Algebraic System Specification and Development - A Survey and Annotated Bibliography
Attribute Grammars: Definitions, Systems, and Bibliography.
Casl User Manual - Introduction to Using the Common Algebraic Specification Language
Das ist Informatik
Semirings for Soft Constraint Solving and Programming
Combinatorics on Traces
Automatische Synthese rekursiver Programme als Beweisverfahren
Flexible, realzeitfähige Kollisionsvermeidung in Mehrroboter-Systemen
Ein universelles Konzept zum flexiblen Informationsschutz in und mit Rechensystemen
Integrity Primitives for Secure Information Systems, Final Report of RACE Integrity Primitives Evaluation RIPE-RACE 1040
Primality Testing in Polynomial Time, From Randomized Algorithms to "PRIMES Is in P"
Network Calculus: A Theory of Deterministic Queuing Systems for the Internet
Mathematik für Informatiker I: Die Methode der Mathematik
Extensions of the UNITY Methodology - Compositionality, Fairness and Probability in Parallelism
Embedded Systems Design: The ARTIST Roadmap for Research and Development
The Stability Theory of Stream Ciphers
Microcomputer Problem Solving Using Pascal
Die strukturierte Analyse Markovscher Modelle
Featurebasierte Integration von CAD/CAM-Systemen
An Optimized Translation Process and Its Application to ALGOL 68
On Object-Oriented Database Systems
An Analytical Description of CHILL, the CCITT High Level Language
Stochastic Automata: Stability, Nondeterminism, and Prediction
Modern Cryptology - A Tutorial
Complementary Definitions of Programming Language Semantics
Massiv parallele Programmierung mit dem Parallaxis-Modell
Error Detection and Recovery in Robotics
Algebraic Specification Techniques in Object Oriented Programming Environments
Efficient Graph Rewriting and Its Implementation
Input/Output Intensive Massively Parallel Computing - Language Support, Automatic Parallelization, Advanced Optimization, and Runtime Systems
Resolution Methods for the Decision Problem
Introduction to Cryptography
Artificial Animals for Computer Animation, Biomechanics, Locomotion, Perception, and Behavior
Fehlermaskierung durch verteilte Systeme
Entwurfstransaktionen für modulare Objektsysteme: Synchronisierung in objektorientierten Datenbanksystemen
Fundamentals of Algebraic Specification 1: Equations und Initial Semantics
Modellbildung, Wissensrevision und Wissensrepräsentation im Maschinellen Lernen
Rechtsprechung und Computer in den neunziger Jahren: Am Beispiel von Begriff und Typologie der Körperschaft des Öffentlichen Rechts
Simple Program Schemes and Formal Languages
Knowledge Discovery in Databases - Techniken und Anwendungen
Generierung portabler Compiler: Das portable System POCO
Exercises in Computer Systems Analysis
Advanced Symbolic Analysis for Compilers: New Techniques and Algorithms for Symbolic Program Analysis and Optimization
Ausnahmebehandlung in objektorientierten Programmiersprachen
Problem-Solving Methods: Understanding, Description, Development, and Reuse
Foundations of Equational Logic Programming
LUCAS Associative Array Processor: Design., Programming and Application Studies
An Attribute Grammar for the Semantic Analysis of Ada
A Systematic Catalogue of Reusable Abstract Data Types
IT-Security and Privacy - Design and Use of Privacy-Enhancing Security Mechanisms
Specification and Compositional Verification of Real-Time Systems
Datenbanksystem für CAD-Arbeitsplätze
Kollisionsfreie Bahnen für Industrieroboter: Ein Planungsverfahren
A Collection of Test Problems for Constrained Global Optimization Problems
Approximative Public-Key-Kryptosysteme
Text Mining, Theoretical Aspects and Applications
Parallele Algorithmen
Automatic Ambiguity Resolution in Natural Language Processing - An Empirical Approach
:This book introduces a new approach to the important NLP issue of automatic ambiguity resolution, based on statistical models of text. This approach is compared with previous work and proved to yield higher accuracy for natural language analysis. At the same time, an effective implementation strategy is described, which is directly useful for natural language analysis. The book is noteworthy for demonstrating a new empirical approach to NLP; it is essential reading for researchers in natural language processing or computational linguistics.
SIL - a Simulation Language, User's Guide
Mechanismen zur Synchronisation paralleler Prozesse
Funktioneller Test der Auflösung von Zugriffskonflikten in Mehrrechnersystemen
Translating Relational Queries into Iterative Programs
FM8501: A Verified Microprocessor
VOCUS: A Visual Attention System for Object Detection and Goal-Directed Search
Agent-Oriented Programming, From Prolog to Guarded Definite Clauses.
Ein Roboteraktionsplanungssystem
Formal Foundations for Software Engineeing Methods
Analysis of Drum and Disk Storage Units
Parallel, Distributed and Multiagent Production Systems
Datenverarbeitung im Hochschulbereich der USA: Stand und Entwicklungstendenzen
Datenverwaltung in verteilten Systemen: Grundlagen und Lösungskonzepte
Matrix Eigensystem Routines - EISPACK Guide Extension
Conclog: A Methodological Approach to Concurrent Logic Programming
On the Shape of Mathematical Arguments
New Concepts for Parallel Object-Relational Query Processing
During the last few years parallel object-relational database management systems have emerged as the leading data management technology on the market place. These systems are extensible by user-defined data types and user-defined functionality for the data. This work focuses on the efficient parallel execution of user-defined functionality. The main contributions describe techniques to support data parallelism for user-defined scalar and aggregate functions, to support intra-function parallelism for the execution of a scalar function on a large object, and a new technology to provide extensibility with regard to new set-oriented database operations that can efficiently implement user-defined functionality in parallel object-relational database management systems. Some of these techniques have been implemented in the MIDAS prototype or on top of a commercial object-relational database management system.
Mes premières constructions de programmes
Spatio-Temporal Image Processing: Theory and Scientific Applications
Modular Algorithms in Symbolic Summation and Symbolic Integration
Pascal User Manual and Report, Second Edition
Visualizing the Semantic Web
Pascal User Manual and Report, Second Edition
Abstract Compositional Analysis of Iterated Relations, A Structural Approach to Complex State Transition Systems.
Evaluating Natural Language Processing Systems, An Analysis and Review
:This comprehensive state-of-the-art book is the first devoted to the important and timely issue of evaluating NLP systems. It addresses the whole area of NLP system evaluation, including aims and scope, problems and methodology. The authors provide a wide-ranging and careful analysis of evaluation concepts, reinforced with extensive illustrations; they relate systems to their environments and develop a framework for proper evaluation. The discussion of principles is completed by a detailed review of practice and strategies in the field, covering both systems for specific tasks, like translation, and core language processors. The methodology lessons drawn from the analysis and review are applied in a series of example cases. The book also refers NLP system evaluation to the neighbouring areas of information and speech processing, and addresses issues of tool and data provision for evaluation. A comprehensive bibliography and subject index are included as well as a term glossary. This monograph will be a valuable source of inspiration in research, practice, and teaching.
Partial-Order Methods for the Verification of Concurrent Systems - An Approach to the State-Explosion Problem
TEMPO: A Unified Treatment of Binding Time and Parameter Passing Concepts in Programming Languages
An Extended Entity-Relationship Model - Fundamentals and Pragmatics
Index Structures for Data Warehouses
Data warehouses differ significantly from traditional transaction-oriented operational database applications. Indexing techniques and index structures applied in the transaction-oriented context are not feasible for data warehouses. This work develops specific heuristic indexing techniques, which process range queries on aggregated data more efficiently than those traditionally used in transaction-oriented systems. The book presents chapters on: - the state of the art in data warehouse research - data storage and index structures - finding optimal tree-based index structures - aggregated data in tree-based index structures - performance models for tree-based index structures - and techniques for comparing index structures.
Axiomatising the Logic of Computer Programming
Sprecherunabhängigkeit und Sprecheradaption: Lösungsansätze für das Problem des Sprecherwechsels bei der automatischen Spracherkennung
DIANA - An Intermediate Language for Ada, Revised Version
GAG: A Practical Compiler Generator
Edinburgh LCF
Zuverlässigkeit und Leistungsfähigkeit objekt-orientierter Datenbanksysteme
Robust Adaptation to Non-Native Accents in Automatic Speech Recognition
Conditionals in Nonmonotonic Reasoning and Belief Revision - Considering Conditionals as Agents
Datenbanksysteme für Software-Produktionsumgebungen
Personalinformationssysteme in deutschen Großunternehmen: Ausbaustand und Rechtsprobleme
Graphgrammatiken in der Softwaretechnik: Theorie und Anwendungen
Query Processing in Database Systems
Term Indexing
Disconnected Operation in a Distributed File System
:The focus of this work is on the issue of availability in distributed file systems. It presents the important new technique called disconnected operation, in which clients mask failures and voluntary network detachments by emulating the functionality of servers where actual server-oriented solutions are inadequate. This permits client operation even under complete isolation from the server; the clean integration of mobile computers into the system is an important side-effect of the new technique. The design and implementation of disconnected file service in a working system, the Coda file system, is described in detail.
The Problem of Incomplete Information in Relational Databases
Metaclasses and Their Applications, Data Model Tailoring and Database Integration
Distributed Programming Paradigms with Cryptography Applications
Concrete and Abstract Voronoi Diagrams
Theory of Program Structures: Schemes, Semantics, Verification
A Study in String Processing Languages
The Science of Programming.
Treewidth, Computations and Approximations
A Logical Approach to Discrete Math.
Semantics of Digital Circuits
Lectures on the Complexity of Bilinear Problems
Optimal Interprocedural Program Optimization, A New Framework and Its Application
A Perspective of Constraint-Based Reasoning - An Introductory Tutorial
Axioms and Hulls
Algebraic Semantics
MMIXware, A RISC Computer for the Third Millennium
Efficient Structures for Geometric Data Management
Benutzermodellierung in Dialogsystemen
List Decoding of Error-Correcting Codes (Winning Thesis of the 2002 ACM Doctoral Dissertation Competition)
Relationale Anfragen - Zerlegung und Optimierung
Übersetzerbau - Techniken, Werkzeuge, Anwendungen
A Unified Approach to Interior Point Algorithms for Linear Complementarity Problems
Prinzipien der Referentialität: Untersuchungen zur propositionalen Repräsentation von Wissen
Prosody in Speech Understanding Systems
Hyperedge Replacement: Grammars and Languages
Model Generation for Natural Language Interpretation and Analysis
Communication and Cooperation in Agent Systems, A Pragmatic Theory
Triggermechanismen in Datenbanksystemen
Representing Plans Under Uncertainty: A Logic of Time, Change, and Action
Verkehrsanalyse in endlichen Zeiträumen: Grundlagen und Erweiterungen der operationalen Analyse
Lexikalisch verteiltes Text-Parsing: Eine objektorientierte Spezifikation eines Wortexpertensystems auf der Grundlage des Aktorenmodells
Specifying Message Passing and Time-Critical Systems with Temporal Logic
Veritying Concurrent Processes Using Temporal Logic
Distributed Reason Maintenance for Multiagent Systems
Time Structures - Formal Description and Algorithmic Representation
LOGPLAN '88 - Report on the Programming Language
Management of Telecommunication Systems and Services, Modelling and Implementing TMN-Based Multi-Domain Management
:This monograph is the final report of the European RACE II Research Project PREPARE, devoted to designing and implementing prototype telecommunication management systems over real testbeds. The excellent results presented are a major contribution to advancing the state of the art in the field and have been widely acknowledged by standards bodies, manufacturers, operators, service providers, and academics. Prospective readers of the book include those concerned with the design, development, delivery, and maintenance of telecommunication services in the global service market as well as those with a general theoretical or practical interest in multi-domain management issues.
Numerical Integration on Advanced Computer Systems
Autonomous Dynamic Reconfiguration in Multi-Agent Systems, Improving the Quality and Efficiency of Collaborative Problem Solving
Situationsmodellierung in der Bildfolgeauswertung
Datenbanksysteme: Konzepte und Techniken der Implementierung, 2. Auflage
Funktionelle Analyse von Kommunikationsprotokollen
Datenbanksysteme: Konzepte und Techniken der Implementierung
CAD und Arbeitssituation: Untersuchungen zu den Auswirkungen von CAD sowie zur menschengerechten Gestaltung von CAD-Systemen
First-Order Dynamic Logic
Semirings, Automata, Languages
Adaptive Modelling, Estimation and Fusion from Data: A Neurofuzzy Approach
This book brings together for the first time the complete theory of data-based neurofuzzy modelling and the linguistic attributes of fuzzy logic in a single cohesive mathematical framework. After introducing the basic theory of data based modelling, new concepts including extended additive and multiplicative submodels are developed and their extensions to state estimation and data fusion are derived. All of these algorithms are illustrated with benchmark and real-life examples to demonstrate their efficiency. The book aims at researchers and advanced professionals in time series modelling, empirical data modelling, knowledge discovery, data mining and data fusion.
Constraint Databases
A Concurrent Pascal Compiler for Minicomputers
Fehlererkennung und Fehlerbehandlung in Speicherungsstrukturen von Datenbanksystemen
Optimization of SQL Queries for Parallel Machines
Parallel execution offers a method for reducing the response time of queries against large databases. We address the problem of parallel query optimization: Given a declarative SQL query, find a procedural parallel plan that delivers the query result in minimal time. We develop optimization algorithms using models that incorporate both sources and obstacles to speedup. We address independent, pipelined and partitioned parallelism. We incorporate inherent constraints on available parallelism and the extra cost of parallel execution. Our models are motivated by experiments with NonStop SQL, a commercial parallel DBMS. We adopt a two-phase approach to parallel query optimization: JOQR (join ordering and query rewrite), followed by parallelization. JOQR minimizes total work. Then, parallelization spreads work among processors to minimize response time. For JOQR, we model communication costs and abstract physical characteristics of data as colors. We devise tree coloring and reordering algorithms that are efficient and optimal. We model parallelization as scheduling a tree whose nodes represent operators and edges represent parallel/precedence constraints. Computation/communication costs are represented as node/edge weights. We prove worst-case bounds on the performance ratios of our algorithms and measure average cases using simulation. Our results enable the construction of SQL compilers that effectively exploit parallel machines.
Non-Standard Inferences in Description Logics
Portable Methodenmonitoren: Dialogsysteme zur Steuerung von Methodenbanken, softwaretechnischer Aufbau und Effizienzanalyse
Bibliography on Abstract Data Types, Sponsored by the "Österr. Fonds zur Förderung der Wissenschaftlichen Forschung"
NEWCAT: Parsing Natural Language Using Left-Associative Grammar
SEMPER - Secure Electronic Marketplace for Europe
Überlast in Rechensystemen: Modellierung und Verhinderung
Part-Whole Reasoning in an Object-Centered Framework
On the Computational Geometry of Pocket Machining
Semantische Repräsentation komplexer Objektstrukturen: Modelle für nichtkonventionelle Datenbankanwendungen
Symbolische und konnektionistische Modelle der menschlichen Informationsverarbeitung - Eine kritische Gegenüberstellung
Modelling Spatial Knowledge on a Linguistic Basis: Theory - Prototype - Integration
The Use of Projective Geometry in Computer Graphics
Datenbankeinsatz.
Interactive Markov Chains: The Quest for Quantified Quality
Out of Their Minds: The Lives and Discoveries of 15 Great Computer Scientists.
Qualitative Representation of Spatial Knowledge
Directions in Human Factors for Interactive Systems
Mehrbenutzerkontrolle in Nicht-Standard-Datenbanksystemen
Grup Theoretical Methods in Image Processing
Uncertain Projective Geometry: Statistical Reasoning for Polyhedral Object Reconstruction
Petri-Netz-Methoden und -Werkzeuge: Hilfsmittel zur Entwurfsspezifikation und -validation von Rechensystemen
A Comparative Study of Very Large Data Bases
The Nested Universal Relation Database Model
Anaphora in Natural Language Understanding: A Survey
A problem that all computer-based natural language understanding (NLU) systems encounter is that of linguistic reference, and in particular anaphora (abbreviated reference). For example, in a text as simple as: \begin{quote} Nadia showed Sue her new car. The seats were Day-Glo orange. \end{quote} knowing that ``her'''' probably means Nadia and not Sue and that ``the seats'''' means the seats of Nadia''s new car is not a simple task. .br This thesis is an extensive review of the reference and anaphor problem, and the approaches to it that NLU systems have taken, from early systems such as STUDENT through to current discourse-oriented ones such as PAL. .br The problem is first examined in detail, and examples are given of many different types of anaphor, some of which have been ignored by previous authors. The approaches taken in traditional systems are then described and abstracted and it is shown why they were inadequate, and why discourse theme and anaphoric focus need to be taken into account. The strengths and weaknesses of current anaphora theories and approaches are evaluated. The thesis closes with a list of some remaining research problems. .br The thesis has been written so as to be as comprehensible as possible to both AI workers who know no linguistics, and linguists who have not studied artificial intelligence.
Planen für autonome Montageroboter
Utilizing Problem Structure in Planning, A Local Search Approach
Call-By-Push-Value: A Functional/Imperative Synthesis
Group-Theoretic Algorithms and Graph Isomorphism
Interaktives Entwerfen großer Programmsysteme: Konzepte und Werkzeuge
A Generative Theory of Shape, Questioning Klein's Erlanger Program
Elements of Finite Model Theory
Die Konfigurierung modular aufgebauter Datenbanksysteme
Iterative Software Engineering for Multiagent Systems: The MASSIVE Method
:"The book will serve as a valuable source of reference for R&D professionals active in agent-based computing as well as a gentle and systematic introduction to agent-based systems development and analysis for software engineers and advanced students."--BOOK JACKET.
Composition of Secure Multi-Party Protocols, A Comprehensive Study
Dynamische Integrität von Datenbanken: Grundlagen der Spezifikation und Überwachung
CLU Reference Manual
Synthesising Synchronous Systems by Static Scheduling in Space-Time
Foundations of Logic Programming, 1st Edition
Foundations of Logic Programming, 2nd Edition
Datenbankhandbuch.
Web-Datenbanken: Einsatz objekt-relationaler Datenbanken für Web-Informationssysteme
The Dynamics of Concepts - A Connectionist Model
Parallele Implementierung funktionaler Programmiersprachen
Towards a CSCW Framework for Scientific Cooperation in Europe
ANNA - A Language for Annotating Ada Programs, Reference Manual
Attributierte Grammatiken und Attributierungsalgorithmen
Exceptionbehandlung und Synchronisation - Entwurf und Methode
Ein inhaltsadressierbares Speichersystem zur Unterstützung zeitkritischer Prozesse der Informationswiedergewinnung in Datenbanksystemen
Fehlertolerante dezentrale Prozeßautomatisierung
Entwurf und Realisierung eines Multiprozessors
Fehlerdiagnose für Schaltnetze aus Modulen mit partiell injektiven Pfadfunktionen
Coroutines: A Programming Methodology, a Language Design and an Implementation
Coordinating Plans of Autonomous Agents
PISA: A Programming System for Interactive Production of Application Software
Verteilte Basisalgorithmen
An Approach to Knowledge Base Management
Relevanzanalyse: Eine Kombination von Striktheits- und Datenflußanalyse zur effizienten Auswertung funktionaler Programme
Automated Deduction in Equational Logic and Cubic Curves
Advances in Cryptology 1981-1997, Electronic Proceedings and Index of the CRYPTO and EUROCRYPT Conferences 1981-1997
Data Structures and Algorithms 1: Sorting and Searching
Data Structures and Algorithms 2: Graph Algorithms and NP-Completeness
Data Structures and Algorithms 3
Erweiterung relationaler Datenbanksysteme für technische Anwendungen
Modified Branching Programs and Their Computational Power
WWW - Kommunikation, Internetworking, Web-Technologien
Algorithms and Data Structures in VLSI Design: OBDD - Foundations and Applications
Algorithmen und Datenstrukturen im VLSI-Design: OBDD - Grundlagen und Anwendungen
Randomness and Completeness in Computational Complexity
Computational complexity studies the inherent difficulty of computational problems and the power of the tools we may use to solve them. We focus on randomness and look at several properties of complete problems to investigate its role as well as issues about nondeterminism, alternation, and space versus time. We investigate the use of randomness in the area of proof checking. By derandomizing Arthur-Merlin games, we show that every language with a bounded round interactive proof system has subexponential size proofs unless the polynomial-time hierarchy collapses. This provides the first strong evidence that graph nonisomorphism has subexponential size proofs of membership. Under a stronger hypothesis we can scale the proof size down to polynomial. We also show how our approach applies to several randomized processes other than Arthur-Merlin games. We develop techniques for separating complexity classes by isolating a structural difference between their complete languages. We look at several properties from this perspective: The density of complete languages, the redundancy in complete languages, and the frequency of occurrence of completeness. We show that there is no sparse hard language for polynomial time under logarithmic space reductions with a bounded number of queries unless polynomial time collapses to logarithmic space. The same approach works for various other complexity classes, in the deterministic as well as in the randomized setting. Autoreducibility defines the most general type of efficient reduction of a problem to itself. We show that settling the question whether all complete languages for doubly exponential time are autoreducible would yield major separations: If yes, we have a proof that polynomial time differs from polynomial space, and nondeterministic logarithmic space from nondeterministic polynomial time; if no, we can separate the polynomial-time hierarchy from exponential time. Resource-bounded measure formalizes the notions of scarceness and abundance within complexity classes. From the separation point of view, the theory seems particularly suited for separating randomized polynomial time from exponential time. We develop several strategies using resource-bounded measure aimed at realizing that goal: Showing a difference in measure between the hard languages of these classes, a probabilistic method approach, and the concept of betting games.
Generic Model Management: Concepts and Algorithms
A Calculus of Communicating Systems
Ein Molekül-Atom-Datenmodell für Non-Standard-Anwendungen: Anwendungsanalyse, Datenmodellentwurf und Implementierungskonzepte
Inkonsistenzen in deduktiven Datenbanken: Diagnose und Reparatur
Prinzipien piktorieller Repräsentationssysteme, Untersuchungen zur bildhaften Repräsentation von Wissen in informationsverarbeitenden Systemen
An Introduction to Formal Language Theory
CASL Reference Manual, The Complete Documentation of the Common Algebraic Specification Language
Live Data Structures in Logic Programs: Derivation by Means of Abstract Interpretation
Modular Specification and Verification of Object-Oriented Programs
This book presents new techniques for the formal specification and verification of object-oriented software. Since modularity is of critical importance for reuse and component-based programming, special emphasis is given to the completeness of the presented specification techniques to allow module verification based on the specification of the imported modules. A formal framework developed for a Java subset illustrates these new techniques.
Realistische Computergraphik: Algorithmen, Datenstrukturen und Maschinen
The Design of Intelligent Agents - A Layered Approach
Modular Compiler Verification - A Refinement-Algebraic Approach Advocating Stepwise Abstraction
The Complexity of Simple Computer Architectures
Handbook of Networked and Embedded Control Systems
Transactional Agents: Towards a Robust Multi-Agent System
Negation and Control in Prolog
Constraint-Based Agents
Quality-Driven Query Answering for Integrated Information Systems
The Internet and the World Wide Web are becoming increasingly important in our highly interconnected world. This book addresses the topic of querying the data available, with regard to its quality, in a systematic and comprehensive way, from a database point of view. First, information quality and information quality measures are systematically introduced before ranking algorithms are developed for selecting Web sources for access. The second part is devoted to quality-driven query answering, particularly to query planning methods and algorithms.
Automated Modeling of Physical Systems
The Newton-Cauchy Framework: A Unified Approach to Unconstrained Nonlinear Minimization
Reasoning and Revision in Hybrid Representation Systems
Computation for Metaphors, Analogy, and Agents.
As an introduction to papers in this book we review the notion of metaphor in language, and of metaphor as conceptual, and as primary to understanding. Yet the view of metaphor here is more general. We propose a constructive view of metaphor as mapping or synthesis of meaning between domains, which need not be conceptual ones. These considerations have implications for artificial intelligence (AI), human-computer interaction (HCI), algebraic structure-preservation, constructive biology, and agent design. In this larger setting for metaphor, contributions of the selected papers are overviewed and key aspects of computation for metaphors, analogy and agents highlighted.
Probability, Stocastic Processes, and Queuing Theory - The Mathematics of Computer Performance Modeling.
Probabilistic and Statistical Methods in Cryptology, An Introduction by Selected Topics
Foundations of Inductive Logic Programming
Context-Free Grammars: Covers, Normal Forms, and Parsing
Principles of Artificial Intelligence
Isabelle/HOL - A Proof Assistant for Higher-Order Logic
Filtering, Segmentation and Depth
Temporally Distributed Symptoms in Technical Diagnosis
Textgenerierung aus visuellen Daten: Beschreibungen von Straßenszenen
Computing in Systems Described by Equations
Sprachkonzepte für benutzergerechte Systeme
Chance Discovery
Coordination of Internet Agents: Models, Technologies, and Applications
A Formal Model of Visualization in Computer Graphics Systems
Reliability Evaluation of Some Fault-Tolerant Computer Architectures
Co-ordination in Artificial Agent Societies, Social Structure and Its Implications for Autonomous Problem-Solving Agents.
:This monograph provides a comprehensive survey of the different approaches to coordination in societies of artificial and human agents. Setting out from a critical assessment of the state of the art, the author develops a method of structuring multi-agent applications with a mechanism called structural cooperation. Agents are equipped with expertise about their environment in order to detect and overcome specific types of problem, they make use of their social knowledge to mutually adjust their activities, and they are coerced toward coherent collective behavior through normative rules.
Direct Methods for Sparse Matrices
The Design of Dynamic Data Structures
Constrained Global Optimization: Algorithms and Applications
The Structure of the Relational Database Model
Software Frameworks and Embedded Control Systems
Active Rules in Database Systems
Learning-Based Robot Vision, Principles and Applications
The Deisgn of an Extendible Graph Editor
Isabelle - A Generic Theorem Prover (with a contribution by T. Nipkow)
Synchronisation in zentralisierten Datenbanksystemen: Algorithmen, Realisierungsmöglichkeiten und quantitative Analyse
Data Mining on Multimedia Data
Computer Programs for Spelling Correction: An Experiment in Program Design
Compiling Natural Semantics.
Erzeugung interaktiver Bildverarbeitungssysteme im Dialog: Konzepte, Entwurf und Implementierung eines Dialogsystems für die Bildverarbeitung in der Medizin
Diensteintegrierende Kommunikationsnetze mit teilnehmerüberprüfbarem Datenschutz
Digital Signature Schemes, General Framework and Fail-Stop Signatures
:This book is based on the author's Ph.D. thesis which was selected during the 1995 GI Doctoral Dissertation Competition as the winning thesis in the foundations-of-informatics track. Birgit Pfitzmann did her Ph.D. work at the University of Hildesheim with Professor Joachim Biskup as advisor. Securing integrity for digital communications in the age of global electronic information exchange and electronic commerce is of vital interest for democratic societies and a central technical challenge for cryptologists. As core contribution to advancing the state of the art, the author rigorously develops the new class of digital fail-stop signatures: in contrary to all previously introduced digital signature schemes, these new signatures enable the supposed signer to actually prove forging of the scheme in the case of a successful attack. This monograph is self-contained with respect to the historical background and cryptographic primitives used. For the first time, a general and sophisticated framework is introduced in which previously proposed and the innovative failstop signatures are systematically presented and evaluated, from theoretical foundations up to engineering aspects. Thus the book is compulsory reading for anybody interested in secure digital communication at the professional level.
On-line Error Detection and Fast Recover Techniques for Dependable Embedded Processors
This book presents a new approach to on-line observation and concurrent checking of processors by refining and improving known techniques and introducing new ideas.The proposed on-line error detection and fast recover techniques support and complement other established methods. In combination with other on-line observation priniciples and with a combined hardware-software test, these techniques are used to fulfill a complete self-check scheme for an embedded processor.
Design of Hashing Algorithms
CONLAN Report
Termination Proofs for Logic Programs
Spatial Representation and Motion Planning
Compiler Specification and Verification
Computational Geometry - An Introduction.
Diagnostisches Problemlösen mit Expertensystemen
Model-Checking Based Data Retrieval, An Application to Semistructured and Temporal Data
Multisensordatenverarbeitung in der Robotik
Synchronisation in Mehrrechner-Datenbanksystemen - Konzepte, Realisierungsformen und quantitative Bewertung
Automatic Differentiation: Techniques and Applications
Bounded Incremental Computation
Audio System for Technical Readings.
The advent of electronic documents makes information available in more than its visual form electronic information can now be display-independent. We describe a computing system, ASTER, that audio formats electronic documents to produce audio documents. ASTER can speak both literary texts and highly technical documents (presently in LaTeX) that contain complex mathematics. Visual communication is characterized by the eye''s ability to actively access parts of a two-dimensional display. The reader is active, while the display is passive. This active-passive role is reversed by the temporal nature of oral communication: information flows actively past a passive listener. This prohibits multiple views it is impossible to first obtain a high-level view and then ``look'''' at details. These shortcomings become severe when presenting complex mathematics orally. Audio formatting, which renders information structure in a manner attuned to an auditory display, overcomes these problems. ASTER is interactive, and the ability to browse information structure and obtain multiple views enables active listening.
Aspect-Oriented Database Systems
Design and Control of Workflow Processes: Business Process Management for the Service Industry
FRM: Ein Frame-Repräsentationsmodell und seine formale Semantik: Zur Integration von Datenbank- und Wissensrepräsentationsansätzen
Focusing Solutions for Data Mining: Analytical Studies and Experimental Results in Real-World Domains
Spielbaum-Suchverfahren
By themselves, speech recognition and natural language processing have limited applications, the reason is that each only accomplishes a part of what humans are capable of when they use language. Spoken language systems represent the merger of these two technologies and provide an integrated functionality that more closely approximates humans capabilities in speech communication.
The Traveling Salesman, Computational Solutions for TSP Applications
Petrinetze, Eine Einführung
Systementwurf mit Netzen
Petri Nets: An Introduction
Petrinetze, Eine Einführung, 2. Auflage
Elements of distributed algorithms: modeling and analysis with Petri nets
Recognizing Planar Objects Using Invariant Image Features
Qualitative Spatial Reasoning with Topological Information
Die Interpretation des Verhaltens mehrerer Akteure in Szenenfolgen
Introduction to Constraint Databases
Introduction to Constraint Databases comprehensively covers both constraint-database theory and several sample systems. The book reveals how constraint databases bring together techniques from a variety of fields, such as logic and model thoery, algebraic and computational geometry, and symbolic computation, to the design and analysis of data models and query languages. Constraint databases are shown to be powerful and simple tools for data modeling and querying in application areas---such as environmental modeling, bioinformatics, and computer vision---that are not suitable for relational databases. Specific applications are examined in geographic information systems, spatiotemporal data management, linear programming, genome databases, model checking of automata, and other areas.
Coevolutionary Fuzzy Modeling
The Cray X-MP/Model 24, A Case Study in Pipelined Architecture and Vector Processing
Intelligent Perceptual Systems: New Directions in Computational Perception
Broadband Network Teletraffic - Performance Evaluation and Design of Broadband Multiservice Networks: Final Report of Action COST 242
Neural Nets - A Theory for Brains and Machines
Parallele Systeme
Feedback Shift Registers
Entscheidungsorientiertes Konfigurationsmagament
CyberLaw: The Law of the Internet
Kommunikationskonzepte für verteilte transaktionsorientierte Systeme
A Connotational Theory of Program Structure
Efficient Visual Recognition Using the Hausdorff Distance
Operational Semantics for Timed Systems: A Non-standard Approach to Uniform Modeling of Timed and Hybrid Systems
Interacting Code Motion Transformations: Their Impact and Their Complexity.
Kryptographische Verfahren in der Datenverarbeitung
Computational Cardiology: Modeling of Anatomy, Electrophysiology, and Mechanics
Darstellung und Nutzung von Expertenwissen für ein Bildanalysesystem
Data Compression: The Complete Reference, 3rd Edition
A Relational Theory of Computing
Using Sophisticated Models in Resolution Theorem Proving
Workflow Management Systems for Process Organisations
View Synthesis Using Stereo Vision
This thesis investigates the use of stereo vision for the application of view synthesis. View synthesis --the problem of creating images of a scene as it would appear from novel viewpoints --has traditionally been approached using methods from computer graphics. These methods, however, suffer from low rendering speed, limited achievable realism, and, most severely, their dependence on a global scene model, which typically needs to be constructed manually. In this thesis, we present a new approach to view synthesis that avoids the above problems by synthesizing new views from existing images of a scene. Using an image-based representation of scene geometry computed by stereo vision methods, a global model can be avoided, and realistic new views can be synthesized quickly using image warping. The new application of stereo for view synthesis makes it necessary to re-evaluate the requirements on stereo algorithms. We compare view synthesis to several traditional applications of stereo, and conclude that stereo vision is better suited for view synthesis than for applications requiring explicit 3D reconstruction. We also discuss ways of dealing with partially occluded regions of unknown depth and with completely occluded regions of unknown texture, and present experiments demonstrating that it is possible to efficiently synthesize realistic new views even from inaccurate and incomplete depth information. This thesis also contributes several novel stereo algorithms that are motivated by the specific requirements imposed by view synthesis. We introduce a new evidence measure based on intensity gradients for establishing correspondences between images. This measure combines the notions of similarity and confidence, and allows stable matching and easy assigning of canonical depth interpretations in image regions of insufficient information. We also present new diffusion-based stereo algorithms that are motivated by the need to correctly recover object boundaries. In particular, we develop a novel Bayesian estimation technique that significantly outperforms area-based algorithms using fixed-sized windows. We provide experimental results for all algorithms on both synthetic and real images.
The Automation of Reasoning with Incomplete Information, From Semantic Foundations to Efficient Computation.
Universal Routing Strategies for Interconnection Networks
Bildanalyse allgemeiner Dokumente
Migrationssteuerung und Konfigurationsverwaltung für verteilte objektorientierte Anwendungen
Nonmonotonic Logics, Basic Concepts, Results, and Techniques
Inductive Synthesis of Functional Programs, Universal Planning, Folding of Finite Programs, and Schema Abstraction by Analogical Reasoning
Computational Aspects of an Order-Sorted Logic with Term Declarations
GPSS-FORTRAN, Version II: Einführung in die Simulation diskreter Systeme mit Hilfe eines FORTRAN-Programmpaketes
Meta-Level Control for Deductive Database Systems
Relationen und Graphen
Relations and Graphs - Discrete Mathematics for Computer Scientists
Theorie der logischen Programmierung
Spatial Data Types for Database Systems, Finite Resolution Geometry for Geographic Information Systems
Peer-to-Peer: Ökonomische, technische und juristische Perspektiven, Das aktuelle P2P-Buch.
Complexity and Structure
Generierung von Worthypothesen in kontinuierlicher Sprache
Programming Constraint Services: High-Level Programming of Standard and New Constraint Services
Testmustergenerierung und Fehlersimulation in digitalen Schaltungen mit hoher Komplexität
Objective Coordination in Multi-Agent System Engineering - Design and Implementation
Security Engineering with Patterns - Origins, Theoretical Models, and New Applications
Higher-Level Hardware Synthesis
Paragon: A Language Using Type Hierarchies for the Specification, Implementation and Selection of Abstract Data Types
Multiagent Systems - A Theoretical Framework for Intentions, Know-How, and Communications
Evolution of Parallel Cellular Machines, The Cellular Programming Approach
Matrix Eigensystem Routines - EISPACK Guide, Second Edition
From Logic Design to Logic Programming: Theorem Proving Techniques and P-Functions
Grading Knowledge, Extracting Degree Information from Texts.
"Text Knowledge Extraction" maps natural language texts onto a formal representation of the facts contained in the texts. Common text knowledge extraction methods show a severe lack of methods for understanding natural language "degree expressions", like "expensive hard disk drive" and "good monitor", which describe gradable properties like price and quality, respectively. However, without an adequate understanding of such degree expressions it is often impossible to grasp the central meaning of a text. This book shows concise and comprehensive concepts for extracting degree information from natural language texts. It researches this task with regard to the three levels of (i) analysing natural language degree expressions, (ii) representing them in a terminologic framework, and (iii) inferencing on them byconstrain t propagation. On each of these three levels, the author shows that former approaches to the degree understanding problem were too simplistic, since theyignored byand large the role of the background knowledge involved. Thus, he gives a constructive verification of his central hypothesis, viz. that the proper extraction of grading knowledge relies heavilyon background grading knowledge. This construction proceeds as follows. First, the author gives an overview of the ParseTalk information extraction system. Then, from the review of relevant linguistic literature, the author derives two distinct categories of natural language degree expressions and proposes knowledge-intensive algorithms to handle their analyses in the ParseTalk system. These methods are applied to two text domains, viz. a medical diagnosis domain and a repositoryof texts from information technologymagazines. Moreover, for inferencing the author generalizes from well-known constraint propagation mechanisms. This generalization is especiallyapt for representing and reasoning with natural language degree expressions, but it is also interesting from the point of view where it originated, viz. the field of temporal reasoning. The conclusion of the book gives an integration of all three levels of understanding showing that their coupling leads to an even more advanced -- and more efficient -- performance of the proposed mechanisms.
Handbook on Ontologies
Ausführbare Spezifikation von Directory-Systemen in einer logischen Sprache
Java and the Java Virtual Machine: Definition, Verification, Validation
Datenschutz bei riskanten Systemen: Eine Konzeption entwickelt am Beispiel eines medizinischen Informationssystems
Stateless Core: A Scalable Approach for Quality of Service in the Internet, Winning Thesis of the 2001 ACM Doctoral Dissertation Competition
Test von OSI-Protokollen
Fehlertoleranz in verteilten Realzeitsystemen: Anwendungsorientierte Techniken
Maschinen-unabhängige Code-Erzeugung als semantikerhaltende beweisbare Programmtransformation
Konzepte für eine verteilte wissensbasierte Softwareproduktionsumgebung
A Hierarchical Associative Processing System
Multimedia Database System: Issues and Research Direction
Efficient Checking of Polynomials and Proofs anf the Hardness of Approximation Problems
Turing Machines with Sublogarithmic Space
Ada 95 Reference Manual, Language and Standard Libraries, International Standard ISO/IEC 8652: 1995(E)
Consolidated Ada Reference Manual. Language and Standard Libraries, International Standard ISO/IEC 8652/1995(E) with Technical Corrigendum 1
Artificail Perception and Music Recognition
Finite Representations of CCS and TCSP Programs by Automata and Petri Nets
Flagorientierte Assoziativspeicher und -prozessoren
Towards Dynamic Randomized Algorithms in Computational Geometry
Boolean Caclulus of Differences
P-Functions and Boolean Matrix Factorization: A Unified Approach for Wired, Programmed and Microprogrammed Implementations of Discrete Algorithms
Algorithms for Parallel Polygon Rendering
Challenges for Action Theories
The Computational Complexity of Equivalence and Isomorphism Problems
Visualization of Scientific Parallel Programs
Global Optimization
Current Trends in Concurrency, Overviews and Tutorials
Extraction and Exploitation of Intensional Knowledge from Heterogeneous Information Sources: Semi-Automatic Approaches and Tools
Algorithms on Trees and Graphs
Uncertainty Handling and Quality Assessment in Data Mining
:"Uncertainty Handling and Quality Assessment in Data Mining provides an introduction to the application of these concepts in Knowledge Discovery and Data Mining. It reviews the state-of-the-art in uncertainty handling and discusses a framework for unveiling and handling uncertainty. Coverage of quality assessment begins with an introduction to cluster analysis and a comparison of the methods and approaches that may be used. The techniques and algorithms involved in other essential data mining tasks, such as classification and extraction of association rules, are also discussed together with a review of the quality criteria and techniques for evaluating the data mining results." "This book presents a general framework for assessing quality and handling uncertainty, which is based on tested concepts and theories. This framework forms the basis of an implementation tool, 'UMiner' which is introduced to the reader for the first time." Aimed at IT professionals involved with data mining and knowledge discovery, the work is supported with case studies from epidemiology that illustrate how the tool works in 'real-world' data mining projects. The book would also be of interest to final year undergraduates or post-graduate students looking at databases, algorithms, artificial intelligence and information systems particularly with regard to uncertainty and quality assessment.
Interactive Multimedia Documents: Modeling, Authoring, and Implementation Experiences
Matchmaking in Electronic Markets - An Agent-Based Approach towards Matchmaking in Electronic Negotiations
Planning and Learning by Analogical Reasoning
Closed Object Boundaries from Scattered Points
Intelligent Information Integration for the Semantic Web
Signaturanalyse: Theoretische Grundlagen und Probleme; Ausblick auf Anwendungen
Software-Diversität und ihre Modellierung: Software-Fehlertoleranz und ihre Bewertung durch Fehler- und Kostenmodelle
Modular Construction and Partial Order Semantics of Petri Nets
Relational Matching
Vivid Logic: Knowledge-Based Reasoning with Two Kinds of Negation
Natürlichsprachliche Argumentation in Dialogsystemen: KI-Verfahren zur Rekonstruktion und Erklärung approximativer Inferenzprozesse
Alternating Sequential/Parallel Processing
Integer Optimization by Local Search
Datenbankgestützte Repräsentation und Extraktion von Episodenbeschreibungen aus Bildfolgen
Data Mining in Bioinformatics
The Logic of Information Structures
The Generic Development Language Deva: Presentation and Case Studies
Komplexitätstheorie: Grenzen der Effizienz von Algorithmen
A Methodology for Uncertainty in Knowledge-Based Systems
Revisions- und Konsistenzkontrolle in einer integrierten Softwareentwicklungsumgebung
Models and Tools for Managing Development Processes
Übersetzerbau - Theorie, Konstruktion, Generierung
Übersetzerbau - Theorie, Konstruktion, Generierung, 2. Auflage
Instantiation Theory - On the Foundations of Automated Deduction
Systematische Software-Qualitätssicherung anhand von Qualitäts- und Produktmodellen
Programming in Modula 2
Programmieren in Modula-2
Grammars and L Forms: An Introduction
Artificial Intelligence Today: Recent Trends and Developments
Probabilistische Verfahren für den Test hochintegrierter Schaltungen
Multicast-Kommunikation in verteilten Systemen
Attribute Grammar Inversion and Source-to-source Translation
High-Dimensional Indexing: Transformational Approaches to High-Dimensional Range and Similarity Searches
Association Rule Mining, Models and Algorithms
Due to the popularity of knowledge discovery and data mining, in practice as well as among academic and corporate R&D professionals, association rule mining is receiving increasing attention. The authors present the recent progress achieved in mining quantitative association rules, causal rules, exceptional rules, negative association rules, association rules in multi-databases, and association rules in small databases. This book is written for researchers, professionals, and students working in the fields of data mining, data analysis, machine learning, and knowledge discovery in databases, and for anyone who is interested in association rule mining.
Agent-Based Hybrid Intelligent Systems: An Agent-Based Framework for Complex Problem Solving
Kopplung von Rechnernetzen: Techniken zu Planung, Entwurf, Vermessung und Leistungsoptimierung
Automatische Komplexitätsanalyse funktionaler Programme
Y12M - Solution of Large and Sparse Systems of Linear Algebraic Equations
John Zukowski's Definitive Guide to Swing for Java 2
Compositionality, Concurrency and Partial Correctness - Proof Theories for Networks of Processes, and Their Relationship
Multi-Agent Programming: Languages, Platforms and Applications
Web Dynamics - Adapting to Change in Content, Size, Topology and Use
Understanding Planning Tasks: Domain Complexity and Heuristic Decomposition.
Metrics for Process Models: Empirical Foundations of Verification, Error Prediction, and Guidelines for Correctness.
OMDoc - An Open Markup Format for Mathematical Documents [version 1.2].
Software Visualization - Visualizing the Structure, Behaviour, and Evolution of Software.
Do-All Computing in Distributed Systems: Cooperation in the Presence of Adversity.
Resource Allocation in Wireless Networks: Theory and Algorithms.
Case-Based Approximate Reasoning
Concurrent Zero-Knowledge - With Additional Background by Oded Goldreich
Dissemination of Information in Communication Networks - Broadcasting, Gossiping, Leader Election, and Fault-Tolerance
Algorithms and Data Structures: The Basic Toolbox.
Formal Models of Communicating Systems - Languages, Automata, and Monadic Second-Order Logic
Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data
Web mining aims to discover useful information and knowledge from the Web hyperlink structure, page contents, and usage data. Although Web mining uses many conventional data mining techniques, it is not purely an application of traditional data mining due to the semistructured and unstructured nature of the Web data and its heterogeneity. It has also developed many of its own algorithms and techniques. Liu has written a comprehensive text on Web data mining. Key topics of structure mining, content mining, and usage mining are covered both in breadth and in depth. His book brings together all the essential concepts and algorithms from related areas such as data mining, machine learning, and text processing to form an authoritative and coherent text. The book offers a rich blend of theory and practice, addressing seminal research ideas, as well as examining the technology from a practical point of view. It is suitable for students, researchers and practitioners interested in Web mining both as a learning text and a reference book. Lecturers can readily use it for classes on data mining, Web mining, and Web search. Additional teaching materials such as lecture slides, datasets, and implemented algorithms are available online. The ACM Portal is published by the Association for Computing Machinery. Copyright © 2010 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics Contact Us Useful downloads: Adobe Acrobat QuickTime Windows Media Player Real Player
Data Quality: Concepts, Methodologies and Techniques
Web Services - Concepts, Architectures and Applications
Ada 2005 Rationale: The Language, The Standard Libraries
Ada 2005 Reference Manual. Language and Standard Libraries - International Standard ISO/IEC 8652/1995 (E) with Technical Corrigendum 1 and Amendment 1
Cooperative Bug Isolation (Winning Thesis of the 2005 ACM Doctoral Dissertation Competition).
Secure Transaction Protocol Analysis: Models and Applications
Neural Networks - A Systematic Introduction
Grid Computing, Experiment Management, Tool Integration, and Scientific Workflows
Business Process Management: Concepts, Languages, Architectures
Pedagogically Founded Courseware Generation for Web-Based Learning, An HTN-Planning-Based Approach Implemented in PAIGOS
Kleeme-Algebren Formaler Ausdrücke
Skriptum Informatik: eine konventionelle Einführung, 5. Aufl.
Skriptum Informatik: eine konventionelle Einführung
Skriptum Informatik: eine konventionelle Einführung, 2., durchges. Aufl.
Skriptum Informatik: eine konventionelle Einführung, 3., durchges. und erw. Aufl.
Skriptum Informatik: eine konventionelle Einführung, 4., durchges. Aufl.
Starthilfe Informatik, 2., durchges. Aufl.
Starthilfe Informatik
Algorithmischen Konzepte der Informatik - Berechenbarkeit, Komplexitätstheorie, Algorithmik, Kryptographie
Grundlagen der Programmiersprachen
Effiziente Algorithmen
Mathematische Grundlagen der Informatik - Mathematisches Denken und Beweisen, Eine Einführung
Mathematische Grundlagen der Informatik - Mathematisches Denken und Beweisen, Eine Einführung, 2. Auflage
Transaktionssysteme
Einführung in die Komplexitätstheorie
Archivierung in Datenbanksystemen - Konzept und Sprache
Datenbanksysteme: Konzepte und Modelle
Arithmetik in Rechananlagen
Aufgaben zum Skriptum Informatik
Aufgaben zum Skriptum Informatik, 2., durchges. Aufl.
Backup und Recovery in Datenbanksystemen
Semantik und Programmverifikation
The complexity of Boolean functions
Kompendium Theoretische Informatik - eine Ideensammlung
Effiziente Algorithmen für grundlegende Funktionen (2. Auflage)
Theoretische Informatik - eine algorithmenorientierte Einführung (2. Auflage)
Compilerbau - Eine Einführung
Didaktik der Informatik, mit praxiserprobtem Unterrichtsmaterial.
Didaktik der Informatik, mit praxiserprobtem Unterrichtsmaterial, 2. Auflage.
Expertensysteme für die Planung der Produktion
Grundlagen von Informationssystemen
Deduktive Datenbanken: Eine Einführung aus der Sicht der logischen Programmierung
Anfrageverarbeitung in Datenbanksystemen - Entwurfs- und Implementierungskonzepte
Datenbank-Engineering: Analyse, Entwurf und Implementierung objektrelationaler Datenbanken mit UML, DB2-SQL und Java
Hochleistungs-Transaktionssysteme. Konzepte und Entwicklungen moderner Datenbankarchitekturen.
Bilddatenkompression: Grundlagen, Codierung, MPEG, JPEG
Automatisierung von Terminierungsbeweisen.
Classification and Regression Trees.
The Probabilistic Method
Environmental Systems Research Institute: Understanding GIS - The ARC/INFO Method, 3rd Ed.
Modeling the Internet and the Web: Probabilistic Method and Algorithms
Sampling Techniques
Sampling Techniques
Sampling Techniques, 3rd Edition.
Exploratory Data Mining and Data Cleaning
Compiler Construction for Digital Computers
Modern Compiler Design
Rdb/VMS: Developing the Data Warehouse
Classification Algorithms
Garbage Collection: Algorithms for Automatic Dynamic Memory Management.
Finding Groups in Data: An Introduction to Cluster Analysis.
The Data Warehouse Toolkit: Practical Techniques for Building Dimensional Data Warehouses.
Foundations of Programming Languages
The Essential Distributed Objects Survival Guide.
CORBA Fundamentals and Programming.
Applied Operating System Concepts, First Edition
Operating System Concepts, Sixth Edition
Statistical Decision Funtions.
Theory of Modeling and Simulation
High Performance Parallel Database Processing and Grid Databases
This book targets the theoretical/conceptual details needed to form a base of understanding and then delivers information on development, implementations, and analytical modeling of parallel databases. It includes key information on new developments with grid databases. Also uses a theoretical and practical balance to support in-depth study of parallel query processing offered by modern DBMS as well as hands on experience of parallel query algorithms development, implementation, and analysis.
Creating Metabolic Network Models using Text Mining and Expert Knowledge.
Graph Theoretic Sequence Clustering Algorithms and Their Applications to Genome Comparison.
Introduction to Self-Assembling DNA Nanostructures for Computation and Nanofabrication.
Phyloinformatics and Tree Networks.
High-Grade Ore for Data Mining in 3D Structures.
Exploring RNA Intermediate Conformations with the Massively Parallel Genetic Algorithm.
Mapping Sequence to Rice FPC.
Interrelated Clustering: An Approach for Gene Expression Data Analysis.
Protein Classification: A Geometric Hashing Approach.
The Protein Information Resource for Functional Genomics and Proteomics.
Conservative Extension in Structural Operational Semantics.
Some Pointed Questions Concerning Asymptotic Lower Bounds, and News from the Isomorphism Front.
Theoretical and Experimental DNA Computation.
Theory of Genetic Algorithms.
Propositional Proof Complexity: Past, Present, and Future.
The Underlying Logic of Hoare Logic.
Security Analysis Using Flow Logics.
Quantum Computing and Communication Complexity.
More Infinite Results.
A Machine Model for the Complexity of NP-Approximation Problems.
Functions Versus Algorithms.
Characterizations of Regular Languages in Low Level Complexity Classes.
GETGRATS and APPLIGRAPH: Theory and Applications of Graph Transformation.
Networks of Language Processors.
Networks of Language Processors: Parallel Communicating Systems.
Herbrand's Theorem and Equational Reasoning: Problems and Solutions.
On the Role of Formal Specification Techniques: From TAPSOFT 1985 to ETAPS 2000.
On Formal Semantics and Integration of Object-Oriented Modeling Languages.
Theory and Practice of Software Development: A Review of Driving Forces and Expectations of TAPSOFT from 1985 to 1997.
Algebraic Techniques in Software Development: A Review of Progress up to the Mid Nineties.
Dynamic Abstract Data Types: An Informal Proposal in 1994.
Integration Paradigm for Data Type and Process Specification Techniques.
From Basic Views and Aspects to Integration of Specification Formalisms.
Why Evolutionary Algorithms?
Diagonalization.
What is Branching Time Semantics and Why to Use it?
Why are Modal Logics so Robustly Decidable?
On a Reference Model for the Formalization and Integration of Software Specification Languages.
Homotopy and Concurrency.
AMAST'91 Banquet Talk.
The Value, if Any, of Decidability.
Platonism, Constructivism, and Computer Proofs vs. Proofs by Hand.
From Invariants to Canonization.
The Sequential ASM Thesis.
An Introduction to Quantum Computing.
On Slender Languages.
The D0L Problem Revisited.
Progress in Descriptive Complexity.
DNA Computers: Tomorrow's Reality.
The Genomics Revolution and its Challenges for Algorithmic Research.
Natural Data Mining Techniques.
Simple Words in Equality Sets.
It is well known that equality sets between two morphisms possess a remarkable generative capacity: an arbitrary recursively enumerable set is obtained from an equality set by certain simple operations. Interconnections between simplicity of computations and structural primitivity of words in equality sets have also been observed. The paper discusses recent work in this area, pointing out certain open problems and emphasizing new directions for research.
Twelve Problems in Resource-Bounded Measure.
Neural Computation: A Research Topic for Theoretical Computer Science? Some Thoughts and Pointers.
Words on Trajectories.
Many-Valued Truth Functions, Cernys' Conjecture, and Road Coloring.
Lindenmayer and DNA: Watson-Crick D0L Systems.
CoFI: The Common Framework Initiative for Algebraic Specification and Development.
Classification of Petri Nets Using Adjoint Functors.
Does Concurrency Theory Have Anything to Say About Parallel Programming?
Splicing: A Challenge for Formal Language Theorists.
Computing with Membranes (P Systems): An Introduction.
Logic on Words.
Towards Global Computations Guided by Concurrency Theory.
The Complexity of Propositional Proofs.
Current Trends in Theoretical Computer Science, Entering the 21th Century
String Searching Algorithms
Computational Biology and Genome Informatics
Analysis of Biological Data: A Soft Computing Approach
In Silico Design of Ligands Using Properties of Target Active Sites.
Sophisticated Methods for Cancer Classification Using Microarray Data.
A Reliable Classification of Gene Clusters for Cancer Samples Using a Hybrid Multi-Objective Evolutionary Procedure.
Reconstructing Phylogenies with Memetic Algorithms and Branch-and-Bound.
An Introduction to Soft Computing.
Beyond String Algorithms: Protein Sequence Analysis Using Wavelet Transforms.
Multiobjective Evolutionary Approach to Fuzzy Clustering of Microarray Data.
Inferring Regulations in a Genomic Network from Gene Expression Profiles.
Distill: A Machine Learning Approach to Ab Initio Protein Structure Prediction.
Filtering Protein Surface Motifs Using Negative Instances of Active Sites Candidates.
Bioinformatics: Mining the Massive Data from High Throughput Genomics Experiments.
Classification of RNA Sequences with Support Vector Machines.
Feature Selection for Cancer Classification Using Ant Colony Optimization and Support Vector Machines.
Six Degrees: The Science of a Connected Age.
Information Retrieval Interaction.
Standard Codecs: Image Compression to Advanced Video Coding.
Neuronale Netze - Eine Einführung in die Grundlagen, Anwendungen und Datenauswertung
The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines
Real-time Active Range Finder Using Light Intensity Modulation.
Strategies for Registering Range Images from Unknown Camera Positions.
High-resolution Ultrafast 3D Imaging.
Optoelectronic dimensional Integral Inspection of Hollow Cylinder-type Articles for conveyor Line.
Development of a 3D Digitizer for Breast Surgery Procedures.
Automatic Reconstruction of Large 3D Models of Real Environments from Unregistered Data-sets.
Three-line High-power Three-dimensional Sensor.
Multispectral Pattern Projection Range Finder.
High-speed Three-dimensional Laser Sensor.
3D Range Optical Sensor: analysis of the Measurement Errors and Development of Procedures for Their Compensation.
3D Profilometry Using a Dynamically Configurable Confocal Microscope.
Examining Laser Triangulation System Performance Using a Software Simulation.
Effect of Sway on Image Fidelity in Whole-body Digitizing.
Moly: A Prototype Handheld 3D Digitizer with Diffraction Optics.
Reconstruction of the Surface of the Human Body from 3D Scanner Data Using 13-splines.
Reverse Engineering Using Optical 3D Sensors.
Wrapping 3D Scanning Data.
Spherical Harmonic Surface Representation with Feedback Control.
Efficient Free-form Surface Representation with Application in Orthodontics.
Real-time 3D Shape Measurement with Digital Stripe Projection by Texas Instruments Micro Mirror Devices DMD.
Multiscale Analysis of 3D Surface Image: Application to Clam Shell Characterization.
Multiple Structured Light System for the 3D Measurement of Feet.
Pose and Motion Estimation Using Dual quaternion-based Extended Kalman Filtering.
Direct Estimation of 3D Motion Parameters and Relative Depth Using the Minimum Description Length Principle.
Depth-based Selective Image Reconstruction Using Spatiotemporal Image Analysis.
Toward a Handheld Laser Range Scanner: Integrating Observation-based Motion Compensation.
Slicing, Fitting, and Linking (SFL): A Modular Triangulation Approach.
3D Object Reconstruction from a Sequence of Images Using Voxel Coloring.
3D Profiling by Optical Demodulation with an Image Intensifier.
Restoration of Broken Earthenware Using Close Range Photogrammetry and a CAD System.
Real-time 3D Reconstruction System using CAM-based Highly Parallel Processing Board.
Novel Fully Integrated Computer System for Custom Footwear: from 3D Digitization to Manufacturing.
Reconstruction of Complete 3D Object Model from Multiview Range Images.
Color Digitizing and Modeling of Free-form 3D Objects.
Object Modeling in Multiple-object 3D Scene Using Deformable Simplex Meshes.
Robust 3D Reconstruction System for Human Jaw Modeling.
Acquisition of 3D Image Representation in Multimedia Ambiance Communication using 3D Laser Scanner and Digital Camera.
New Approach for the Modeling and Smoothing of Scattered 3D Data.
Fast Shape from Focus Using Dynamic Programming.
Axi-vision Camera: A Three-dimensional Camera.
Real-time Structured Light Depth Extraction.
Impact of Intensity Edge map on Segmentation of Noisy Range Images.
Interpolation of Ray-space Data by Adaptive Filtering.
Three Steps to Make Shape from Shading Work Consistently on Real Scenes.
Robust Cooperation Concept for Low-level Vision Modules.
Error Sensitivity of Rotation Angles in the ICP Algorithm.
3D Surface Real-time Measurement Using Phase-shifted Interference Fringe Technique for Craniofacial Identification.
Automated Fudicial Labeling on Human Body Data.
Segmenting 3D Surface Scan Data of the Human Body by 2D Projection.
Extracting Surface Area Coverage by Superimposing 3D Scan Data.
Entropy of Profile Sections to Estimate the Next Sensor Position.
Optimizing Triangular Mesh Generation from Range Images.
CyberModeler: A Compact 3D Scanner Based on Monoscopic Camera.
Adaptive Area-based Stereo Matching.
Gel Tomography for 3D Acquisition of Plant Root Systems.
Real-time Monitoring of Icebreaker Propeller Blades' Ice Load Using Underwater Laser ranging System.
Scanning Projection Grating Moire Topography.
ShapeGrabber FootScanner: A Low Cost High Accuracy 3D System for the Acquisition of Human Feet.
3D Reconstruction, Visualization, and Measurement of MRI Images.
Generating Animated Sequences from 3D Whole-body Scans.
Tilted Planes in 3D Image Analysis.
Monitoring and Measurement of Movement of Objects by Fringe Projection Method.
Parameters Matching of Objects in Video Sequences.
Optimizing Random Patterns for Invariants-based Identification.
Three-dimensional Scene Reconstruction from Images.
3D Shape Initialization of Objects in Multiview Image Sequences.
3D Textured Models of Indoor Scenes from Composite Range and Video Images.
Three-dimensional Shape Reconstruction from Two-dimensional Images Using Symmetric Camera Location.
New Class Library for Animation of Voxel Humans.
Opto-numerical Methods of Data Acquisition for Computer Graphics and Animation Systems.
VIRO 3D: Tast Three-dimensional Full-body Scanning for Humans and Other Living Objects.
High-resolution Triangulation of Arbitrary Shaped Surfaces based on Coordinate Curves.
Differential Motions for Recovering 3D Structure and Motions from an Unstructured Environment.
Recreation of Three-dimensional Objects in a Real-time Simulated Environment by Means of a Panoramic Single Lens Stereoscopic Image-Capturing Device.
Volumetric Apparel for Visible Female.
Gaussian Scale-Space Dense Disparity Estimation with Anisotropic Disparity-Field Diffusion.
We present a new reliable dense disparity estimation algorithm which employs Gaussian scale-space with anisotropic disparity-field diffusion. This algorithm estimates edge-preserving dense disparity vectors using a diffusive method on iteratively Gaussian-filtered images with a scale, i.e. the Gaussian scalespace. While a Gaussian filter kernel generates a coarser resolution from stereo image pairs, only strong and meaningful boundaries are adaptively selected on the resolution of the filtered images. Then, coarse global disparity vectors are initialized using the boundary constraint. The per-pixel disparity vectors are iteratively obtained by the local adjustment of the global disparity vectors using an energy-minimization framework. The proposed algorithm preserves the boundaries while inner regions are smoothed using anisotropic disparity-field diffusion. In this work, the Gaussian scale-space efficiently avoids illegal matching on a large baseline by the restriction of the range. Moreover, it prevents the computation from iterating into local minima of ill-posed diffusion on large gradient areas e.g. shadow and texture region, etc. The experimental results prove the excellent localization performance preserving the disparity discontinuity of each object.
Projection-Based Registration Using a Multi-View Camera for Indoor Scene Reconstruction.
A registration method is proposed for 3D reconstruction of an indoor environment using a multi-view camera. In general, previous methods have a high computational complexity and are not robust for 3D point cloud with low precision. Thus, a projection-based registration is presented. First, depth are refined based on temporal property by excluding 3D points with a large variation, and spatial property by filling holes referring neighboring 3D points. Second, 3D point clouds acquired at two views are projected onto the same image plane, and two-step integer mapping enables the modified KLT to find correspondences. Then, fine registration is carried out by minimizing distance errors. Finally, a final color is evaluated using colors of corresponding points and an indoor environment is reconstructed by applying the above procedure to consecutive scenes. The proposed method reduces computational complexity by searching for correspondences within an image plane. It not only enables an effective registration even for 3D point cloud with low precision, but also need only a few views. The generated model can be adopted for interaction with as well as navigation in a virtual environment.
Registration of Multiple Range Scans as a Location Recognition Problem: Hypothesis Generation, Refinement and Verification.
This paper addresses the following version of the multiple range scan registration problem. A scanner with an associated intensity camera is placed at a series of locations throughout a large environment; scans are acquired at each location. The problem is to decide automatically which scans overlap and to estimate the parameters of the transformations aligning these scans. Our technique is based on (1) detecting and matching keypoints ¿ distinctive locations in range and intensity images, (2) generating andrefining a transformation estimate from each keypoint match, and (3) deciding if a given refined estimate is correct. While these steps are familiar, we present novel approaches to each. A new range keypoint technique is presented that uses spin images to describe holes in smooth surfaces. Intensity keypoints are detected using multiscale filters, described using intensity gradient histograms, and backprojected to form 3D keypoints. A hypothesized transformation is generated by matching a single keypoint from one scan to a single keypoint from another, and is refined using a robust form of the ICP algorithm in combination with controlled region growing. Deciding whether a refined transformation is correct is based on three criteria: alignment accuracy, visibility, and a novel randomness measure. Together these three steps produce good results in test scans of the Rensselaer campus.
In Process 3D-Sensing for Laser Material Processing.
From Coarse to Fine Correspondence of 3-D Facial Images and its Application to Facial Caricaturing.
Automatic Class Selection and Prototyping for 3-D Object Classification.
Most research on 3-D object classification and recognition focuses on recognition of objects in 3-D scenes from a small database of known 3-D models. Such an approach does not scale well to large databases of objects and does not generalize well to unknown (but similar) object classification. This paper presents two ideas to address these problems (i) class selection, i.e., grouping similar objects into classes (ii) class prototyping, i.e., exploiting common structure within classes to represent the classes. At run time matching a query against the prototypes is sufficient for classification. This approach will not only reduce the retrieval time but also will help increase the generalizing power of theclassification algorithm. Objects are segmented into classes automatically using an agglomerative clustering algorithm. Prototypes from these classes are extracted using one of three class prototyping algorithms. Experimental results demonstrate the effectiveness of the two steps in speeding up the classification process without sacrificing accuracy.
Weighted Cone-Curvature: Applications for 3D Shapes Similarity.
Capturing 2½D Depth and Texture of Time-Varying Scenes Using Structured Infrared Light.
Accurate Principal Directions Estimation in Discrete Surfaces.
Accurate local surface geometry estimation in discrete surfaces is an important problem with numerous applications. Principal curvatures and principal directions can be used in applications such as shape analysis and recognition, object segmentation, adaptive smoothing, anisotropic fairing of irregular meshes, and anisotropic texture mapping. In this paper, a novel approach for accurate principal direction estimation in discrete surfaces is described. The proposed approach is based on local directional curve sampling of the surface where the sampling frequency can be controlled. This local model has a large number of degrees of freedoms compared with known techniques and so can better represent the local geometry. The proposed approach is quantitatively evaluated and compared with known techniques for principal direction estimation. In order to perform an unbiased evaluation in which smoothing effects are factored out, we use a set of randomly generated Bezier surface patches for which the principal directions can be computed analytically.
3D Modeling System of Human Face and Full 3D Facial Caricaturing.
This paper proposes a method for modeling 3D face from the 2D facial images captured from the surrounding 2D cameras by which the color texture and surface shape information of the face are synchronously measured. And 3D facial caricaturing method is proposed by using the 3D(the polygon data) face model. Automatic method for extracting regions of the facial parts is technically proposed, and the feature points are extracted from those regions. We propose the mesh model composed of 44 feature points and 82 meshes to cover a head. To generate the caricature from this polygon data, the individuality feature is defined in value by the difference of the feature points between the input face and the mean face, which was defined from the average of many input faces.
Colour Texture Fusion of Multiple Range Images.
On the Detection of Feature Points of 3D Facial Image and its Application to 3D Facial Caricature.
3D Animation Of Cerebral Activity Using Both Spatial And Temporal fMRI Information.
Interactive Shape Acquisition using Marker Attached Laser Projecto.
CAD and Vision in Rangefinder-based Dimensional Metrology.
A concept of CAD model-based automated 3D measurement and the evaluation of its feasibility in cases drawn from industrial needs. Automated 3D measurement can be seen as comprising two steps: measurement planning and measurement execution. The concept was evaluated by implementing a graphical measurement planning tool and two automatic optical measurement systems equipped with vision systems for sensory feedback. The operation chain from CAD model-based measurement planning to comparison between the measured and designed geometries was demonstrated. Successful experiments with automatic operation controlled by the measurement plan and vision guidance were carried out and the key performance criteria were met.
Uncalibrated Multiple Image Stereo System with Arbitrarily Movable Camera and Projector for Wide Range Scanning.
In this paper, we propose an uncalibrated, multi-image 3D reconstruction, using coded structured light. Normally, a conventional coded structured light system consists of a camera and a projector and needs precalibration before scanning. Since the camera and the projector have to be fixed after calibration, reconstruction of a wide area of the scene or reducing occlusions by multiple scanning are difficult and sometimes impossible. In the proposed method, multiple scanning while moving the camera or the projector is possible by applying the uncalibrated stereo method, thereby achieving a multi-image 3D reconstruction. As compared to the conventional coded structured light method, our system does not require calibration of extrinsic camera parameters, occlusions are reduced, and a wide area of the scene can be acquired. As compared to image-based multi-image reconstruction, the proposed system can obtain dense shape data with higher precision. As a result of these advantages, users can freely move either the cameras or projectors to scan a wide range of objects, but not if both the camera and the projector are moved at the same time.
Three-Dimensional Reconstruction of the Bony Structures involved in the Articular Complex of the Human Shoulder Using Shape-Based Interpolation and Contour-Based Extrapolation.
Virtual Environment Modeling by Integrated Optical and Acoustic Sensing.
AVENUE: Automated Site Modeling in Urban Environments.
OSCAR: Object Segmentation Using Correspondence and Relaxation.
Efficient Discovery Service for a Digital Library of 3D Models.
Many geographically distributed experts in different areas such as medical imaging, e-commerce, and digital museums, are in need of 3D models. Although 3D models are becoming widely available due to the recent technological advancement and modeling tools, we lack a digital library system where they can be searched and retrieved efficiently. In this paper we focus on an efficient discovery service consisting of multi-level hierarchical browsing service that enables users to navigate large sets of 3D models. For this purpose, we use shape based clustering to abstract a large set of 3D models to a small set of representative models (key models). Our service applies clustering recursively to limit the number of key models that a user views at a time. Clustering is derived from metrics that are based on a concept of compression and similarity computation using surface signatures. Signatures are the two-dimensional representations of a 3D model and they can be used to define similarity between 3D models. We integrated the proposed browsing capability with 3DLIB,(a digital library for 3-D models that we are building at Old Dominion University), and evaluated the proposed browsing service using the Princeton Shape Benchmark (PSB). Our evaluation shows significant better precision and recall as compared to other approaches.
Industrial Painting Inspection using Specular Sharpness.
Digital 3D plus color imaging is a growing field as it has many industrial applications. One of them is the precise understanding of optical properties of objects, which is possible to obtain from 3D plus brightness or color data. Those properties are usually important inspection criteria; for example, the specular sharpness of a coatted surface is usually related to the coating quality. This paper is related to a procedure for recovering the specular sharpness and other surface-related properties from objects. The current approach works on 3D plus brightness or color data, which are retrieved in two steps using a common structured-light triangulation-based technique and an ordinary non-structured spot light. Some experimental results obtained on industrial parts are also shown.
Automatic 3D modeling of palatal plaster casts.
Geometrically Stable Sampling for the ICP Algorithm.
On-Line Hand-Eye Calibration.
Multiresolution Interactive Modeling with Efficient Visualization.
3D interactive modeling from range data aims at simultaneously producing and visualizing the surface model of an object while data is collected. The current research challenge is producing the final result in real-time. Using a recently proposed framework, a surface model is built in a volumetric structure encoding a vector field in the neighborhood of the object surface. In this paper, it is shown that the framework allows one to locally control the model resolution during acquisition. Using ray tracing, efficient visualization approaches of the multiresolution vector field are described and compared. More precisely, it is shown that volume traversal can be optimized while preventing holes and reducing aliasing in the rendered image.
A Robust Image-Based Method for 3D Registration.
Today modeling from reality receives more and more attention. In this paper, we present a novel image-based 3D registration method. Compared with a previous one it does not require accurate starting position, albedo and geometric invariants, but has to address the more apparent mismatch problems. First, distinctive corner points, which act as salient features for subsequent image matching, are detected via the minimal eigenvalue of the auto-correlation matrix. Then, a verification scheme discards the potential mismatches by thresholds of the correlation coefficients and point-to-point distances. Experimental results demonstrate the superiority of our proposed verification scheme to the previous one using only correlation coefficients under the relaxed conditions.
A 3D Laser Micro-sensor Integrating Control and Data Processing in an FPGA-Based Calculator.
Hierarchical Coarse to Fine Depth Estimation for Realistic View Interpolation.
This paper presents a novel approach for view synthesis and image interpolation. The algorithm is build up in a hierarchical way, and this on different structural levels instead of using a classic image pyramid. First coarse matching is done on a ýshape basisý only. A background-foreground segmentation yields a fairly accurate contour for every incoming video stream. Inter-relating these contours is a 1D problem and as such very fast. This step is then used to compute small position dependent bounding-boxes in 3D space which enclose the underlying object. The next step is a more expensive window based matching, within the volume of these bounding-boxes. This is limited to a number of regions around ýpromisingý feature points. Global regularisation is obtained by a graph cut. Speed results here from limiting the number of feature points. In a third step the interpolation is ýpre-renderedý and simultaneously evaluated on a per pixel basis. This is done by computing a Birchfield dissimilarity measure on the GPU. Per pixel parallelised operations keep computational cost low. Finally the bad interpolated parts are ýpatchedý. This per pixel correction yields the final interpolated view at the finest level. Here we will also deal explicitly with opacity at the borders of the foreground object.
3D Modeling of Outdoor Environments by Integrating Omnidirectional Range and Color Images.
This paper describes a 3D modeling method for wide area outdoor environments which is based on integrating omnidirectional range and color images. In the proposed method, outdoor scenes can be efficiently digitized by an omnidirectional laser rangefinder which can obtain a 3D shape with high-accuracy and by an omnidirectional multi-camera system (OMS) which can capture a high-resolution color image. Multiple range images are registered by minimizing the distances between corresponding points in the different range images. In order to register multiple range images stably, points on plane portions detected from the range data are used in registration process. The position and orientation acquired by RTK-GPS and gyroscope are used as initial values of simultaneous registration. The 3D model obtained by registration of range data is mapped by textures selected from omnidirectional images in consideration of the resolution of texture and occlusions of the model. In experiments, we have carried out 3D modeling of our campus with the proposed method.
Detecting Cylinders in 3D Range Data Using Model Selection Criteria.
In this paper, we use a model selection criterion to decide whether two cylinders should be merged as a single cylinder or they should be left separated. We compare and evaluate an extensive number of different model selection criteria for this purpose and examine which factors can affect their performance. We conclude that SSC, GIC, MCAIC and CAIC have a better performance (for this particular application) compared to the other criteria.
Constructing Models of Articulating Objects: Range Data Partitioning.
In this paper we consider one aspect of the problem of automatically building shape models of articulating objects from example range images. Central to the model construction problem is the registration of range data, taken from different vantage points, into a common coordinate frame. This involves determining a transformation for each set of range data which aligns overlapping surface points in the common frame. Current registration algorithms have been developed specifically for rigid objects, but it is not obvious how these can be extended to articulated or more generally deformable objects. Here, we propose that range images of articulated objects are first segmented into their rigid subcomponents. Each subcomponent can then be registered in isolation using the existing algorithms designed specifically for rigid parts and the final model formed by reassembling all of the submodels. This has motivated the development of a rigid part segmentation algorithm which is described and demonstrated here. The algorithm is currently limited to non-umbilic surfaces, but in this more restricted domain is shown to work well.
Human Identification from Body Shape.
Extracting Main Modes of Human Body Shape Variation from 3-D Anthropometric Data.
Characterizing the variations of the human body shape is fundamentally important to many applications ranging from animation to product design. 3-D scanning technology makes it possible to digitize the complete surfaces of a large number of human bodies, providing much richer information about the body shape than the traditional anthropometric measurements. This technology opens up opportunities to extract new measurements for quantifying the body shape. Using the data from the first large scale 3-D anthropometric survey, the CAESAR project, we demonstrate that the human body shape can be represented by a small number of principal components. Principal Component Analysis extracts orthogonal basis vectors, called eigenpersons, from the space of body shapes. The shape of any individual person can then be expressed by the linear combination of the basis vectors. We demonstrate that some of these components correspond to the commonly used body measurements like height and weight and others indicate new ways of charactering body shape variations. We develop tools to visualize the changes of the body shape along the main components. These tools help understand the meaningful components of the human body shape.
A Method for the Registration of Attributed Range Images.
Data Acquisition and Representation of Mechanical Parts and Interfaces to Manufacturing Devices.
In this paper we address data acquisition, data represen- tation and interfaces to manufacturing devices as they relate to automatic creation of electronic files for representing the complete geometry of an arbitrarily shaped part.
Accuracy of 3D Range Scanners by Measurement of the Slanted Edge Modulation Transfer Function.
Design Considerations for a Range Image Sensor Containing a PSD-array and An On-chip Multiplexer.
In the present paper we introduce a range image sensor, the PSD-chip, designed for sheet of light range imaging. The image sensor consists of an array of 128 Position Sensitive Detector (PSD)-strips with a 20 mm length and a 28 /spl mu/m pitch. Design considerations for the image sensor are discussed, as well as the on-chip electronics. The on-chip electronics consists of an analog part and a digital part. The analog preamplifiers deal with the low-pass filtering of the sensor-signals in order to reduce the noise bandwidth. The digital part consists of an analog current multiplexer, implemented in ECL technology. Our goal is to achieve a 2 MHz range/frequency at 12 bits resolution.
Image-Gradient-Guided Real-Time Stereo on Graphics Hardware.
We present a real-time correlation-based stereo algorithm with improved accuracy. Encouraged by the success of recent stereo algorithms that aggregate the matching cost based on color segmentation, a novel image-gradient-guided cost aggregation scheme is presented in this paper. The new scheme is designed to fit the architecture of recent graphics processing units (GPUs). As a result, our stereo algorithm can run completely on the graphics board: from rectification, matching cost computation, cost aggregation, to the final disparity selection. Compared with many real-time stereo algorithms that use fixed windows, noticeable accuracy improvement has been obtained without sacrificing realtime performance. In addition, existing global optimization algorithms can also benefit from the new cost aggregation scheme. The effectiveness of our approach is demonstrated with several widely used stereo datasets and live data captured from a stereo camera.
Structure and Motion from Two Uncalibrated Views Using Points on Planes.
Automatic CAD Modeling of Industrial Pipes from Range Images.
We present in this paper a method to obtain automatically CAD models of industrial pipes from range images. The models are based on two geometric primitives, cylinders and torii, which are enough to represent most parts of the pipes. The images are obtained with an accurate long-distance laser range sensor developed for reverse engineering in industrial structures. The key issue for automatic CAD modeling is the automatic segmentation of the data into subsets of points corresponding to the desired primitives. To do so, we use differential geometry to segment lines of centers of curvature into straight and curved parts, corresponding to the cylinder and torus parts of the original image. Differential geometry results being noisy and biased, we use an optimal approach for the computation of centers of curvature.
Fast Global Registration of 3D Sampled Surfaces using a Multi-Z-Buffer Technique.
We present a new method for the global registration of several overlapping 3D surfaces sampled on an object. The method is based on the ICP (iterative closest point) algorithm and on a segmentation of the sampled points in an optimized set of z-buffers. This multi-z-buffer technique provides a 3D space partitioning which greatly accelerates the search of the nearest neighbours in the establishment of the point-to-point correspondence between overlapping surfaces. Then a randomized iterative registration is processed on the surface set. We have tested an implementation of this technique on real sampled surfaces. It appears to be rapid accurate and robust, especially in the case of highly curved objects.
3-D Vision Technology for Occupant Detection and Classification.
This paper describes a 3-D vision system based on a new 3-D sensor technology for the detection and classification of occupants in a car. New generation of so-called "smart airbags" require the information about the occupancy type and position of the occupant. This information allows a distinct control of the airbag inflation. In order to reduce the risk of injuries due to airbag deployment, the airbag can be suppressed completely in case of a child seat oriented in reward direction. In this paper we propose a 3-D vision system based on a 3-D optical time-of-flight (TOF) sensor, for the detection and classification of the occupancy on the passenger seat. Geometrical shape features are extracted from the 3-D image sequences. Polynomialclassifier is considered for the classification task. A comparison of classifier performance with principle components (eigenimages) is presented. This paper also discuss the robustness of the features with variation of the data. The full scale tests have been conducted on a wide range of realistic situations (adults/children/child seats etc.) which may occur in a vehicle.
Digital 3D Imaging System for Rapid Response on Remote Sites.
Is Appearance-Based Structure from Motion Viable?
Using k-d Trees for Robust 3D Point Pattern Matching.
Optimized Position Sensors for Flying-Spot Active Triangulation Systems.
Registration and Fusion of Intensity and Range Data for 3D Modelling of Real World Scenes.
Automatic Editing and Curve-fitting of 3-D Surface Scan Data of the Human Body.
This paper presents an automatic method to trim arms from the surface scan data of the human body. Curve and surface approximation are employed to refill the data gap after the trimming process. This delivers a more realistic torso model for further applications.
Object Model Creation from Multiple Range Images: Acquisition, Calibration, Model Building and Verification.
This paper demonstrates the accuracy of a prototype Laser Range Camera (LRC) developed at the National Research Council of Canada for the creation of models of real objects. A laser survey performed in collaboration with the Canadian Space Agency and NASA is used as a test case. The object selected for this particular test case is the Orbiter Docking System (ODS) located at the Kennedy Space Center, Florida. During the laser survey, 128 range (and registered intensity) images were acquired all around the ODS. These images were then processed in our laboratory. A full model of the top portion of the ODS was created along with an almost complete model of the ODS. The ODS has a diameter of 1.6 m and a height of 3.9 m. Targets mounted on the top portion of the ODS were used to assess the accuracy of the calibration and of the image registration process. These targets were measured with a network of theodolites a day prior to the laser survey and used as a reference. With the current calibration and range image registration techniques, an accuracy better than 0.25 mm in X and Y, and, 0.80 mm in Z was achieved. These results compare favorably with the single point accuracy obtained after calibration, i.e., about 0.25 mm in X and Y, and, 0.50 mm in Z. These figures and others should testify on the usefulness of a LRC for accurate model building.
A Scene Analysis System for the Generation of 3-D Models.
A scene analysis system for the 3-D modeling of objects is presented. It combines surface reconstruction techniques with object recognition for the generation of 3-D models for computer graphic applications. The system permits the insertion of highlevel constraints, like a specific angle between two house walls, in an explicit knowledge base implemented as a semantic net. The applicability of those constraints is proved by asserting and testing hypotheses in an interpretation phase. In the case of rejection a more general constraint or model is selected. The capabilities of the system were shown for the modeling of buildings using depth from stereo and contour information. The system reconstructs the surface of the scene objects using the constraints selected in the prior interpretation.
Real-Time Geometrical Tracking and Pose Estimation Using Laser Triangulation and Photogrammetry.
Efficient and Reliable Template Set Matching for 3D Object Recognition.
Recursive Model Optimization Using ICP and Free Moving 3D Data Acquisition.
A Nearest Neighbor Method for Efficient ICP.
A Geometric Approach to the Segmentation of Range Images.
Approximate K-D Tree Search for Efficient ICP.
Accuracy of 3D Scanning Technologies in a Face Scanning Scenario.
In this paper, we will review several different 3D scanning devices. We will present a method for empirical accuracy analysis, and apply it to several scanners providing an overview of their technologies. The scanners include both general purpose and face specific scanning devices. We will focus on face scanning technique, although the technique should be applicable to other domains as well. The proposed method involves several different calibration faces of known shape and comparisons of their scans to investigate both absolute accuracy and repeatability.
Hand Posture Estimation from 2D Monocular Image.
Hierarchical Segmentation of Range Images with Contour Constraints.
This paper describes a new algorithm to segment in continuous parametric regions range images. The algorithm starts with an initial partition of small first order regions using a robust fitting algorithm constrained by the detection of depth and orientation discontinuities. The algorithm then optimally group these regions into larger and larger regions using parametric functions until an approximation limit is reached. The algorithm uses Bayesian decision theory to determine the local optimal grouping and the complexity of the parametric model used to represent the range signal. After the segmentation process an exact description of the boundary of each region is computed from the mutual intersections of the extracted surfaces. Experimental results show significant improvement of region boundary localization. A systematic comparison of our algorithm to the most well known algorithm in the literature is presented to highlight the contributions of this paper.
Reliable 3D Surface Acquisition, Registration and Validation Using Statistical Error Models.
Non-Parametric 3D Surface Completion.
We consider the completion of the hidden or missing portions of 3D objects after the visible portions have been acquired with 2½D (or 3D) range capture. Our approach uses a combination of global surface fitting, to derive the underlying geometric surface completion, together with an extension, from 2D to 3D, of non-parametric texture synthesis in order to complete localised surface texture relief and structure. Through this combination and adaptation of existing completion techniques we are able to achieve realistic, plausible completion of 2½D range captures.
Robust Surface Matching for Registration.
Micro-Stereoscopic Vision System for the Determination of Air Bubbles and Aqueous Droplets Content within Oil Drops in Simulated Processes of Multiphase Fermentations.
Industrial fermentation procedures involve the mixing of multiple phases (solid, liquid, gaseous), where the interfacial area between the phases (air bubbles, oil drops and aqueous medium) determines the nutrients transfer and hence the performance of the culture. Interactions between phases occur, giving rise to the formation of complex structures containing air bubbles and small drops from the aqueous phase, trapped in oil drops (water-in-oil-in-water). A two-dimensional observation of this phenomenon may lead to an erroneous determination of the phenomena occurring since bubbles and droplets coming from different focal planes may appear overlapped. In the present work, an original strategy to solve this problem is described. Micro-stereoscopic on-line image acquisition techniques have been used, so as to obtain accurate images from the cultures for further three-dimensional analysis. Using this methodology, the three-dimensional spatial position of the trapped bubbles and droplets moving at high speed can be calculated in order to determine their relative concentration. To evaluate the accuracy of this technique, the results obtained with our system have been compared with those obtained by an expert. An agreement of 95% was achieved. Also, this technique was able to evaluate 14% more bubbles and droplets corresponding to overlaps that the expert was not able to discern in non-stereoscopic images.
3D optical scanning diagnostics for Leonardo Da Vinci's "Adorazione dei Magi" conservation.
Unsupervised 3D Object Recognition and Reconstruction in Unordered Datasets.
This paper presents a system for fully automatic recognition and reconstruction of 3D objects in image databases. We pose the object recognition problem as one of finding consistent matches between all images, subject to the constraint that the images were taken from a perspective camera. We assume that the objects or scenes are rigid. For each image we associate a camera matrix, which is parameterised by rotation, translation and focal length. We use invariant local features to find matches between all images, and the RANSAC algorithm to find those that are consistent with the fundamental matrix. Objects are recognised as subsets of matching images. We then solve for the structure and motion of each object, using a sparse bundle adjustment algorithm. Our results demonstrate that it is possible to recognise and reconstruct 3D objects from an unordered image database with no user input at all.
Accuracy Verification and Enhancement in 3D Modeling: Application to Donatello's Maddalena.
Optimal Postures and Positioning for Human Body Scanning.
Advancements in technology for digitizing the surface of the human body are providing new opportunities for research in engineering anthropometry, the study of human body measurement for design and evaluation purposes. The availability of the technology is just the first step in applying surface scanning to engineering anthropometry; several issues remain to be resolved to make these tools useful for engineering applications. One important issue is the standardization of positioning and the posture of the subject for scanning. In engineering it is not enough to be able to measure one individual one time in one posture, but it is also necessary to measure the individual in different postures and compare the individual with many other people who have been comparably measured. Not surprisingly, people can be more difficult to measure precisely than fixed stationary objects. In the process of developing standardized procedures for surveying the civilian populations of North America and Europe, an experiment was conducted to determine optimal scanning positions. While this experiment used just one type of scanning technology, many of the methods are transferable to other methods as well. This paper discusses the results from that investigation.
3D Digitization of a Large Model of Imperial Rome.
This paper describes 3D acquisition and modeling of the "Plastico di Roma antica", a large plaster-of-Paris model of imperial Rome (16x17 meters) created in the last century. Its overall size demands an acquisition approach typical of large structures, but it is also characterized by extremely tiny details, typical of small objects: houses are a few centimeters high; their doors, windows, etc. are smaller than 1 cm. The approach followed to resolve this "contradiction" is described. The result is a huge but precise 3D model created by using a special metrology Laser Radar. We give an account of the procedures of reorienting the large point clouds obtained after each acquisition step (50-60 million points) into a single reference system by means of measuring fixed redundant reference points. Finally we show how the data set can be properly divided into 2x2 meters sub-areas for allowing data merging and mesh editing..
3D Registration by Textured Spin-Images.
This work is motivated by the desire of exploiting for 3D registration purposes the photometric information current range cameras typically associate to range data. Automatic pairwise 3D registration procedures are two steps procedures with the first step performing an automatic crude estimate of the rigid motion parameters and the second step refining them by the ICP algorithm or some of its variations. Methods for efficiently implementing the first crude automatic estimate are still an open research area. Spin-images are a 3D matching tecnique very effective in this task. Since spin-images solely exploit geometry information it appears natural to extend their original definition to include texture information. Such an operation can clearly be made in many ways. This work introduces one particular extension of spin-images, called textured spin-images, and demostrates its performance for 3D registration. It will be seen that textured spin-images enjoy remarkable properties since they can give rigid motion estimates more robust, more precise, more resilient to noise than standard spin-images at a lower computational cost.
Calibration of a Zooming Camera using the Normalized Image of the Absolute Conic.
3-D Landmark Detection and Identification in the CAESAR Project.
Euclidean Reconstruction from Translational Motion Using Multiple Cameras.
We investigate the possibility of Euclidean reconstruction from translational motion, using multiple uncalibrated cameras. We show that in the case of multiple cameras viewing a translating scene, no additional constraints are given by the translational motion compared to the more general case with one camera viewing a scene undergoing a general motion. However, the knowledge of translational motion allows an intermediate affine reconstruction from each camera, and aids in the reconstruction process by simplifying several steps, resulting in a more reliable algorithm for 3D reconstruction. We also identify the critical directions of translation, for which no affine reconstruction is possible. Experiments on real and simulated data are performed to illustrate that the method works in practice.
Automated Pavement Distress Collection and Analysis: A 3-D Approach.
Compact and Portable 3D Camera for Space Applications.
Comparison of HK and SC Curvature Description Methods.
Generalized Cylinders Extraction in a Range Image.
We deal with 3D object modeling using generalized cylinders from a single range image. We focus on right generalized cylinders, a class of generalized cylinders for which the cross-section is at right angle with the axis. No homogeneity or straightness constraints are imposed. The crucial part of this work is the extraction of axis points and the representation of the axis curve in 3D space. Interesting results are obtained with a broad variety of range images.
Realistic Human Head Modeling with Multi-View Hairstyle Reconstruction.
We present a method for constructing photorealistic 3D head models from color images and a geometric head model of a specific person. With a simple experimental setup, we employ a user-assisted technique to register the uncalibrated images with the geometric model. A weighted averaging method is then used to extract a panoramic texture map from the input images. To recover the hairstyle of the specified person, a virtual photo plane is defined, according to a corresponding true photo plane, on which one input image is recorded. It provides hints to compute the 3D positions of the visual contour points of the hair from the images taken at different viewpoints. Finally, more hairs are grown to cover the whole region of the scalp using an interpolation method on the 3D scalp mesh surfaces.
Large Data Sets and Confusing Scenes in 3-D Surface Matching and Recognition.
How Much 3D-Information Can We Acquire? Optical Range Sensors at the Physical Limit, and Where to Apply Them.
Automatic Body Measurement for Mass Customization of Garments.
Automatic Registration of Range Images Based on Correspondence of Complete Plane Patches.
One of the difficulties in registering two range images scanned by 3D laser scanners is how to get a correct correspondence over the two images automatically. In this paper, we propose an automatic registration method based on matching of extracted planes. First, we introduce a new class of features: complete plane patches (CPP) on the basis of analysis of properties of real scenes. Then we generate a compact interpretation tree for these features. Finally, the image registration is accomplished automatically by searching the interpretation tree.
Semi-Automatic Range to Range Registration: A Feature-Based Method.
A Self-Referenced Hand-Held Range Sensor.
Segmentation of Range Images into Planar Regions.
This paper presents a hybrid approach to the segmentation of range images into planar regions. The term hybrid refers to a combination of edge- and region-based considerations. A reliable computational procedure which takes the range image discontinuities into account is presented for computing the pixel's normal. The segmentation algorithm consists of two parts. In the first one, the pixels are aggregated according to local properties derived from the input data and are represented by a region adjacency graph (RAG). At this stage, the image is still over-segmented. In the second part, the segmentation is refined thanks to the construction of an irregular pyramid. The base of the pyramid is the RAG previously extracted. The over-segmented regions are merged using a surface-based description. This algorithm has been evaluated on 80 real images acquired by two different range sensors using the methodology proposed in (Hoover et al., 1996). Experimental results are presented and compared to others obtained by four research groups.
Locking onto 3D-Structure by a Combined Vergence and Fusion System.
A 3D Scanning System Based on Low-Occlusion Approach.
Multi-Resolution Geometric Fusion.
Geometric fusion of multiple sets of overlapping surface measurements is an important problem for complete 3D object or environment modelling. Fusion based on a discrete implicit surface representation enables fast reconstruction for complex object modelling. However, surfaces are represented at a single resolution resulting in impractical storage costs for accurate reconstruction of large objects. This paper addresses accurate reconstruction of surface models independent of object size. An incremental algorithm is presented for implicit surface representation of an arbitrary triangulated mesh in a volumetric envelope around the surface. A hierarchical volumetric structure is introduced for efficient representation by local approximation of the surface within a fixed error bound using the maximum voxel size. Multi-resolution geometric fusion is achieved by incrementally constructing a hierarchical surface representation with bounded error. Results are presented for validation of the multi-resolution representation accuracy and reconstruction of real objects. Multi-resolution geometric fusion achieves a significant reduction in representation cost for the same level of geometric accuracy.
A Light Modulation/Demodulation Method for Real-Time 3D Imaging.
This paper describes a novel method for digitizing the 3D shape of an object in real-time, which can be used for capturing live sequence of the 3D shape of moving or deformable objects such as faces. Two DMD (= Digital Micro Mirror) devices are used as high speed switches for modulating and demodulating light rays. One DMD is used to generate rays of light pulses, which are projected onto the object to be measured. Another DMD is used to demodulate the light reflected from the object illuminated by the light pulses into intensity image that describes the disparity. A prototype range finder implementing the proposed method has been built. The experimental results showed that the proposed method works and video sequences of disparity images can be captured in real time.
Toward Optimal Structured Light Patterns.
A methodology for the optimal design of projection patterns for stereometric structured light systems is presented. The similarity as well as the difference between the design of projection patterns and the design of optimal signals for digital communication are discussed. The design of K projection patterns for a structured light system with L distinct planes of light is shown to be equivalent to the placement of L points in a K dimensional space subject to certain constraints. optimal design in the MSE sense is defined, but shown to lead to an intractable multi-parameter global optimization problem. Intuitively appealing suboptimal solutions derived from the family of K dimensional space-filling Hilbert curves are obtained. Preliminary experimental results are presented.
Edge-Based Approach to Mesh Simplification.
A Complete U-V-Disparity Study for Stereovision Based 3D Driving Environment Analysis.
Reliable understanding of the 3D driving environment is vital for obstacle detection and Adaptive Cruise Control (ACC) applications. Laser or millimeter wave radars have shown good performance in measuring relative speed and distance in a highway driving environment. However the accuracy of these systems decreases in an urban traffic environment as more confusion occurs due to factors such as parked vehicles, guardrails, poles and motorcycles. A stereovision based sensing system provides an effective supplement to radar-based road scene analysis with its much wider field of view and more accurate lateral information. This paper presents an efficient solution using a stereovision based road scene analysis algorithm which employs the "U-V-disparity" concept. This concept is used to classify a 3D road scene into relative surface planes and characterize the features of road pavement surfaces, roadside structures and obstacles. Real-time implementation of the disparity map calculation and the "U-V-disparity" classification is also presented.
Calibration-Free Approach to 3D Reconstruction Using Light Stripe Projections on a Cube Frame.
Moving Objects Detection from Time-Varied Background: An Application of Camera 3D Motion Analysis.
PALM: Portable Sensor-Augmented Vision System for Large-Scene Modeling.
Automatic Burr Detection on Surfaces of Revolution Based on Adaptive 3D Scanning.
This paper describes how to automatically extract the presence and location of geometrical irregularities on a surface of revolution. To this end a partial 3D scan of the workpiece under consideration is acquired by structured light ranging. The application we focus on is the detection and removal of burrs on industrial workpieces. Cylindrical metallic objects will cause a strong specular reflection in every direction. These highlights are compensated for in the projected patterns, hence ýadaptive 3D scanningý. The triangular mesh produced is then used to identify the axis and generatrix of the corresponding surface of revolution. The search space for finding this axis is four dimensional: a valid choice of parameters is two orientation angles (as in spherical coordinates) and the 2D intersection point with the plane spanned by two out of three axis of the local coordinate system. For finding the axis we test the circularity of the planar intersections of the mesh in different directions, using statistical estimation methods to deal with noise. Finally the ýidealý generatrix derived from the scan data is compared to the real surface topology. The difference will identify the burr. The algorithm is demonstrated on a metal wheel that has burrs on both sides. Visual servoing of a robotic arm based on this detection is work in progress.
Automatic 3D Modeling Using Range Images Obtained from Unknown Viewpoints.
Partial Surface Integration Based on Variational Implicit Functions and Surfaces for 3D Model Building.
Most three-dimensional acquisition systems generate several partial reconstructions that have to be registered and integrated for building a complete 3D model. In this paper, we propose a volumetric shape integration method, consisting of weighted signed distance functions represented as variational implicit functions (VIF) or surfaces (VIS). Texture integration is solved similarly by using three weighted color functions also based on VIFs. Using these continuous (not grid-based) representations solves current limitations of volumetric methods: no memory inefficient and resolution limiting grid representation is required. The built-in smoothing properties of the VIS representations also improve the robustness of the final integration against noise in the input data. Experimental results are performed on real-live, noiseless and noisy synthetic data of human faces in order to show the robustness and accuracy of the integration algorithm.
Geometric Matching of 3-D Objects: Assessing the Range of Successful Initial Configurations.
Robust and Accurate Partial Surface Registration Based on Variational Implicit Surfaces for Automatic 3D Model Building.
Three-dimensional models are often assembled from several partial reconstructions from unknown viewpoints. In order to provide a fully automatic, robust and accurate method for aligning and integrating partial reconstructions without any prior knowledge of the relative viewpoints of the sensor or the geometry of the imaging process, we propose a 4-step registration and integration algorithm based on a common Variational Implicit Surface (VIS) representation of the partial surface reconstructions. First, a global crude registration without a priori knowledge is performed followed by a pose refinement of partial reconstruction pairs. Pair-wise registrations are converted into a multi-view registration, before a final integration of the reconstructions into one entity or model occurs. Furthermore, making use of the smoothing properties of the VIS representations, the algorithm proves to be robust against noise in the reconstruction data. Experimental results on real-live, as well as noiseless and noisy simulated data are presented to show the feasibility, the accuracy and robustness of our registration scheme.
Stereo by Multiperspective Imaging under 6 DOF Camera Motion.
Multiperspective imaging has been used to recover the structure of a scene. Although several algorithms for structure recovery have been developed as typified by stereo panoramas, there exists no common framework which subsumes various camera motions to capture stereo images. This paper presents a framework for stereo by multiperspective imaging, which is general in that it can handle 6 degree-of-freedom (DOF) camera motion. We derive geometric constraints, equation for structure recovery and that for an epipolar curve by modeling the acquisition of stereo images using push-broom cameras (line sensors). We consider a class of camera motion called a vertical view plane class and demonstrate that several previous results are really special cases of our results. Numerical examples are given to show the correctness of the derived equations.
Planar Patch Extraction with Noisy Depth Data.
A Hierarchical Method for Aligning Warped Meshes.
Joined Segmentation of Cortical Surface and Brain Volume in MRI Using a Homotopic Deformable Cellular Model.
Modeling from Reality.
Spatio-Temporal Fusion of Multiple View Video Rate 3D Surfaces.
We consider the problem of geometric integration and representation of multiple views of non-rigidly deforming 3D surface geometry captured at video rate. Instead of treating each frame as a separate mesh we present a representation which takes into consideration temporal and spatial coherence in the data where possible. We first segment gross base transformations using correspondence based on a closest point metric and represent these motions as piecewise rigid transformations. The remaining residual is encoded as displacement maps at each frame giving a displacement video. At both these stages occlusions and missing data are interpolated to give a representation which is continuous in space and time. We demonstrate the integration of multiple views for four different non-rigidly deforming scenes: hand, face, cloth and a composite scene. The approach achieves the integration of multiple-view data at different times into one representation which can processed and edited.
Fast Alignment of 3D Geometrical Models and 2D Color Images Using 2D Distance Maps.
This paper presents a fast pose estimation algorithm of a 3D free form object in 2D images using 2D distance maps. One of the popular techniques of the pose estimation of 3D object in 2D image is the point-based method such as the ICP algorithm. However, the calculation cost for determining point correspondences is expensive. To overcome this problem, the proposed method utilizes a distance map on the 2D image plane, which is constructed quite rapidly by the Fast Marching Method. For pose estimation of the object, contour lines of the 2D image and the projection of the 3D object are aligned using the distance map iteratively by the robust M-estimator. Some experimental results with simulated models and actual images of the endoscopic operation are successfully carried out.
Acquisition of View-Based 3-D Object Models Using Supervised, Unstructured Data.
Existing techniques for view-based 3-D object recognition using computer vision rely on training the system on a particular object before it is introduced into an environment. This training often consists of taking over 100 images at predetermined points around the viewing sphere in an attempt to account for most angles for viewing the object. However, in many circumstances, the environment is well known and we only expect to see a small subset of all possible appearances. In this paper, we will test the idea that under these conditions, it is possible to train an object recognition system on-the-fly using images of an object as it appears in its environment, with supervision from the user. Furthermore, because some views of an object are much more likely than others, the number of training images required can be greatly reduced.
Fusing and Guiding Range Measurements with Colour Video Images.
Combining data from different sensors provides richer data for visualisation and helps in automatic detection and recognition of objects. This paper presents two methods relating two dimensional images from colour CCD cameras with three dimensional data from range scanners. The first method is applicable in a static case and provides the geometrically correct solution. The second one offers an approximate solution but can be used in a dynamic environment for guiding selective range measurements and simplifies the calibration process. The error introduced by this approximation has been derived as a function of the distance to the object and geometry of the set-up. Results of combining range information from a laser camera and colour camera are presented.
Using PCA to Model Shape for Process Control.
Before surface mount components can be placed on a circuit board, it is necessary to print solder paste onto pads. The paste is then melted to make an electrical connection (reflow). A screen printing process is used to print the solder paste onto the board. This is a complicated process with a large number of input parameters. Some of these parameters can be controlled and it is the purpose of this work to investigate control of the process based on measurement of the output shape of the printed paste. The shape is measured using a laser range scanner Principal Component Analysis (PCA) is proposed as a tool for describing solder paste shape with a small number of parameters. This paper discusses the use of PCA for shape analysis in range images as well as explaining how such a description can be incorporated into a process control loop.
Fitting of 3D Circles and Ellipses Using a Parameter Decomposition Approach.
Many optimization processes encounter a problem inefficiently reaching a global minimum or a near global minimum. Traditional methods such as Levenberg-Marquardt algorithm and trust-region method face the problems of dropping into local minima as well. On the other hand, some algorithms such as simulated annealing and genetic algorithm try to find a global minimum but they are mostly time-consuming. Without a good initialization, many optimization methods are unable to guarantee a global minimum result. We address a novel method in 3D circle and ellipse fitting, which alleviates the optimization problem. It can not only increase the probability of getting in global minima but also reduce the computation time. Based on our previous work, we decompose the parameters into two parts: one part of parameters can be solved by an analytic or a direct method and another part has to be solved by an iterative procedure. Via this scheme, the topography of optimization space is simplified and therefore we reduce the number of local minima and the computation time. We experimentally compare our method with the traditional ones and show superior performance.
Reducing Movement Artifacts in Whole Body Scanning.
Movement artifacts during whole body scanning are a major concern when reproducible results are required. To determine the magnitude of the artifacts, 11 subjects were scanned with and without a positioning device on the head. The sway of the body was determined by a force plate. It was shown that the pointer on the head reduced the magnitude of the forward/backward sway by over 50%. The resulting standard deviation is less than the resolution of the scanner. Movement artifacts within the body, like head rotation, are hard to control. Again, the pointer on the head assists in reducing the artifacts. The ventilation depth during the scan determines the shape of the chest and should be standardized to get reproducible results.
Extraction and Tracking of Surfaces in Range Image Sequences.
Range Image Registration: A Software Platform and Empirical Evaluation.
Surface Registration by Matching Oriented Points.
For registration of 3-D free-form surfaces we have developed a representation which requires no knowledge of the transformation between views. The representation comprises descriptive images associated with oriented points on the surface of an object. Constructed using single point bases, these images are data level shape descriptions that are used for efficient matching of oriented points. Correlation of images is used to establish point correspondences between two views; from these correspondences a rigid transformation that aligns the views is calculated. The transformation is then refined and verified using a modified iterative closest point algorithm. To demonstrate the generality of our approach, we present results from multiple sensing domains.
A Laser Range Scanner Designed for Minimum Calibration Complexity.
A System for Semi-Automatic Modeling of Complex Environments.
We present a perception system, called Artisan, that semi-automatically builds 3-D models of a robot's workspace. Range images are acquired with a scanning laser rangefinder and then processed, based an a systematic sensor characterization, to remove noise and artifacts. Complex 3-D objects represented as surface meshes are subsequently recognized in the range images and inserted into a virtual workspace. This graphical virtual workspace is then used to by human operators to plan and execute remote robotic operations.
Image-Based Techniques for Digitizing Environments and Artifacts.
Registration and Integration of Textured 3-D Data.
In general, multiple views are required to create a complete 3-D model of an object or of a multi-roomed indoor scene. In this work, we address the problem of merging multiple textured 3-D data sets, each of which corresponds to a different view of a scene or object. There are two steps to the merging process: registration and integration. To register, or align, data sets we use a modified version of the Iterative Closest Point algorithm; our version, which we call color ICP, considers not only 3-D information, but color as well. We show that the use of color decreases registration error by an order of magnitude. Once the 3-D data sets have been registered we integrate them to produce a seamless, composite 3-D textured model. Our approach to integration uses a 3-D occupancy grid to represent likelihood of spatial occupancy through voting. In addition to occupancy information, we store surface normal in each voxel of the occupancy grid. Surface normal is used to robustly extract a surface from the occupancy grid; on that surface we blend textures from multiple views.
Building Symbolic Information for 3D Human Body Modeling from Range Data.
Automatic 3 -- D Digitization using a Laser Rangefinder with a Small Field of View.
We address the problem of the automation of surface digitization using a precision 3-D laser rangefinder. Because of their small field of view, such sensors navigate closely to the digitized object and are subject to collisions. Unlike previous techniques that addressed only the exhaustiveness of digitization, this work focuses on collision avoidance. To safely identify empty space, shadow and occlusion phenomena are carefully analyzed, and so is the effect of sampling. Then, the planning problem is solved using a hierarchical approach. At low level, we digitize a single view and address collision avoidance using path planning techniques. At high level, the problem becomes the choice of the next best view. Some implementation details and experimental results are presented.
Three-Dimensional Modelling and Rendering of the Human Skeletal Trunk from 2D Radiographic Images.
Compact 3D Profilometer with Grazing Incidence Diffraction Optics.
Fast Multiple-Baseline Stereo with Occlusion.
This paper presents a new and fast algorithm for multi-baseline stereo designed to handle the occlusion problem. The algorithm is a hybrid between fast heuristic occlusion overcoming algorithms that precompute an approximate visibility and slower methods that use correct visibility handling. Our approach is based on iterative dynamic programming and computes simultaneously disparity and camera visibility. Interestingly, dynamic programming makes it possible to compute exactly part of the visibility information. The remainder is obtained through heuristics. The validity of our scheme is established using real imagery with ground truth and compares favorably with otherstate-of-the-art multi-baseline stereo algorithms.
Building 3-D City Models from Multiple Unregistered Profile Maps.
The paper presents an approach for building 3-D city models for virtual environments from multiple 3-D data sets acquired from different viewpoints by light striping. The raw data sets are represented as single valued parametric surfaces called the 3-D profile maps. The profile maps are registered to the same coordinate system by an iterative surface matching algorithm developed previously. The registration proceeds hierarchically from low to high resolution and all the data sets are matched simultaneously but an initial registration is assumed to be known. After having segmented each map by a region growing algorithm, the maps are integrated into a piecewise planar surface model by merging compatible segments in the overlapping areas. The borders of the segments are also traced an the parametric domains of the maps as a step for building a wireframe model. Test results are shown in the case of a scale model of an urban area digitized in laboratory conditions.
An Improved Calibration Technique for Coupled Single-Row Telemeter and CCD Camera.
Toward a successful 3D and textural reconstruction of urban scenes, the use of both single-row based telemetric and photographic data in a same framework has proved to be a powerful technique. A necessary condition to obtain good results is to accurately calibrate the telemetric and photographic sensors together. We present a study of this calibration process and propose an improved extrinsic calibration technique. It is based on an existing technique which consists in scanning a planar pattern in several poses, giving a set of relative position and orientation constraints. The innovation is the use of a more appropriate laser beam distance between telemetric points and the planar target. Moreover, we use robust methods to manage outliers at several steps of the algorithm. Improved results on both theoretical and experimental data are given.
Self-Calibration of a Light Striping System by Matching Multiple 3-D Profile Maps.
Transform-Based Methods for Indexing and Retrieval of 3D Objects.
We compare two transform-based indexing methods for retrieval of 3D objects. We apply 3D Discrete Fourier Transform (DFT) and 3D Radial Cosine Transform (RCT) to the voxelized data of 3D objects. Rotation invariant features are derived from the coefficients of these transforms. Furthermore we compare two different voxel representations, namely, binary denoting object and background space, and continuous after distance transformation. In the binary voxel representation the voxel values are simply set to 1 on the surface of the object and 0 elsewhere. In the continuous-valued representation the space is filled with a function of distance transform. The rotation invariance properties of the DFT and RCT schemes are analyzed. We have conducted retrieval experiments on the Princeton Shape Benchmark and investigated the retrieval performance of the methods using several quality measures.
A Multi-Resolution ICP with Heuristic Closest Point Search for Fast and Robust 3D Registration of Range Images.
Effective 3D Modeling Of Heritage Sites.
Identifying the Interface between Two Sand Materials.
To study the behavior of water flow at interfaces between different soil materials we made computed tomography scans of sand samples using synchrotron light. The samples were prepared with an interface between two sand materials. The contact points between grains at the interface between the sands were identified using a combination of watershed segmentation and a classifier that used the grain-size and -location. The process from a bilevel image to a classified image is described. In the classified image five classes are represented; two for the grains and three for the contact points to represent intra- and inter-class contact points.
Silhouette and Stereo Fusion for 3D Object Modeling.
In this paper, we present a new approach to high quality 3D object reconstruction. Starting from a calibrated sequence of color images, the algorithm is able to reconstruct both the 3D geometry and the texture. The core of the method is based on a deformable model, which defines the framework where texture and silhouette information can be fused. This is achieved by defining two external forces based on the images: a texture driven force and a silhouette driven force. The texture force is computed in two steps: a multi-stereo correlation voting approach and a gradient vector flow diffusion. Due to the high resolution of the voting approach, a multi-grid version of the gradient vector flow has been developed. Concerning the silhouette force, a new formulation of the silhouette constraint is derived. It provides a robust way to integrate the silhouettes in the evolution algorithm. As a consequence, we are able to recover the contour generators of the model at the end of the iteration process. Finally, a texture map is computed from the original images for the reconstructed 3D model.
Extracting Surface Patches from Complete Range Descriptions.
Constructing a full CAD model of a part requires feature descriptions from all sides; in this case we consider surface patches as the geometric primitives. Most previous research in surface patch extraction has concentrated on extracting patches from a single view. This leads to several problems with aligning and combining partial patch fragments in order to produce complete part models. We have avoided these problems by adapting our single view, range data segmentation program to extract patches, and thus models, directly from fully merged range datasets.
Foreword.
Scanning and Processing 3D Objects for Web Display.
Coordination of Appearance and Motion Data for Virtual View Generation of Traditional Dances.
A novel method is proposed for virtual view generation of traditional dances. In the proposed framework, a traditional dance is captured separately for appearance registration and motion registration. By coordinating the appearance and motion data, we can easily control virtual camera motion within a dancer-centered coordinate system. For this purpose, a coordination problem should be solved between the appearance and motion data, since they are captured separately and the dancer moves freely in the room. The present paper shows a practical algorithm to solve it. A set of algorithms are also provided for appearance and motion registration, and virtual view generation from archived data. In the appearance registration, a 3D human shape is recovered in each time from a set of input images after suppressing their backgrounds. By combining the recovered 3D shape and a set of images for each time, we can compose archived dance data. In the motion registration, stereoscopic tracking is accomplished for color markers placed on the dancer. A virtual view generation is formalized as a color blending among multiple views, and a novel and efficient algorithm is proposed for the composition of a natural virtual view from a set of images. In the proposed method, weightings of the linear combination are calculated from both an assumed viewpoint and a surface normal.
Exploiting Mirrors for Laser Stripe 3D Scanning.
Virtual Reconstruction of broken and unbroken Pottery.
Multisensor Fusion for Volumetric Reconstruction of Large Outdoor Areas.
This paper presents techniques for the merging of 3D Data coming from different sensors, such as ground and aerial laser range scans. The 3D Models created are reconstructed to give a photo-realistic scene enabling interactive virtual walkthroughs, measurements and scene change analysis. The reconstructed model is based on a weighted integration of all available data based on sensor-specific parameters such as noise level, accuracy, inclination and reflectivity of the target, spatial distribution of points. The geometry is robustly reconstructed with a volumetric approach. Once registered and weighed, all data is re-sampled in a multi-resolution distance field usingout-of-core techniques. The final mesh is extracted by contouring the iso-surface with a feature preserving dual contouring algorithm. The paper shows results of the above technique applied to Verona (Italy) city centre.
Further Improving Geometric Fitting.
We give a formal definition of geometric fitting in a way that suits computer vision applications. We point out that the performance of geometric fitting should be evaluated in the limit of small noise rather than in the limit of a large number of data as recommended in the statistical literature. Taking the KCR lower bound as an optimality requirement and focusing on the linearized constraint case, we compare the accuracy of Kanataniýs renormalization with maximum likelihood (ML) approaches including the FNS of Chojnacki et al. and the HEIV of Leedan and Meer. Our analysis reveals the existence of a method superior to all these.
Solving architectural modelling problems using knowledge.
Processing Range Data for Reverse Engineering and Virtual Reality.
A Low-Cost Range Finder Using a Visually Located, Structured Light Source.
Refining Triangle Meshes by Non-linear Subdivision.
Real-time Range Scanning of Deformable Surfaces by Adaptively Coded Structured Light.
Deformable Model with Adaptive Mesh and Automated Topology Changes.
Scale Selection for Classification of Point-Sampled 3-D Surfaces.
Three-dimensional ladar data are commonly used to perform scene understanding for outdoor mobile robots, specifically in natural terrain. One effective method is to classify points using features based on local point cloud distribution into surfaces, linear structures or clutter volumes. But the local features are computed using 3-D points within a support-volume. Local and global point density variations and the presence of multiple manifolds make the problem of selecting the size of this support volume, or scale, challenging. In this paper we adopt an approach inspired by recent developments in computational geometry [5] and investigate the problem of automatic data-driven scale selection to improve point cloud classification. The approach is validated with results using data from different sensors in various environments classified into different terrain types (vegetation, solid surface and linear structure)¹.
An Automation System for Industrial 3-D Laser Digitizing.
Estimation of Elastic Constants from 3D Range-Flow.
Bayesian Estimation of Distance and Surface Normal with a Time-of-Flight Laser Rangefinder.
Advances in the Cooperation of Shape from Shading and Stereo Vision.
The Parallel Iterative Closest Point Algorithm.
Building 3D Facial Models and Detecting Face Pose in 3D Space.
The Mapping of Texture on VR Polygonal Models.
Road Surface Inspection using Laser Scanners Adapted for the High Precision Measurements of Large Flat Surfaces.
Toward a Near Optimal Quad/Triangle Subdivision Surface Fitting.
In this paper we present a new framework for subdivision surface fitting of arbitrary surfaces (not closed objects) represented by polygonal meshes. Our approach is particularly suited for output surfaces from a mechanical or CAD object segmentation for a piecewise subdivision surface approximation. Our algorithm produces a mixed quadrangle-triangle control mesh, near optimal in terms of face and vertex numbers while remaining independent of the connectivity of the input mesh. The first step approximates the boundaries with subdivision curves and creates an initial subdivision surface by optimally linking the boundary control points with respect to the lines of curvature of the target surface. Then, a second step optimizes the initial control polyhedron by iteratively moving control points and enriching regions according to the error distribution. Experiments conducted on several surfaces and on a whole segmented mechanical object, have proven the coherency and theefficiency of our algorithm, compared with existing methods.
An Efficient Scattered Data Approximation Using Multilevel B-Splines Based on Quasi-Interpolants.
In this paper, we propose an efficient approximation algorithm using multilevel B-splines based on quasi-interpolants. Multilevel technique uses a coarse to fine hierarchy to generate a sequence of bicubic B-spline functions whose sum approaches the desired interpolation function. To compute a set of control points, quasi-interpolants gives a procedure for deriving local spline approximation methods where a B-spline coefficient only depends on data points taken from the neighborhood of the support corresponding the B-spline. Experimental results show that the smooth surface reconstruction with high accuracy can be obtained from a selected set of scattered or dense irregular samples.
The Digital Michelangelo Project.
Evaluating Collinearity Constraint for Automatic Range Image Registration.
While most of the existing range image registration algorithms either have to extract and match structural (geometric or optical) features or have to estimate the motion parameters of interest from outliers corrupted point correspondence data for the elimination of false matches in the process of image registration, the registration error and the collinearity error derived directly from the traditional closest point criterion are also capable of doing the same job. However, the latter has an advantage of easy implementation. The purpose of this paper is to investigate which definition of collinearity is more accurate and stable in eliminating false matches inevitably introduced by the closest point criterion. The experiments based on real images show the advantages and disadvantages of different definitions of collinearity.
Multi-Resolution Modeling and Locally Refined Collision Detection for Haptic Interaction.
The computational cost of a collision detection (CD) algorithm on polygonal surfaces depends highly on the complexity of the models. A novel "locally refined" approach is introduced in this paper for fast CD in haptic rendering applications, e.g. haptic surgery and haptic sculpture simulations. Exact interference detections are performed on proposed locally refined meshes, which are in multi-resolution representation. The meshes are generated using mesh simplification and space partition. A new BVH algorithm called "Active Bounding Tree", or AB-Tree, handling collision queries is introduced. At runtime the meshes are dynamically refined to higher resolution in areas that are most likely to collide with other objects. The algorithms are successfully demonstrated in an interactive haptic environment. Compared to existing CD algorithms on single resolution models, noticeable performance improvement has been observed in terms of the precision of collision queries, frame rate, and memory usage.
Evaluating Structural Constraints for Accurate Range Image Registration.
An Adaptive Dandelion Model for Reconstructing Spherical Terrain-Like Visual Hull Surfaces.
In this paper we present an adaptive Dandelion Model for reconstructing spherical terrain-like Visual Hull (VH) surfaces. The Dandelion Model represents a solid by a pencil of organized line segments emitted from a common point. The directions and the topology of the line segments are derived from the triangle facets of a geodesic sphere, which are recursively subdivided until the desired precision is achieved. The initial lines are cut by silhouettes in 2D and then lifted back to 3D to determine the ending points of the line segments defining sampling points on the spherical terrain-like VH surface. A mesh model can be easily constructed from the Dandelion Model. Our algorithm has the advantages of controllable precision, adaptive resolution, simplicity and speediness. We validate our algorithm by theories and experiments.
A Fast and Accurate 3-D Rangefinder using the Biris Technology: The TRID Sensor.
In many industrial applications such as autonomous vehicle guidance and robotic manipulations, reliable 3-D informations must be extracted from the scene in order to provide a robust description of the environment in which the system evolves. This paper presents the TRID sensor, a simple, fast, robust and accurate 3-D sensing device based on the Biris technology. TRID is limited to a lateral resolution of one point. Experimental results show that an accuracy better than 0.15% can be obtained for the depth component (Z) of 3-D points at a distance of 1.3 meter from a wooden black painted beam surface with an acquisition rate of 8.5 points/s.
3D Statistical Shape Models for Medical Image Segmentation.
Reliable and Rapidly-Converging ICP Algorithm Using Multiresolution Smoothing.
Frequency Domain Estimation of 3-D Rigid Motion Based on Range and Intensity Data.
Video-rate registered range and intensity data are at reach of current sensor technology. This wealth of data can be profitably exploited in order to estimate rigid motion parameters as the approaches to 3-D motion estimation, based on the optical flow of both types of data, indicate. This work introduces an alternative for 3-D motion estimation based on the Fourier transform of the 3-D intensity function implicitly described by the registered time-sequences of range and intensity data. The proposed procedure can lead to an unsupervised method for 3-D rigid motion estimation. This method has several advantages related to the fact that it uses the total available information and not sets of features. With respect to memory occupancy the use of a time-sequence of a 3-D intensity function represents a considerable data reduction with respect to a pair of time-sequences of 2-D functions. The proposed technique, which extends to the 3-D case previous frequency domain estimation algorithms developed for the planar case, retain their robustness.
Automatic Modeling of Animatable Virtual Humans - A Survey.
Rangefinder using Time Correlated Single Photon Counting.
This paper describes a method for acquiring range data based on time-correlated single photon counting (TCSPC) and details the data analysis techniques used to compute the range measurements. The prototype sensor being built in our laboratory is capable of measuring up to 100 points per second with an accuracy of about 15 /spl mu/m.
Generation of Geometric Model by Registration and Integration of Multiple Range Images.
Surface Curvature Estimation from the Signed Distance Field.
Simultaneous Determination of Registration and Deformation Parameters among 3D Range Images.
Conventional registration algorithms are mostly concerned with rigid-body transformation parameters between a pair of 3D range images. Our proposed framework aims to determine, in a unified manner, not only such rigid transformation parameters but also various deformation parameters, assuming that the deformation we handle here is strictly defined by some parameterized formulation derived from the deformation mechanism. While conventional registration algorithms usually calcurate six parameters (three translation and three rotation parameters), our proposed algorithm estimates deformation parameters as well. In this paper, we describe how we formulated such an algorithm, implemented it, and evaluated its performance.
A Method of Style Discrimination of Oil Painting Based on 3D Range Data.
A Portable Three-Dimensional Digitizer.
A portable three-dimensional digitizer using a monocular camera is presented in this paper. The digitizer automatically acquires the shape of a target object as well as its texture. The digitizer has the following advantages: 1) compact and inexpensive, 2) skill-free 3D image acquisition, and 3) handles a wide range of objects of various materials. The digitizing algorithm is based on the "Shape-from-Silhouette" framework, where several novel techniques are embedded as follows. In the silhouette extraction, not only pixel-level subtraction between images but also region-level subtraction are embedded so as to achieve precise extraction. The texture acquisition is treated as a labeling problem in an energy minimization framework, which enables us to get realistic textures with a simple operation. Our experiments showed that the digitizing speed of the digitizer was practical.
Calibration of a Laser Stripe Profiler.
3-D Motion and Shape from Multiple Image Sequences.
Streaming Transmission of Point-Sampled Geometry Based on View-Dependent Level-of-Detail.
Active Balloon Model Based On 3D Skeleton Extraction By Competitive Learning.
Computations on a Spherical View Space for Efficient Planning of Viewpoints in 3-D Object Modeling.
Reconstruction of Complex Environments by Robust Pre-aligned ICP.
Stroboscopic Stereo Rangefinder.
Fast and Robust Registration of 3D Surfaces Using Low Curvature Patche.
Automatic Reconstruction of 3D Objects using a Mobile Monoscopic Camera.
A method for the automatic reconstruction of 3D objects from multiple camera views for 3D multimedia applications is presented. Conventional 3D reconstruction techniques use equipment that restrict the flexibility of the user. In order to increase this flexibility, the presented method is characterized by a simple measurement environment, that consists of a new calibration pattern placed below the object allowing object and pattern acquisition simultaneously. This ensures, that each view can be calibrated individually. From these obtained calibrated camera views, a textured 3D wireframe model is estimated using a shape-from-silhouette approach and texture mapping of the original camera views. Experiments with this system have confirmed a significant gain of flexibility for the user and a drastic reduction of costs for technical equipment while ensuring comparable model quality as conventional reconstruction techniques at the same time.
3D Reconstruction from Two Orthogonal Views Using Simulated Annealing Approach.
Automatic Model Refinement for 3D Reconstruction with Mobile Robots.
Locating Landmarks on Human Body Scan Data.
Software for locating anthropometric landmarks from a cloud of more than 100000 three dimensional data points, captured from a human subject, is presented. The software is part of an incremental approach that progressively refines the identification of data points. The first phase of identification is to orient and segment the human body data points. Algorithms for these tasks are presented, with a description of their use. One of the algorithms, a discrete point cusp detector is believed to be unique. The software has been tested on twenty different body scan data sets and shown to be robust.
Fast Simultaneous Alignment of Multiple Range Images Using Index Images.
This paper describes a fast and easy-to-use simultaneous alignment method of multiple range images. The most time consuming part of alignment process is searching corresponding points. Although "Inverse calibration" method quickly searches corresponding points in complexity O(n), where n is the number of vertices, the method requires some look-up tables or precise sensorýs parameters. Then, we propose an easy-to-use method that uses "Index Image": "Index image" can be rapidly created using graphics hardware without precise sensorýs parameters. For fast computation of rigid transformation matrices of a large number of range images, we utilized linearized error function and applied incomplete Cholesky conjugate gradient (ICCG) method for solving linear equations. Some experimental results that aligned a large number of range images measured with laser range sensors show the effectiveness of our method.
Parallel Alignment of a Large Number of Range Images.
Contour Point Tracking by Enforcement of Rigidity Constraints.
The aperture problem is one of the omnipresent issues in computer vision. Its local character constrains point matching to high textured areas, so that points ingradient- oriented regions (such as straight lines) can not be reliably matched. We propose a new method to overcome this problem by devising a global matching strategy under the factorization framework. We solve the n-frame correspondence problem under this context by assuming the rigidity of the scene. To this end, a geometric contraint is used that selects the matching solution resulting in a rank-4 observation matrix. The rank of the observation matrix is a function of the matching solutions associated to each image and as such a simulteaneous solution for all frames has to be found. An optimization procedure is used in this text in order to find the solution.
Dense Disparity Estimation Using Gabor Filters and Image Derivatives.
Incremental Catmull-Clark Subdivision.
In this paper, a new adaptive method for Catmull-Clark subdivision is introduced. Adaptive subdivision refines specific areas of a model according to user or application needs. Naive adaptive subdivision algorithm change the connectivity of the mesh, causing geometrical inconsistencies that alter the limit surface. Our method expands the specified region of the mesh such that when it is adaptively subdivided, it produces a smooth surface whose selected area is identical to when the entire mesh is refined. This technique also produces a surface with an increasing level of detail from coarse to fine areas of the surface. We compare our adaptive subdivision with other schemes and present some example applications.
A Contour-Based Approach to 3D Text Labeling on Triangulated Surfaces.
This paper presents a simple and efficient method of forming a 3D text label on a 3D triangulated surface. The label is formed by projecting the 2D contours that define the text silhouette onto the triangulated surface, forming 3D contour paths. Surface polygons upon which the 3D contour paths lie are retriangulated using a novel approach that forms a polyline defining the region outside the contour. This algorithm produces labeled 3D surfaces that conform to the specifications of the STL format, making them suitable for fabrication by a rapid prototyping machine. We demonstrate the effectiveness of the algorithm in forming flat and extruded labels on non-trivial surfaces.
Nefertiti: A Query by Content Software for Three-Dimensional Models Databases Management.
This paper presents a new approach for the classification and retrieval of three-dimensional images and models from databases. A set of retrieval algorithms is introduced. These algorithms are content-based, meaning that the input is not made out of keywords but of three-dimensional models. Tensor of inertia, distribution of normals, distribution of cords and multiresolution analysis are used to describe each model. The database can be searched by scale, shape or color or any combination of these parameters. A user friendly interface makes the retrieval operation simple and intuitive and allows to edit reference models according to the specifications of the user. Experimental results using a database of more than 400 range images and 1000 VRML models are presented.
Automatic Feature Correspondence for Scene Reconstruction from Multiple Views.
Dual-Beam Structured-Light Scanning for 3-D Object Modeling.
Estimating Pose Through Local Geometry.
Constructing NURBS Surface Model from Scattered and Unorganized Range Data.
A Point-and-Shoot Color 3D Camera.
A Range Image Refinement Technique for Multi-view 3D Model Reconstruction.
Reconstruction of Surfaces behind Occlusions in Range Images.
A Fast Point-to-Tangent Plane Technique for Multi-view Registration.
Bayesian Modelling of Camera Calibration and Reconstruction.
Camera calibration methods, whether implicit or explicit, are a critical part of most 3D vision systems. These methods involve estimation of a model for the camera that produced the visual input, and subsequently to infer the 3D structure that gave rise to the input. However, in these systems the error in calibration is typically unknown, or if known, the effect of calibration error on subsequent processing (e.g. 3d reconstruction) is not accounted for. In this paper, we propose a Bayesian camera calibration method that explicitly computes calibration error, and we show how knowledge of this error can be used to improve the accuracy of subsequent processing. What distinguishes the work is the explicit computation of a posterior distribution on unknown camera parameters, rather than just a best estimate. Marginalizing (averaging) subsequent estimates by this posterior is shown to reduce reconstruction error over calibration approaches that rely on a single best estimate. The method is made practical using sampling techniques, that require only the evaluation of the calibration error function and the specification of priors. Samples with their corresponding probability weights can be used to produce better estimates of the camera parameters. Moreover, these samples can be directly used to improve estimates that rely on calibration information, like 3D reconstruction. We evaluate our method using simulated data for a structure from motion problem, in which the same point matches are used to calibrate the camera, estimate the motion, and reconstruct the 3D geometry. Our results show improved reconstruction over non-linear Camera calibration methods like the Maximum Likelihood estimate. Additionally, this approach scales much better in the face of increasingly noisy point matches.
Affine Transformations of 3D Objects Represented with Neural Networks.
Acquisition of Three-Dimensional Information in a Real Environment by Using the Stereo Omni-directional System (SOS).
A Registration Aid.
Although the iterative closest point algorithm is very effective in registering range data, the quality of registration depends on the geometry of the sampled surfaces. This work presents a registration aid which can greatly reduce the possibility of catastrophic registration failure, increase the quality of registration and register range data which does not overlap. By placing the aid into the scene along with the object(s) of interest, range images taken from any vantage point within the range scanner's workspace can be registered onto a model of the aid and thus into a common reference frame. By considering the causes of registration failure, the surface of the aid is designed so that any range image taken of it from any point in the scanner's workspace will properly register with a model of the aid. The reliability of the aid is demonstrated with a Monte Carlo experiment and its utility is demonstrated with examples from an automated surface acquisition system.
Free-Form Surface Reconstruction from Multiple Images .
Hand-Held Acquisition of 3D Models with a Video Camera.
A MRF Formulation for Coded Structured Light.
Multimedia projectors and cameras make possible the use of structured light to solve problems such as 3D reconstruction, disparity map computation and camera or projector calibration. Each projector displays patterns over a scene viewed by a camera, thereby allowing automatic computation of camera-projector pixel correspondences. This paper introduces a new algorithm to establish this correspondence in difficult cases of image acquisition. A probabilistic model formulated as a Markov Random Field uses the stripe images to find the most likely correspondences in the presence of noise. Our model is specially tailored to handle the unfavorable projector-camera pixel ratios that occur in multiple-projector single-camera setups. For the case where more than one camera is used, we propose a robust approach to establish correspondences between the cameras and compute an accurate disparity map. To conduct experiments, a ground truth was first reconstructed from a high quality acquisition. Various degradations were applied to the pattern images which were then solved using our method. The results were compared to the ground truth for error analysis and showed very good performances, even near depth discontinuities.
The ModelCamera: a Hand-Held Device for Interactive Modeling.
Multi-projectors for arbitrary surfaces without explicit calibration nor reconstructio.
Virtual Environments for Critical Intervention Support: Modeling, Design and Implementation Issues.
Cramer-Rao Bounds for Nonparametric Surface Reconstruction from Range Data.
Tolerance Control with High Resolution 3D Measurement.
Anisotropic diffusion of surface normals for feature preserving surface reconstruction.
CAD-Based Range Sensor Placement for Optimum 3D Data Acquisitio.
A Flexible 3D Modeling System Based on Combining Shape-from-Silhouette with Light-Sectioning Algorithm.
In this paper we present a flexible modeling system for obtaining the texture-mapped 3D geometric model. The modeling system uses an algorithm combining shape-from-silhouette with light-sectioning. In the algorithm, at first, a rough shape model is obtained by shape-from-silhouette method almost automatically. Next, concavities and complex parts on the object surface are obtained by light-sectioning method with manual scanning. For applying light-sectioning method to volume data, we propose volumetric light-sectioning algorithm. Then our modeling system can realize easy and accurate generation of 3D geometric model.
Multiview Registration for Large Data Sets.
Physics-Based Models for Image Analysis/Synthesis and Geometric Design.
This paper reviews recently developed physics-based surface modeling techniques for geometric design, medical image analysis, and human facial modeling. It briefly motivates the problems of interest in each application area, describes the models that the authors have developed to address them, presents sample results, and provides references to technical papers containing the full details.
Robust Meshes from Multiple Range Maps.
This paper presents a method for modeling the surface of an object from a sequence of range maps. Our method is based on a volumetric approach that produces a compact surface without boundary. It provides robustness through the use of interval analysis techniques and computational efficiency through hierarchical processing using octrees.
Determining Characteristic Views of a 3D Object by Visual Hulls and Hausdorff Distance.
Nowadays, with the exponential growing of 3D object representations in private databases or on the web, it is all the more required to match these objects from some views. To improve the results of their matching, we work on the characteristic views of an object. The aim of this study is to find how many characteristic views are required and what relative positions are optimal. This is the reason why the visual hulls are used. From some 2D masks, the nearest possible 3D mesh from the original object is computed. OpenGL views are used to build the visual hulls of 3D models from a given collection and then the distance between the visual hulls and the models are measured thanks to the Hausdorff distance. Then the best view parameters are deduced to reduce the distance. These shots show that three orthogonal views give results very close to the ones given by twelve views on a isocahedron. Some other results on the view resolution and the field of view are discussed.
Projective Surface Matching of Colored 3D Scans.
We present a new method for registering multiple 3D scans of a colored object. Each scan is regarded as a color and range image of the object recorded by a pinhole camera. Consider a pair of cameras that see overlapping parts of the objects. For correct camera poses, the actual image of the overlap area in one camera matches the rendition of the overlap area as seen by the other camera. We define a mismatch score summarizing discrepancies in color, range, and silhouette between pairs of images, and we present an algorithm to efficiently minimize this mismatch score over camera poses.
3D Modeling of Archaeological Vessels Using Shape from Silhouette.
Adaptive Enhancement of 3D Scenes using Hierarchical Registration of Texture-Mapped 3D models.
Relighting Acquired Models of Outdoor Scenes.
In this paper we introduce a relighting algorithm for diffuse outdoor scenes that enables us to create geometrically correct and illumination consistent models from a series of range scans and a set of overlapping photographs that have been taken under different illumination conditions. To perform the relighting we compute a set of mappings from the overlap region of two images. We call these mappings Irradiance Ratio Maps (IRMs). Our algorithm handles cast shadows, being able to relight shadowed regions into non-shadowed regions and vice-versa. We solve these cases by computing four different IRMs, to handle all four combinations of shadowed vs. non-shadowed surfaces. To relight the non-overlapping region of an image, we look into the appropriate IRM which we index on surface normal, and apply its value to the corresponding pixels. The result is an illumination consistent set of images.
Approaches to a Color Scannerless Range Imaging System.
Efficient Surface Reconstruction from Range Curves.
3-D Modeling from Range Imagery: An Incremental Method with a Planning Component.
In this paper we present a method for automatically constructing a CAD model of an unknown object from range images. The method is an incremental one that interleaves a sensing operation that acquires and merges information into the model with a planning phase to determine the next sensor position or "view". This is accomplished by integrating a system for 3-D model acquisition with a sensor planner. The model acquisition system provides facilities for range image acquisition, solid model construction and model merging: both mesh surface and solid representations are used to build a model of the range data from each view, which is then merged with the model built from previous sensing operations. The planning system utilizes the resulting incomplete model to plan the next sensing operation by finding a sensor viewpoint that will improve the fidelity of the model. Experimental results are presented for a complex part that includes polygonal faces, curved surfaces, and large self-occlusions.
Virtual Clay Modeling System Using Multi-Viewpoint Images.
This paper proposes a "non-contact virtual clay modeling system." We developed a prototype of three-dimensional modeling system that allows users to shape "virtual clay" with their hand movements. In our proposed method, the usersý hand movements are observed with multiple cameras to estimate their positions. The human hand surface and virtual clay are modeled by using subdivision surface. Using these estimated hand positions, virtual clay is shaped based on a direct free-form deformation technique. To improve processing speed, we implemented the proposed system on a PC cluster. This system proves the feasibility of an intuitive virtual clay modeling system.
Human Figure Reconstruction and Modeling from Single Image or Monocular Video Sequence.
Generating Smooth Surfaces with Bicubic Splines over Triangular Meshes: Toward Automatic Model Building from Unorganized 3D Points.
3D Models from Extended Uncalibrated Video Sequences: Addressing Key-Frame Selection and Projective Drift.
In this paper; we present an approach that is able to reconstruct 3D models from extended video sequences captured with an uncalibrated hand-held camera. We focus on two specific issues: (1) key-frame selection, and ( 2 ) projective drift. Given a long video sequence it is often not practical to work with all videoframes. In addition, to allow for effective outlier rejection and motion estimation it is necessary to have a sufficient baseline between frames. For this purpose, we propose a key-frame selection procedure based on a robust model selection criterion. Our approach guarantees that the camera motion can be estimated reliably by analyzing the feature correspondences between three consecutive views. Another problem for long uncalibrated video sequences is projective drift. Error accumulation leads to a non-projective distortion of the model. This causes the projective basis at the beginning and the end of the sequence to become inconsistent and leads to the failure of self-calibration. We propose a self-calibration approach that is insensitive to this global projective drift. Afterself-calibration triplets of key-frames are aligned using absolute orientation and hierarchically merged into a complete metric reconstruction. Next, we compute a detailed 3D surface model using stereo matching. The 3D model is textured using some of the frames.
Correction of Color Information of a 3D Model Using a Range Intensity Image.
A range intensity image, which is also called a reflectance image, refers to the intensity image that is acquired simultaneously with the range image captured using an active range sensor. This paper proposes a method that uses this image to correct the color information of a textured 3D model. The color information is usually obtained by texture mapping of color images acquired by a digital camera. The lighting condition for the color images are usually not controlled, thus the color information is not precise. On the other hand, the lighting condition for the range intensity image is controlled since it is obtained from a controlled and known lighting as required for the purpose of range measurement. The paper describes the method for combining the two sources of information; experiments show the effectiveness of the correction method.
Modeling Structured Environments by a Single Moving Camera.
Combining Off- and On-Line Calibration of a Digital Camera.
Shape Recovery and Analysis of Large Screw Threads.
Stable Real-Time Interaction Between Virtual Humans And Real Scenes .
The Caesar Project: A 3-D Surface Anthropometry Survey.
Surflet-Pair-Relation Histograms: A Statistical 3D-Shape Representation for Rapid Classification.
3D Shape Recovery and Registration based on the Projection of Non Coherent Structured Light.
Efficient Reconstruction of Indoor Scenes with Color.
Computing Camera Positions from a Multi-camera Head.
Shape Reconstruction of Human Foot from Multi-Camera Images Based on PCA of Human Shape Database.
Recently, researches and developments for measuring and modeling of human body are taking much attention. Our aim is to capture accurate shape of human foot, using 2D images acquired by multiple cameras, which can capture dynamic behavior of the object. In this paper, 3D active shape models is used for accurate reconstruction of surface shape of human foot. We apply Principal Component Analysis (PCA) of human shape database, so that we can represent humanýs foot shape by approximately 12 principal component shapes. Because of the reduction of dimensions for representing the object shape, we can efficiently recover the object shape from multi-camera images, even though the object shape is partially occluded in some of input views. To demonstrate the proposed method, two kinds of experiments are presented: high accuracy reconstruction of human foot in a virtual reality environment with CG multi-camera images and in real world with eight CCD cameras. In those experiments, the recovered shape error with our method is around 2mm, while the error is around 4mm with volume intersection method.
Registering Two Overlapping Range Images.
Registration of 3-D Partial Surface Models using Luminance and Depth Information.
Textured surface models of three-dimensional objects are gaining importance in computer graphics applications. These models often have to be merged from several overlapping partial models which have to be registered (i.e. the relative transformation between the partial models has to be determined) prior to the merging process. In this paper a method is presented that makes use of both camera-based depth information (e.g. from stereo) and the luminance image. The luminance information is exploited to determine corresponding point sets on the partial surfaces using an optical flow approach. Quaternions are then employed to determine the transformation between the partial models which minimizes the sum of the 3-D Euclidian distances between the corresponding point sets. In order to find corresponding points on the partial surfaces luminance information is linearized. The procedure is iterated until convergence is reached. In contrast to only using depth information, employing luminance speeds up convergence and reduces remaining degrees of freedom (e.g. when registering sphere-like shapes).
Curve and Surface Models to Drive 3D Reconstruction Using Stereo and Shading.
Faithful Recovering of Quadric Surfaces from 3D Range Data.
Segmentation and Modeling of Approximately Rotationally Symmetric Objects in 3D Ultrasound.
Indoor Scene Reconstruction from Sets of Noisy Range Images.
3D Capture for Computer Graphics.
Real-Time Image Based Rendering from Uncalibrated Images.
We present a novel real-time image-based rendering system for generating realistic novel views of complex scenes from a set of uncalibrated images. A combination ofstructure-and-motion and stereo techniques is used to obtain calibrated cameras and dense depth maps for all recorded images. These depth maps are converted into restrictive quadtrees, which allow for adaptive, view-dependent tessellations while storing per-vertex quality. When rendering a novel view, a subset of suitable cameras is selected based upon a ranking criterion. In the spirit of the unstructured lumigraph rendering approach a blending field is evaluated, although the implementation is adapted on several points. We alleviate the need for the creation of a geometric proxy for each novel view while the camera blending field is sampled in a more optimal, non-uniform way and combined with the per-vertex quality to reduce texture artifacts. In order to make real-time visualization possible, all critical steps of the visualization pipeline are programmed in a highly optimized way on commodity graphics hardware using the OpenGL Shading Language. The proposed system can handle complex scenes such as large outdoor scenes as well as small objects with a large number of acquired images.
Computing Consistent Normals and Colors from Photometric Data.
Model-Based Scanning Path Generation for Inspection.
Image-Based Object Editing.
Enhanced, Robust Genetic Algorithms for Multiview Range Image Registration.
Efficient Variants of the ICP Algorithm.
Combining texture and shape for automatic crude patch registration.
On Estimating the Position of Fragments on Rotational Symmetric Pottery.
A Discrete Reeb Graph Approach for the Segmentation of Human Body Scans .
Next View Planning for a Combination of Passive and Active Acquisition Techniques.
Additional Reviewers.
Curvature Estimation for Segmentation of Triangulated Surfaces.
3-D Modeling of Human Hand with Motion Constraints.
Taking Consensus of Signed Distance Field for Complementing Unobservable Surface.
Progressive Multilevel Meshes from Octree Particles.
Effective Nearest Neighbor Search for Aligning and Merging Range Images.
Digitizing Archaeological Excavations from Multiple Views.
We present a novel approach on digitizing large scale unstructured environments like archaeological excavations using off-the-shelf digital still cameras. The cameras are calibrated with respect to few markers captured by a theodolite system. Having all cameras registered in the same coordinate system enables a volumetric approach. Our new algorithm has as input multiple calibrated images and outputs an occupancy voxel space where occupied pixels have a local orientation and a confidence value. Both, orientation and confidence facilitate an efficient rendering and texture mapping of the resulting point cloud. Our algorithm combines the following new features: Images are back-projected to hypothesized local patches in the world and correlated on these patches yielding the best orientation. Adjacent cameras build tuples which yield a product of pairwise correlations, called strength. Multiple camera tuples compete each other for the best strength for each voxel. A voxel is regarded as occupied if strength is maximum along the normal. Unlike other multi-camera algorithms using silhouettes, photoconsistency, or global correspondence, our algorithm makes no assumption on camera locations being outside the convex hull of the scene. We present compelling results of outdoors excavation areas.
Appearance-Based Virtual View Generation of Temporally-Varying Events from Multi-Camera Images in the 3D Room.
A Mechanism for Range Image Integration without Image Registration.
A mechanism is introduced that automatically integrates multi-view range images without registering the images. The mechanism is based on a reference double-frame that acts as the coordinate system of the scene. A single-view range image of a scene is obtained by sweeping a laser line over the scene by hand and analyzing the acquired light stripes. Range images captured from different views of the scene will be in the coordinate system of the double-frame, and thus, will automatically integrate without further processing.
Three-Dimensional Shape Modeling with Extended Hyperquadrics.
Dynamic Gaze-Controlled Levels of Detail of Polygonal Objects in 3-D Environment Modeling.
Recovery of Shape and Surface Reflectance of Specular Object from Rotation of Light Source.
Active Modeling of 3-D Objects: Planning on the Next Best Pose (NBP) for Acquiring Range Images.
We propose a new method of creating a complete model of a curved object from multiple range images acquired by showing it at different poses. The pose of the object is changed by a manipulator in order to view the object from some specified viewpoints. The pose is planned after each new image is merged into a unified representation. A rating function for the planning is defined to take into consideration the factors such as possibility of merging new data, registration accuracy and control point selection.
Shape Measurement of Discontinuous Objects using Projected Fringes and Temporal Phase Unwrapping.
Temporal phase unwrapping is a method of analyzing fringe patterns in which the fringe phase, /spl Phi/, at each pixel is measured and unwrapped as a function of time, t. We propose a method for improving the signal-to-noise ratio of the total phase change by incorporating data from the intermediate phase values. The performance of the method is compared theoretical and experimentally, using data from a fringe projector based on a spatial tight modulator. The best way to use the method is with /spl Phi/ decreasing exponentially with time from its maximum value to zero; this provides significant improvements in reliability, accuracy and computation time compared with the original temporal unwrapping algorithm.
Bootstrapped Real-Time Ego Motion Estimation and Scene Modeling.
Estimating the motion of a moving camera in an unknown environment is essential for a number of applications ranging from as-built reconstruction to augmented reality. It is a challenging problem especially when real-time performance is required. Our approach is to estimate the camera motion while reconstructing the shape and appearance of the most salient visual features in the scene. In our 3D reconstruction process, correspondences are obtained by tracking the visual features from frame to frame with optical flow tracking. Optical-flow-based tracking methods have limitations in tracking the salient features. Often larger translational motions and even moderate rotational motions can result in drifts. We propose to augment flow-based tracking by building a landmark representation around reliably reconstructed features. A planar patch around the reconstructed feature point provides matching information that prevents drifts in flow-based feature tracking and allows establishment of correspondences across the frames with large baselines. Selective and periodic such correspondence mappings drastically improve scene and motion reconstruction while adhering to the real-time requirements. The method is experimentally tested to be both accurate and computational efficient.
3-D Imager for Dimensional Gauging of Industrial Workpieces: State-of-the-Art of the Development of a Robust and Versatile System.
Experimental Analysis of Harmonic Shape Images.
Fast Range Image Segmentation by an Edge Detection Strategy.
A Physically-Based Model for Real-Time Facial Expression Animation.
Globally Convergent Range Image Registration by Graph Kernel Algorithm.
Automatic range image registration without any knowledge of the viewpoint requires identification of common regions across different range images and then establishing point correspondences in these regions. We formulate this as a graph-based optimization problem. More specifically, we define a graph in which each vertex represents a putative match of two points, each edge represents binary consistency decision between two matches, and each edge orientation represents match quality from worse to better putative match. Then strict sub-kernel defined in the graph is maximized. The maximum strict sub-kernel algorithm enables us to uniquely determine the largest consistent matching of points. To evaluate the quality of a single match, we employ the histogram of triple products that are generated by all surface normals in a point neighborhood. Our experimental results show the effectiveness of our method for rough range image registration.
Robust Recognition and Pose Determination of 3-D Objects Using Range Images in Eigenspace.
The Planar: A Mobile VR Tool with Pragmatic Pose Estimation for Generation and Manipulation of 3D Data in Industrial Environments.
We present the prototype of the Planar, a novel input/output device designed for applications in task areas focusing on the generation and manipulation of 3D data, e.g. CAD or styling. Technically, the Planar offers a spatially aware pen-sensitive display, mounted on an adjustable, scooter-like autonomous platform. The movable screen, with 6 degrees of freedom, can act like a window into 3D virtual environments and allows for efficient 2D and 3D interaction at the same time. We report on the pose estimation system of the Planar's display where we focus on two enhanced optical mice combined to track horizontal 2D position/orientation. Results of this pragmatic approach are presented and discussed. In order to demonstrate the potential of the Planar we show a small review/sketching/annotation application. The overall goal of this work is to contribute to the development of real VR applications.
From Range Data to Animated Anatomy-Based Faces: A Model Adaptation Method.
This paper presents a new method for reconstructing animated, anatomy-based facial models of individuals from range data with minimal manual intervention. A prototype model with a multi-layer skin-muscle-skull structure serves as the starting point for our method. After the global adaptation, the skin mesh of the prototype model is represented as a dynamic deformable model which is deformed to fit scanned data according to internal force stemming from the elastic properties of the surface and external forces produced from the scanned data points and features. The underlying muscle layer that consists of three types of facial muscles is automatically adapted. According to the adapted skin and muscle structures, a set of automatically generated skull feature points is transformed to drive a volume morphing of the template skull model for skull fitting. The reconstructed model realistically reproduces the shape and features of a specific person and can be animated instantly.
View Planning with a Registration Constraint.
Reconstructing Urban 3D Model Using Vehicle-Borne Laser Range Scanners.
Discrete Pose Space Estimation to Improve ICP-Based Tracking.
Iterative Closest Point (ICP) -based tracking works well when the interframe motion is within the ICP minimum well space. For large interframe motions resulting from a limited sensor acquisition rate relative to the speed of the object motion, it suffers from slow convergence and a tendency to be stalled by local minima. A novel method is proposed to improve the performance of ICP-based tracking. The method is based upon the Bounded Hough Transform (BHT) which estimates the object pose in a coarse discrete pose space. Given an initial pose estimate, and assuming that the interframe motion is bounded in all 6 pose dimensions, the BHT estimates the current frameýs pose. On its own, the BHT is able to track an objectýs pose in sparse range data both efficiently and reliably, albeit with a limited precision. Experiments on both simulated and real data show the BHT to be more efficient than a number of variants of the ICP for a similar degree of reliability. A hybrid method has also been implemented wherein at each frame the BHT is followed by a few ICP iterations. This hybrid method is more efficient than the ICP, and is more reliable than either the BHT or ICP separately.
3D Head Pose Estimation with Optical Flow and Depth Constraints.
Efficient Photometric Stereo Technique for Three-Dimensional Surfaces with Unknown BRDF.
The present paper focuses on efficient inverse rendering using a Photometric Stereo technique for realistic surfaces. The technique primarily assumes the Lambertian reflection model only. For non-Lambertian surfaces, application of the technique to real surfaces in order to estimate 3-D shape and spatially varying reflectance from sparse images remains difficult. In the present paper, we propose a new Photometric Stereo technique by which to efficiently recover a full surface model, starting from a small set of photographs. The proposed technique allows diffuse albedo to vary arbitrarily over surfaces while non-diffuse characteristics remain constant for a material. Specifically, the basic approach is to first recover the specular reflectance parameters of the surfaces by a novel optimization procedure. These parameters are then used to estimate the diffuse reflectance and surface normal for each point. As a result, a lighting-independent model of the geometry and reflectance properties of the surface is established using the proposed method, which can be used to re-render the images under novel lighting via traditional rendering methods.
Extract and Display Moving Object in All Direction by Using Stereo Omnidirectional System(SOS).
Estimation of 3-D Pose and Shape from a Monocular Image Sequence and Realtime Human Tracking.
Digital Preservation of Ancient Cuneiform Tablets Using 3D-Scanning.
Program Committee.
Optimized Compression of Triangle Mesh Geometry Using Prediction Trees.
Efficient Interactive Rendering of Detailed Models with Hierarchical Levels of Detail.
Recent acquisition systems, such as the one developed at the University of California at Berkeley, are capable of collecting large, detailed, highly textured models that standard levels of detail (LOD) rendering techniques [Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments] cannot handle efficiently. We propose an out-of-core rendering engine which applies the cost and benefit approach of the Adaptive Display algorithm by Funkhouser and Séquin [Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments] to Hierarchical Levels of Detail (HLODs) [HLODs for faster display of large static and dynamic environments]. Unlike the Adaptive Display algorithm, we do not skip objects to maintain interactivity when many objects are visible. Funkhouser and Séquin apply hysteresis by adding a penalty in the benefit heuristics to discourage disturbing visual effects due to fast switching of detail in the model. However, this penalty may not be sufficient if the user is moving around rapidly in the scene. Instead, we have developed a more robust temporal hysteresis by retaining how much detail is rendered over a time period. We have implemented our rendering engine to run on a common personal computer with a standard graphics card. The engine is capable of visualizing, in both walk-through and fly-through mode, a detailed model of 25 city blocks comprised of 7 million triangles and 720 million color pixels. Our engine maintains a constant frame rate and limits excessive flickering simultaneously.
Network Protocol for I teraction and Scalable Distributed Visualization.
Disordered Patterns Projection For 3D Motion Recovering.
In this paper we present a new color structured light technique based on a disordered codeword strategy. The aim of this method is to recover 3D information in moving scenes in such a way that the correspondence problem is easily and robustly solved. With this goal in mind a six-connectivity topology has been introduced in our pattern and color features have been inserted on it. Repetition and disorder are allowed in the codeword which implies that the Hamming distance between contiguous codewords increases. As a consequence of that, code loss circumstances can be efficiently handled. Additionally, computational cost in the code recovering phase is highly reduced since codewords are defined as sets. A structured light projection system has been built in our lab and a wide test under real moving conditions has been carried out. This experimentation has been performed on medium resolution and for slow movements specifications giving promising results.
Object Classification by Functional Parts.
Reconstruction of Spherical Representation Models From Multiple Partial Models.
Automated Texture Mapping of 3D City Models With Oblique Aerial Imagery.
This paper describes an approach to texture mapping a 3D city model obtained from aerialand ground-based laser scans with oblique aerial imagery. First, the images are automatically registered by matching 2D image lines with projections of 3D lines from the city model. Then, for each triangle in the model, the optimal image is selected by taking into account occlusion, image resolution, surface normal orientation, and coherence with neighboring triangles. Finally, the utilized texture patches from all images are combined into one texture atlas for compact representation and efficient rendering. We evaluate our approach on a data set of downtown Berkeley.
Human Motion: Modeling and Recognition of Actions and Interactions.
Processing of image sequences has progressed from simple structure from motion paradigm to the recognition of actions / interactions as events. Understanding human activities in video has many potential applications including automated surveillance, video archival/retrieval, medical diagnosis, sports analysis, and human-computer interaction. Understanding human activities involves various steps of low-level vision processing such as segmentation, tracking, pose recovery, and trajectory estimation as well as high-level processing tasks such as body modeling and representation of action. While low-level processing has been actively studied, high-level processing is just beginning to receive attention. This is partly because high-level processing depends on the results of low-level processing. However, high-level processing also requires some independent and additional approaches and methodologies. In this paper, we focus on the following aspects of high-level processing: (1) human body modeling, (2) level of detail needed to understand human actions, (3) approaches to human action recognition, and (4) high-level recognition schemes with domain knowledge. The review is illustrated by examples of each of the areas discussed, including recent developments in our work on understanding human activities.
Data Processing Algorithms for Generating Textured 3D Building Façade Meshes From Laser Scans and Camera Images.
Linear Shift-Invariant Operators for Processing Surface Meshes.
Shift-invariant operators for surface meshes are defined using geometric realizations of the mesh. Then, shift-invariance essentially means isotropy w.r.t. a distance metric. The particular case of the so-defined LSI operators with small support is analyzed in detail, showing a connection to mean value coordinates. The topological Laplacian operator turns out to be the LSI operator of the topological realization of the mesh. More generally, assuming different geometric realizations or metrics allows interpreting various mesh processing techniques as LSI operators.
Combining fringe projection method of 3D object monitoring with virtual reality environment: concept and initial results.
Seeing into the Past: Creating a 3D Modeling Pipeline for Archaeological Visualization.
Archaeology is a destructive process in which accurate and detailed recording of a site is imperative. As a site is exposed, documentation is required in order to recreate and understand the site in context. We have developed a 3D modeling pipeline that can assist archaeologists in the documentation effort by building rich, geometrically and photometrically accurate 3D models of the site. The modeling effort begins with data acquisition (images, range scans, GIS data, and video) and ends with the use of a sophisticated visualization tool that can be used by researchers to explore and understand the site. The pipeline includes new methods for shadow-based registration of 2D images and temporal change detection. Our multimodal augmented reality system allows users wearing head-tracked, see-through, head-worn displays to visualize the site model and associated archaeological artifacts, and to interact with them using speech and gesture.
Real Time Visualization of 3D variable in time Object based on Cloud of Points Data Gathered by Coloured Structure Light Projection System.
The problem of virtual view creation has received increasing attention in recent years. Major current approaches are based on modified stereo vision systems. Recently the structure light measurement system based on digital light projection supported by special data coding and processing allow to rapid 3D shape acquisition. Application of this technology to record 3D data has increased significantly the accuracy of reconstructed shape and simplified data manipulation process. In the paper the general concept of virtual reality system supported by data gathered by means of structure light projection is presented. The methodology of conversion of cloud of measurement points (x,y,z,R,G,B) into virtual reality environment is described. It is supported by implementation of a virtual camera concept, as the mean for interactive object visualization. The methodology of real time 3D object visualization based on its coding by means of specially formed contours and their B-spline approximation is presented. The applicability of the methodology has been shown on numerically generated data which simulate performance of the measurement system. The total processing path was successfully tested.
Frequency Domain Registration of Computer Tomography Data.
This paper presents a new method for registering computer tomography (CT) volumetric data of human bone structures relative to observations made at different times. The system we advance was tested with different kinds of CT data sets. In this paper we report on some representative experimental results obtained with the CT data of the hip bones of a patient prior to and after prosthetic surgery aimed at the reconstruction of the hip articulation. The method works with rigid data having arbitrary relative position and orientation and proves to be robust with respect to CT acquisition noise and with respect to the segmentation technique adopted to select the region of interest for registration. The method is capable of registering correctly data sets taken from the same articulation and whose components have undergone small relative displacements. The method is also amenable to registration of heteregeneous kinds of volumetric data, e.g., CT scans and Magnetic Resonance Imagery (MRI) scans, which show different characteristics in correspondence to the same organic structure.
Recognition of Object Contours from Stereo Images: An Edge Combination Approach.
In this paper, we present an algorithm to combine edge information from stereo-derived depth maps with edges from the original intensity/color image to improve the contour detection in images of natural scenes. After computing the disparity map, we generate a so-called "edge combination image", which relies on those edges of the original image that are also present in the stereo map. We describe an algorithm to identify corresponding intensity and depth edges, which are usually slightly displaced due to non-perfect stereo reconstruction. Our experiments demonstrate how the proposed edge combination approach can be used in conjunction with an active contours algorithm to achieve better segmentation results.
Enhanced Real-time Stereo Using Bilateral Filtering.
In recent years, there have been significant strides in increasing quality of range from stereo using global techniques such as energy minimization. These methods cannot yet achieve real-time performance. However, the need to improve range quality for real-time applications persists. All real-time stereo implementations rely on a simple correlation step which employs some local similarity metric between the left and right image. Typically, the correlation takes place on an image pair modified in some way to compensate for photometric variations between the left and right cameras. Improvements and modifications to such algorithms tend to fall into one of two broad categories: those which address the correlation step itself (e.g., shiftable windows, adaptive windows) and those which address the pre-processing of input imagery (e.g. band-pass filtering, Rank, Census). Our efforts lie in the latter area. We present in this paper a modification of the standard band-pass filtering technique used by many SSD- and SAD-based correlation algorithms. By using the bilateral filter of Tomasi and Manduchi [Bilateral filtering for gray and color images], we minimize blurring at the filtering stage. We show that in conjunction with SAD correlation, our new method improves stereo quality at range discontinuities while maintaining real-time performance.
Fast Interpolated Cameras by Combining a GPU based Plane Sweep with a Max-Flow Regularisation Algorithm.
The paper presents a method for the high speed calculation of crude depth maps. Performance and applicability are illustrated for view interpolation based on two input video streams, but the algorithm is perfectly amenable to multi-camera environments. First a fast plane sweep algorithm generates the crude depth map. Speed results from hardware accelerated transformations and parallel processing available on the GPU. All computations on the graphical board are performed pixel-wise and a single pass of the sweep only processes one input resolution. A second step uses a min-cut/max-flow algorithm to ameliorate the previous result. The depth map, a noisy interpolated image and correlation measures are available on the GPU. They are reused and combined with spatial connectivity information and temporal continuity considerations in a graph formulation. Position dependent sampling densities allow the system to use multiple image resolutions. The min-cut separation of this graph yields the global minimum of the associated energy function. Limiting the search range according to the initialisation provided by the plane sweep further speeds up the process. The required hardware are only two cameras and a regular PC.
A Bayesian Framework for 3D Models Retrieval Based on Characteristic Views.
The management of big databases of three-dimensional models (used in CAD applications, visualization, games, etc.) is a very important domain. The ability to characterize and easily retrieve models is a key issue for the designers and the final users. In this frame, two main approaches exist: search by example of a three-dimensional model, and search by a 2D view. In this paper, we present a novel framework for the characterization of a 3D model by a set of views (called characteristic views), and an indexing process of these models with a Bayesian probabilistic approach using the characteristic views. The framework is independent from the descriptor used for the indexing. We illustrate our results using different descriptors on a collection of threedimensional models supplied by Renault Group.
3D Model Watermarking for Indexing using the Generalized Radon Transform.
The present paper proposes a novel method for 3D-model watermarking for indexing. The proposed approach is based on the use of a Generalized Radon Transformation. More specifically, the Cylindrical Integration Transform (CIT) is initially applied to the 3D models in order to produce descriptor vectors. At the same time a watermarking technique, based on CIT is used in order to embed a specific model identifier in the nodes of the 3D model. This identifier links the model to its descriptor vector, which is extracted only once and stored in a database. Every time this model is employed as a query model, watermark detection is used so as to retrieve the corresponding identifier and further the descriptor vector, which can be further used in a matching algorithm. The proposed techniques are evaluated experimentally in terms of both watermarking efficiency and content-based retrieval performance.
A Structure-from-Motion Method: Use of Motion in Three-Dimensional Reconstruction of Moving Objects from Multiple-View Image Sequences.
Solving the correspondence problem is the most essential task for multiview reconstruction techniques, yet finding unique correspondences between multiple views is impossible at some points, due to such problems as occlusions and ambiguities. We have developed a closed-form solution through constructive geometry for a special case of the structure-from-motion (SfM) problem with four rigidly moving points. This solution allows the 3-D position of a point on a moving object to be computed without having to find the correspondence between its projections on the image planes of multiple views, given its projected 2-D motion vector on an image plane and 3-D information of three other points. With this method we do not have to depend entirely on stereo/multiview feature correspondences in reconstructing 3-D objects, hence easing those problems caused by occlusions and ambiguities.
3D Data Acquisition and Elaboration for Classification and Recognition of Objects and People.
Distributed quantitative evaluation of 3D patient specific arterial models.
New Imaging Frontiers: 3D and Mixed Reality.
Projection Model, 3D Reconstruction and Rigid Motion Estimation from Non-Central Catadioptric Images.
This paper addresses the problem of rigid motion estimation and 3D reconstruction in vision systems where it is possible to recover the incident light ray direction from the image points. Such systems include pinhole cameras and catadioptric cameras. Given two images of the same scene acquired from two different positions, the transformation is estimated by means of an iterative process. The estimation process aims at having corresponding incident rays intersecting at the same 3D point. Geometrical relationships are derived to support the estimation method. Furthermore, this paper also addresses the problem of the mapping from 3D points to image points, for non-central catadioptric cameras with mirror surfaces given by quadrics. The projection model presented can be expressed in a non-linear equation of only one variable, being more stable and easier to solve than the classical Snell's law. Experiments with real images are presented, by using simulated annealing as estimation method.
Scalable and Efficient Coding of 3D Model Extracted from a Video.
This paper presents an efficient and scalable coding scheme for transmitting a stream of 3D models extracted from a video.As in classical model-based video coding, the geometry, connectivity, and texture of the 3D models have to be transmitted, as well as the camera position for each frame in the original video. The proposed method is based on exploiting the interrelations existing between each type of information, instead of coding them independently, allowing a better prediction of the next 3D model in the stream. Scalability is achieved through the use of wavelet-based representations for both texture and geometry of the models. A consistent connectivity is built for all 3D models extracted from the video sequence, which allows a more compact representation and straightforward geometric morphing between successive models. Furthermore this leads to a consistent wavelet decomposition for 3D models in the stream. Quantitative and qualitative results for the proposed scheme are compared with the state of the art video coder H264, 3D model-based Galpin coder and independent MPEG4-based coding of the information. Targeted applications include distant visualization of the original video at very low bitrate and interactive navigation in the extracted 3D scene on heterogeneous terminals.
Modeling shapes and textures from images: new frontiers.
Fusing Multiple Color Images for Texturing Models.
A commonly encountered problem when creating 3D models of large real scenes is unnatural color texture fusion. Due to variations in lighting and camera settings (both manual and automatic), captured color texture maps of the same structure can have very different colors. When fusing multiple views to create larger models, this color variation leads to a poor appearance with odd color tilings on homogeneous surfaces. This paper extends previous research on pairwise global color correction to multiple overlapping images. The central idea is to estimate a set of blending transformations that minimize the overall color discrepancy in the overlapping regions, thus spreading residual color errors, rather than letting them accumulate.
Half-Edge Multi-Tessellation: A Compact Representation for Multi-Resolution Tetrahedral Meshes.
A Prototype of Video See-through Mixed Reality Interactive System.
Mixed reality (MR), sometimes called enhanced reality, is a variety of virtual environment (VE) which explores various approaches to combine natural environment with immersive display technology. VE technologies completely immerse a user inside a synthetic environment. In contrast, MR system adds electronic data from a cyberspace on the physical space as a base, allows the users to see the real world with virtual objects superimposed upon. Moreover, MR can assist the users interact with the virtual object in the more realistic environment. The objective of our research is to investigate the potential of MR technique on improving interaction between human and computer through developing a video-see through mixed reality system. Further applications will be built upon this generic platform.
Novel Diffractive Optical Elements and Algorithms for Real-Time 3D and Hyperspectral Imaging.
Geometry Processing - A Personal Perspective.
Optimized Spectral Estimation Methods for Improved Co orimetry with Laser Scanning Systems.
Using the Expectation-Maximization Algorithm for Depth Estimation and Segmentation of Multi-view Images.
3D Stereoscopic Image Pairs by Depth-Map Generation.
This paper presents a new unsupervised technique aimed to generate stereoscopic views estimating depth information from a single input image.Using a single input image, vanishing lines/points are extracted using a few heuristics to generate an approximated depth map.The depth map is then used to generate stereo pairs.The overall method is well suited for real time application and works also on CFA (Colour Filtering Array) data acquired by consumer imaging devices. Experimental results on a large dataset are reported.
Visualizing Legacy Stratigraphic Data From Archaeological Handbooks.
Architecture of a 3D - Simulation Environment for Active Vision Systems and Mobile Robots.
Multi-Stereo 3D Object Reconstruction.
The ORIGAMI Project: advanced tools and techniques for high-end mixing and interaction between real and virtual content.
Stereo Image Coder Based on MRF Analysis for Disparity Estimation and Morphological Encoding.
This paper presents a stereoscopic image coder based on the MRF model and MAP estimation of the disparity field. The MRF model minimizes the noise of the disparity compensation because it takes into account the residual energy, smoothness constraints and the occlusion field. The disparity compensation is formulated as a MAP-MRF problem in the spatial domain and the MRF field consists of the disparity vector and occlusion field, which is partitioned into three regions by an initial double-threshold setting. The MAP search is conducted in a block-based sense on one or two of the three regions, providing faster execution. The reference and the residual images are decomposed by a discrete wavelet transform and the transform coefficients are encoded by employing the morphological representation of wavelet coefficients algorithm. As a result of the morphological encoding, the reference and residual images together with the disparity vector field are transmitted in partitions lowering the total entropy. The experimental evaluation on synthetic and real images shows beneficial performance of the proposed algorithm.
Remote Machinery Maintenance System with the use of Virtual Reality.
Point Samples for Efficient 3D Processing and Content Creation.
Improvement of Metric Accuracy of Digital 3D Models through Digital Photogrammetry. A Case Study: Donatello's Maddalena.
How can we exploit typical architectural structures to improve model recovery?
An Analysis of Errors in Feature-Preserving Mesh Simplification Based on Edge Contraction.
The Quadric Error Metric (QEM) [1] criterion has been widely applied in mesh simplification procedures. Related criteria--such as the Quasi-Covariance Error Metric (QCEM) [2], which may produce superior results to the QEM--have also been reported in recent years. In this paper, the underlying reasons that criteria such as QEM and QCEM are so successful in mesh simplification processes are considered through analysis of error in the simplification process. Focus is on the use of the QEM and QCEM criteria in edge contraction-based mesh simplification.
IBR-based compression for remote visualization.
Analysis of Secondary Structure Elements of Proteins Using Indexing Techniques.
Real-Time, Accurate Depth of Field using Anisotropic Diffusion and Programmable Graphics Cards.
Computer graphics cameras lack the finite Depth of Field (DOF) present in real world ones. This results in all objects being rendered sharp regardless of their depth, reducing the realism of the scene. On top of that, real-world DOF provides a depth cue, that helps the human visual system decode the elements of a scene. Several methods have been proposed to render images with finite DOF, but these have always implied an important trade-off between speed and accuracy. In this paper, we introduce a novel anisotropic diffusion Partial Differential Equation (PDE) that is applied to the 2D image of the scene rendered with a pin-hole camera. In this PDE, the amount of blurring on the 2D image depends on the depth information of the 3D scene, present in the Z-buffer. This equation is well posed, has existence and uniqueness results, and it is a good approximation of the optical phenomenon, without the visual artifacts and depth inconsistencies present in other approaches. Because both inputs to our algorithm are present at the graphics card at every moment, we can run the processing entirely in the GPU. This fact, coupled with the particular numerical scheme chosen for our PDE, allows for real-time rendering using a programmable graphics card.
Dynamically Optimised 3D (Virtual Reality) Data Transmission for Mobile Devices.
Nowadays the processing power of mobile phones, Smartphones and PDA's is increasing as well as the transmission bandwidth.Nevertheless there is still the need to reduce the content and the need of processing the data.Discussed will be proposals and solutions for dynamic reduction of the transmitted content.For that, device specific properties will be taken into account, as much as for the aim to reduce the need of processing power at the client side to be able to display the 3D (Virtual Reality) data.Therefore, well known technologies e.g. data compression are combined with new developed ideas to reach the goal of adaptive content transmission.To achieve a device dependent reduction of processing power the data have to be pre processed at the server side or the server even has to take over functionality of weak mobile devices.
Learning Illumination Models While Tracking.
In this paper we present a method for estimation of 3D motion of a rigid object from a video sequence, while simultaneously learning the parameters of an illumination model that describe the lighting conditions under which the video was captured. This is achieved by alternately estimating motion and illumination parameters in a recently proposed mathematical framework for integrating the effects of motion, illumination and structure. The motion is represented in terms of translation and rotation of the object centroid, and the illumination is represented using a spherical harmonics linear basis. The method does not assume any model for the variation of the illumination conditions-lighting can change slowly or drastically, locally or globally. Also, it can be composed of combinations of point and extended sources. For multiple cameras viewing an object, we derive a new photometric constraint that relates the illumination parameters in two or more independent video sequences. This constraint allows verification of the illumination parameters obtained from multiple views and synthesis of new views under the same lighting conditions. We demonstrate the effectiveness of our algorithm in tracking under severe changes of lighting conditions.
VENUS Subsurface Ionosphere Radar Sounder: VENSIS.
Due to optically opaque atmosphere, radar is the best way to observe the surface of Venus from orbit. Magellan has obtained global SAR imaging, as well as altimetry and emissivity. As a subsurface sounder, VENSIS would obtain fundamentally different kinds of geologic information than Magellan. Mapping of interfaces of geologic units (e.g. tessera, plains, lava flows, impact debris) could be extended into the third dimension. Reflectivity variations recorded at the surface by Magellan are likely to extend into subsurface, providing dielectric contrast at interfaces.
Human Action Recognition By Sequence of Movelet Codewords.
Fast and Robust Bore Detection in Range Image Data for Industrial Automation.
This paper presents a fast and robust method to precisely segment and locate bore holes of 4 to 50mm diameter. The task is solved by a robot moving a compact triangulation scanning sensor to all sides of the object and scanning the bore holes. Exploiting the knowledge about the expected bore diameter and bore pose makes it possible to develop highly robust algorithms for an industrial application. Sparse data of the bore hole is sufficient to segment the bore independent of bore hole chamfer type using a robust normal vector fit and a classification based on the Gaussian image. A sequential algorithm to fit the bore cylinder makes it possible to calculate the bore pose in less than 1 second. Experiments demonstrate that 120 degrees of the bore hole surface are sufficient for robust localization within 0.3mm and 0.5 degrees even in the presence of ghost points and notches in the bore holes.
Helmholtz Stereopsis on Rough and Strongly Textured Surfaces.
Helmholtz Stereopsis (HS) has recently been explored as a promising technique for capturing shape of objects with unknown reflectance. So far, it has been widely applied to objects of smooth geometry and piecewise uniform Bidirectional Reflectance Distribution Function (BRDF). Moreover, for non-convex surfaces the inter-reflection effects have been completely neglected. We extend the method to surfaces which exhibit strong texture, nontrivial geometry and are possibly non-convex. The problem associated with these surface features is that Helmholtz reciprocity is apparently violated when point-based measurements are used independently to establish the matching constraint as in the standard HS implementation. We argue that the problem is avoided by computing radiance measurements on image regions corresponding exactly to projections of the same surface point neighbourhood with appropriate scale. The experimental results demonstrate the success of the novel method proposed on real objects.
Ellipsoid decomposition of 3D-model.
Construction of Large-Scale Virtual Environment by Fusing Range Data, Texture Images, and Airborne Altimetry Data.
Accurate 3D Acquisition of Freely Moving Objects.
This paper presents a new acquisition method for 3D laser scanners that combines imaging, fast geometrical object tracking, and automatic pose estimation to register range profiles of freely moving objects. The method was developed to solve the constraint of rigidity between free-moving objects and a 3D scanner while preserving the accuracy of the range measurements. Rigidity constraint imposes that a 3D scanner or any external positioning devices must be perfectly stable relative to the object during scanning. This is often impossible for moving structures such as when using scaffolding, industrial conveyers, or robotic arms. The method starts by creating a rough, partial, and distorted estimate of the model of the object from an initial subset of sparse range data. Then, it recursively improves and refines the model by adding new range information. In parallel, real-time tracking of the object is performed to center the scan on the object. A high-resolution and accurate 3D model of a free-floating object, and real-time tracking of its position is obtained.
Using Omnidirectional Structure from Motion for Registration of Range Images of Minimal Overlap.
In this paper, we propose a novel method of merging a series of range images with a minimal overlap between any two consecutive range images. We rigidly mount a parabolic catadioptric camera to the range scanner. Using two omniviews we are able to accurately estimate the relative displacement between two range views. The resultant motion is used for the registration of all range data to the same coordinate system. An additional perspective camera calibrated with respect to the scanner is used for texture mapping.
A Statistical Method for Robust 3D Surface Reconstruction from Sparse Data.
General information about a class of objects, such as human faces or teeth, can help to solve the otherwise ill-posed problem of reconstructing a complete surface from sparse 3D feature points or 2D projections of points. We present a technique that uses a vector space representation of shape (3D Morphable Model) to infer missing vertex coordinates. Regularization derived from a statistical approach makes the system stable and robust with respect to noise by computing the optimal tradeoff between fitting quality and plausibility. We present a direct, non-iterative algorithm to calculate this optimum efficiently, and a method for simultaneously compensating unknown rigid transformations. The system is applied and evaluated in two different fields: (1) reconstruction of 3D faces at unknown orientations from 2D feature points at interactive rates, and (2) restoration of missing surface regions of teeth for CAD-CAM production of dental inlays and other medical applications.
Enhanced Vector Quantization for Data Reduction and Filtering.
Modern automatic digitizers can sample huge amounts of 3D data points on the object surface in a short time. Point based graphics is becoming a popular framework to reduce the cardinality of these data sets and to filter measurement noise, without having to store in memory and process mesh connectivity. Main contribution of this paper is the introduction of soft clustering techniques in the field of point clouds processing. In this approach data points are not assigned to a single cluster, but they contribute in the determination of the position of several cluster centres. As a result a better representation of the data is achieved. In Soft clustering techniques, a data set is represented with a reduced number of points called Reference Vectors (RV), which minimize an adequate error measure. As the position of the RVs is determined by "learning", which can be viewed as an iterative optimization procedure, they are inherently slow. We show here how partitioning the data domain into disjointed regions called hyperboxes (HB), the computation can be localized and the computational time reduced to linear in the number of data points (O(N)), saving more than 75% on real applications with respect to classical soft-VQ solutions, making therefore VQ suitable to the task. The procedure is suitable for a parallel HW implementation, which would lead to a complexity sub-linear in N. An automatic procedure for setting the voxel side and the other parameters can be derived from the data-set analysis. Results obtained in the reconstruction of faces of both humans and puppets as well as on models from clouds of points made available on the WEB are reported and discussed in comparison with other available methods.
ShapeLab: A Unified Framework for 2D & 3D Shape Retrieval.
2D or 3D shapes are the most important visual information that we use to recognize an object. We propose a unified framework "ShapeLab" to search similar 2D or 3D shapes from an existing database. Users can search 3D shapes with a 2D input, and vice versa. ShapeLab is composed of four key components: (1) pose determination for 3D models; (2) 2D orthogonal view generation based on multiple levels of detail; (3) similarity measurement between 2D shapes; and (4) freehand sketch-based user interface. Key algorithms supporting the above components are briefly described. Experiments show ShapeLab can provide a better performance such as high accuracy, flexibility and scalability compared to the available methods.
Online Surface Reconstruction from Unorganized 3D-Points for the DLR Hand-Guided Scanner System.
Hand-guided scanners allow for digitization by manually sweeping a laser beam over an object's surface. The result highly depends on the way the user handles the system and his ability to keep track of the parts of the surface that are already scanned. Processing and visualization during data acquisition are helpful in this context. In this paper, we propose an online surface reconstruction algorithm for the visualization of the DLR scanner system data. The algorithm successively generates a triangle mesh by incrementally inserting 3D points. Point neighborhoods are used to limit the point density, to estimate the surface normal at the inserted point, and to locally re-triangulate the mesh. A dynamic data structure for fast neighborhood search without restrictions to the amount of vertices or the object size and with low complexity is introduced. Finally, results with the hand-guided scanner system are presented.
Robust Structure from Motion under Weak Perspective.
It is widely known that, for the affine camera model, both shape and motion data can be factorized directly from the measurement matrix constructed from 2D image points coordinates. However, classical algorithms for Structure from Motion (SfM) are not robust: measurement outliers, that is, incorrectly detected or matched feature points can destroy the result. A few methods to robustify SfM have already been proposed. Different outlier detection schemes have been used. We examine an efficient algorithm by Trajkovic et al. [Robust recursive structure and motion recovery under affine projection] who use affine camera model and the Least Median of Squares (LMedS) method to separate inliers from outliers. LMedS is only applicable when the ratio of inliers exceeds 50%. We show that the Least Trimmed Squares (LTS) method is more efficient in robust SfM than LMedS. In particular, we demonstrate that LTS can handle inlier ratios below 50%. We also show that using the real (Euclidean) motion data results in a more precise SfM algorithm that using the affine camera model. Based on these observations, we propose a novel robust SfM algorithm and discuss its advantages and limits. The proposed method and the Trajkovicprocedure are quantitatively compared on synthetic data in different simulated situations. The methods are also tested on synthesized and real video sequences.
GPU-Assisted Z-Field Simplification.
Height fields and depth maps which we collectively refer to as z-fields, usually carry a lot of redundant information and are often used in real-time applications. This is the reason why efficient methods for their simplification are necessary. On the other hand, the computation power and programmability of commodity graphics hardware has significantly grown. We present an adaptation of an existing real-time z-field simplification method for execution in graphics hardware. The main parts of the algorithm are implemented as fragment programs which run on the GPU. The resulting polygonal models are identical to the ones obtained by the original method. The main benefit is that the computation load is imposed on the GPU, freeing-up the CPU for other tasks. Additionally, the new method exhibits a performance improvement when compared to a pure CPU implementation.
Estimating the Principal Curvatures and the Darboux Frame from Real 3D Range Data.
Tracking Densely Moving Markers.
Using Principal Curvatures and Darboux Frame to Recover 3D Geometric Primitives from Range Images.
3D Shape Estimation Based on Density Driven Model Fitting.
Multiple View Reconstruction of People.
This paper presents a unified framework for model-based and model-free reconstruction of people from multiple camera views in a studio environment. Shape and appearance of the reconstructed model are optimised simultaneously based on multiple view silhouette, stereo and feature correspondence. A priori knowledge of surface structure is introduced as regularisation constraints. Model-based reconstruction assumes a known generic humanoid model a priori, which is fitted to the multi-view observations to produce a structured representation for animation. Model-free reconstruction makes no priori assumptions on scene geometry allowing the reconstruction of complex dynamic scenes. Results are presented for reconstruction of sequences of people from multiple views. The model-based approach produces a consistent structured representation, which is robust in the presence of visual ambiguities. This overcomes limitations of existing visual-hull and stereo techniques. Model-free reconstruction allows high-quality novel view-synthesis with accurate reproduction of the detailed dynamics for hair and loose clothing. Multiple view optimisation achieves a visual quality comparable to the captured video without visual artefacts due to misalignment of images.
Blind Watermarking of 3D Shapes using Localized Constraints.
This paper develops a digital watermarking methodology for 3-D graphical objects defined by polygonal meshes. In watermarking or fingerprinting the aim is to embed a code in a given media without producing identifiable changes to it. One should be able to retrieve the embedded information even after the shape had suffered various modifications. Two blind watermarking techniques applying perturbations onto the local geometry for selected vertices are described in this paper. The proposed methods produce localized changes of vertex locations that do not alter the mesh topology. A study of the effects caused by vertex location modification is provided for a general class of surfaces. The robustness of the proposed algorithms is tested at noise perturbation and object cropping.
Tele-3D - Developing a Handheld Scanner Using Structured Light Projection.
Acquisition, Modelling and Rendering of Very Large Urban Environments.
In this paper we describe a vehicle borne data acquisition system for urban environments and associated 3D data management and interactive rendering software. The data acquisition system is capable of acquire 3D data from urban areas with centimetre resolution including automatic capturing of colour. The system includes a management and interactive rendering software which is designed to cope with the huge quantities of data generated by the acquisition system. It uses out-of-core pre-processing to transform data into octrees. Real-time interactive rendering is achieved by using novel techniques such as front-to-back octree traversal, occlusion query and speculative pre-fetching. The paper presents the results of the described techniques applied to large public areas including the City Centre of Verona, Italy.
From 3D Shape Capture to Animated Models.
Neuroanatomical Imaging: Constrained 3D Reconstruction Using Variational Implicit Techniques.
Real-Time Speech-Driven 3D Face Animation.
Some Unusual Ways of Visually Sensing 3D Shapes.
User-Controlled Simplification of Polygonal Models.
Polygonal Models are ubiquitous in Computer Graphics but their real time manipulation and rendering especially in interactive application environments have become a threat because of their huge sizes and complexity. A whole family of automatic algorithms emerged during the last decade to help out this problem, but they reduce the complexity of the models uniformly without caring about semantic importance of localized parts of a mesh. Only a few algorithms deal with adaptive simplification of polygonal models. We propose a new model for user-driven simplification exploiting the simplification hierarchy and hypertriangulation model [A resolution modeling system] that lends a user the most of the functionalities of existing adaptive simplification algorithms in one place and is quite simple to implement. The proposed new underlying data structures are compact and support real time navigation across continuous LODs of a model; any desirable LOD can be extracted efficiently and can further be fine-tuned. The proposed model for adaptive simplification supports two key operations for selective refinement and selective simplification; their effect has been shown on various polygonal models. Comparison with related work shows that the proposed model provides combined environment at reduced overhead of memory space usage and faster running times.
Non-Rigid Range-Scan Alignment Using Thin-Plate Splines.
We present a non-rigid alignment algorithm for aligning high-resolution range data in the presence of low-frequency deformations, such as those caused by scanner calibration error. Traditional iterative closest points (ICP) algorithms, which rely on rigid-body alignment, fail in these cases because the error appears as a non-rigid warp in the data. Our algorithm combines the robustness and efficiency of ICP with the expressiveness of thin-plate splines to align high-resolution scanned data accurately, such as scans from the Digital Michelangelo Project [The Digital Michelangelo Project: 3-D scanning of large statues]. This application is distinguished from previous uses of the thin-plate spline by the fact that the resolution and size of warping are several orders of magnitude smaller than the extent of the mesh, thus requiring especially precise feature correspondence.
Shape Distortion Analysis of the Shape-from-Shading Algorithm Using Jacobi Iterative Method.
Metrological Analysis of a Procedure for the Automatic 3D Modeling of Dental Plaster Casts.
As well known, in the reconstruction of the 3D models through optical systems, the errors are due to the single-view acquisition error and to the 3D modeling procedure. The latter can be ascribed to the various phases of the 3D modeling pipeline: pairwise registration, global registration, surface integration. This work examines the acquisition error as well as the errors due to an automatic procedure recently proposed for the 3D modeling of dental plaster casts. This contribution derives a simple error propagation model, rather useful for practical simulation purposes. From a general viewpoint, this contribution proposes a useful simulation of error propagation in 3D modeling, it shows the quality of an automatic 3D modeling procedure recently proposed and it shows the accuracy of 3D modeling dental plaster casts by current commercial range cameras and the considered automatic method.
Mathematical Aspects of Shape Reconstruction from an Image Sequence.
Spherical Diffusion for 3D Surface Smoothing.
A diffusion-based approach to surface smoothing is presented. Surfaces are represented as scalar functions defined on the sphere. The approach is equivalent to Gaussian smoothing on the sphere and is computationally efficient since it does not require iterative smoothing. Furthermore, it does not suffer from the well-known shrinkage problem. Evolution of important shape features (parabolic curves) under diffusion is demonstrated. A nonlinear modification of the diffusion process is introduced in order to improve smoothing behavior of elongated and poorly centered objects.
Coding with ASCII: compact, yet text-based 3D content.
Realistic Models of Children Heads from 3D-MRI Segmentation and Tetrahedral Mesh Construction.
In order to analyze the sensitivity of children to RF fields and mobile phones in particular, the SAR (Specific Absorption Ratio) defined as the power absorbed by a unit of mass of tissues (W/kg) should be computed based on a numerical model of the head.We propose to build realistic models from 3D-MRI of children heads.The method is composed of two steps. The first one consists in segmenting the main tissues in these images (skin, fat, muscles, cortical and marrow bones, cerebrospinal fluid, grey and white matter, blood, etc).The segmentation is based on mathematical morphology methods which are well adapted to this aim and provide a robust and automatic method requiring minimum user intervention.Using simplified segmented images, the second step concerns the tetrahedral mesh generation.Our method uses almost regular meshes and topological tools to preserve the topological arrangement of the head tissues.A method to guarantee a good geometrical quality is also provided.
Theoretical Accuracy Analysis of N-Ocular Vision Systems for Scene Reconstruction, Motion Estimation, and Positioning.
Theoretical models are derived to analyze the accuracy of N-Ocular vision systems for scene reconstruction, motion estimation and self positioning. Covariance matrices are given to estimate the uncertainty bounds for the reconstructed points in 3-D space, motion parameters, and 3-D position of the vision system. Simulation results of various experiments, based on synthetic and real data acquired with a 12-camera stereo panoramic imaging system, are given to demonstrate the application of these models, as well as to evaluate the performance of the panoramic system for high-precision 3-D mapping and positioning.
Local Approximate 3D Matching of Proteins in Viral Cryo-EM Density Maps.
Experimental structure analysis of biological molecules (e.g, proteins) or macromolecular complexes (e.g, viruses) can be used to generate three-dimensional density maps of these entities. Such a density map can be viewed as a three-dimensional gray-scale image where space is subdivided in voxels of a given size. The focus of this paper is the analysis of virus density maps. The hull of a virus consists of many copies of one or several different proteins. An important tool for the study of viruses is cryo-electron microscopy (cryo-EM), a technique with insufficient resolution to directly determine the arrangement of the proteins in the virus. We therefore created a tool that locates proteins in the three-dimensional density map of a virus. The goal is to fully determine the locations and orientations of the protein(s) in the virus given the virus' three-dimensional density map and a database of density maps of one or more protein candidates.
Stochastic Mesh-Based Multiview Reconstruction.
An Experimental Comparison of Feature-Based 3D Retrieval Methods.
3D objects are an important type of multimedia data with many promising application possibilities. Defining the aspects that constitute the similarity among 3D objects, and designing algorithms that implement such similarity definitions is a difficult problem. Over the last few years, a strong interest in methods for feature-based 3D similarity search has arisen, and a growing number of competing algorithms for content-based retrieval of 3D objects have been proposed. We present an extensive experimental evaluation of the retrieval effectiveness and efficiency of a large part of the current state-of-the-art feature-based methods for 3D similarity search, giving a contrasting assessment of the different approaches.
Neural Mesh Ensembles.
This paper proposes the use of neural network ensembles to boost the performance of a neural network based surface reconstruction algorithm. Ensemble is a very popular and powerful statistical technique based on the idea of averaging several outputs of a probabilistic algorithm. In the context of surface reconstruction, two main problems arise. The first is finding an efficient way to average meshes with different connectivity, and the second is tuning the parameters for surface reconstruction to maximize the performance of the ensemble. We solve the first problem by voxelizing all the meshes on the same regular grid and taking majority vote on each voxel. We tune the parameters experimentally, borrowing ideas from weak learning methods.
Weaver, an automatic texture builder.
From the Lab to the Silver Screen: Computer Vision and the Art of Special Effects.
RoboScan: An Automatic System for Accurate and Unattended 3D Scanning.
We describe an automatic system for fast unattended acquisition of accurate and complete 3D models, called RoboScan. The design goal is to reduce the three main bottlenecks in human-assisted 3D scanning: the selection of the range maps to be taken (view planning), the positioning of the scanner in the environment, and the range maps' alignment. The system is designed around a commercial laser-based 3D scanner moved by a robotic arm. The acquisition session is organised in two stages. First, an initial sampling of the surface is performed by the automatic selection of a set of views. Then, some added views are automatically selected, acquired and merged to the initial set in order to fill the surface regions left unsampled. Both the initial set of range maps and the subsequently added ones are post-processed automatically, by using the known scanner positions to initialise the alignment phase. Results of the assessment of the system on real acquisitions are presented and discussed.
Computational Experiments with Area-Based Stereo for Image-Based Rendering.
Optimal and Near - Optimal Solutions for 3D Structure Comparisons.
Photo-Consistency Based Registration of an Uncalibrated Image Pair to a 3D Surface Model Using Genetic Algorithm.
We consider the following data fusion problem. A 3D object with textured Lambertian surface is measured and independently photographed. A triangulated model of the object and two uncalibrated images are obtained. The goal is to precisely register the images to the model. Solving this problem is necessary for building a geometrically accurate, photorealistic model from laser-scanned 3D data and high quality images. Recently, we have proposed a novel method that generalises the photo-consistency approach by Clarkson et al. [Using photo-consistency to register 2D optical images of the human face to a 3D surface model] to the case of uncalibrated cameras, when both intrinsic and extrinsic parameters are unknown. This gives a user the freedom of taking the pictures by a conventional digital camera, from arbitrary positions and with varying zoom. The method is based on manual pre-registration followed by a genetic optimisation algorithm. A brief description of the pilot version of the method [Precise registration of an uncalibrated image pair to a 3D surface model] has been given together with the results of a few initial tests. In this paper, we report on some new significant developments in this project. The critical issue of robustness against illumination changes is addressed and various colour representations and cost functions are tested and compared. Natural constraints are introduced and experimentally validated to simplify the camera model and accelerate the algorithm. Finally, we present synthetic and real data with ground truth, apply the improved method to the data and measure the quality of the results.
Multiresolution Approach to Three-Dimensional Stereo Vision.
An Approach to Using Image-Based Techniques across Unreliable Peer-to-Peer Networks.
Acquiring Height Maps of Faces from a Single Image.
In this paper we explore how to improve the quality of the height map recovered from faces using shape-from-shading. One of the problems with reliable face surface reconstruction using shape-from-shading is that local errors in the needle map can cause implosion of facial features, and in particular the nose. To overcome this problem in this paper we develop a method for ensuring surface convexity. This is done by modifying the gradient orientations in accordance with critical points on the surface. We utilize a local shape indicator as a criteria to decide which surface normals are to be modified. Experiments show that altering the directions of a surface normal field of a face leads to a considerable improvement in its integrated height map.
Towards Urban 3D Reconstruction from Video.
The paper introduces a data collection system and a processing pipeline for automatic geo-registered 3D reconstruction of urban scenes from video. The system collects multiple video streams, as well as GPS and INS measurements in order to place the reconstructed models in geo-registered coordinates. Besides high quality in terms of both geometry and appearance, we aim at real-time performance. Even though our processing pipeline is currently far from being real-time, we select techniques and we design processing modules that can achieve fast performance on multiple CPUs and GPUs aiming at real-time performance in the near future. We present the main considerations in designing the system and the steps of the processing pipeline. We show results on real video sequences captured by our system.
Improving Environment Modelling by Edge Occlusion Surface Completion.
Visual Data Navigators "Collaboratories".
View Dependence of 3D Recovery from Folded Pictures and Warped 3D Faces.
In a popular visual illusion, the portrait on paper currency is folded into an M shape along vertical lines through the nose and the eyes. When this folded picture is tilted back and forth horizontally the face undergoes striking changes in expression. This distortion reveals two insights concerning 3D representation in the human visual system and we have explored these with experiments on simple schematic faces and observations on distortions of laser range images of faces. The observations show first that when recovering depicted depth, pictorial cues are interpreted independently of binocular depth information and second, that the recovery of facial expression is based on a scaled prototypical face structure.
Variational Multiframe Stereo in the Presence of Specular Reflections.
A Depth Map Representation for Real-Time Transmission and View-Based Rendering of a Dynamic 3D Scene.
Probabilistic 3D Data Fusion for Adaptive Resolution Surface Generation.
3D Shape Registration using Regularized Medial Scaffolds.
This paper proposes a novel method for global registration based on matching 3D medial structures of unorganized point clouds or triangulated meshes. Most practical known methods are based on the Iterative Closest Point (ICP) algorithm, which requires an initial alignment close to the globally optimal solution to ensure convergence to a valid solution. Furthermore, it can also fail when there are points in one dataset with no corresponding matches in the other dataset. The proposed method automatically finds an initial alignment close to the global optimal by using the medial structure of the datasets. For this purpose, we first compute the medial scaffold of a 3D dataset: a 3D graph made of special shock curves linking special shock nodes. This medial scaffold is then regularized exploiting the known transitions of the 3D medial axis under deformation or perturbation of the input data. The resulting simplified medial scaffolds are then registered using a modified graduated assignment graph matching algorithm. The proposed method shows robustness to noise, shape deformations, and varying surface sampling densities.
A Multi-Resolution Scheme ICP Algorithm for Fast Shape Registration.
Super High Resolution 3D Imaging and Efficient Visualization.
Applying Mesh Conformation on Shape Analysis with Missing Data.
A mesh conformation approach that makes use of deformable generic meshes has been applied to establishing correspondences between 3D shapes with missing data.Given a group of shapes with correspondences, we can build up a statistical shape model by applying principal component analysis (PCA). The conformation at first globally maps the generic mesh to the 3D shape based on manually located corresponding landmarks, and then locally deforms the generic mesh to clone the 3D shape.The local deformation is constrained by minimizing the energy of an elastic model.An algorithm was also embedded in the conformation process to fill missing surface data of the shapes.Using synthetic data, we demonstrate that the conformation preserves the configuration of the generic mesh and hence it helps to establish good correspondences for shape analysis.Case studies of the principal component analysis of shapes were presented to illustrate the successes and advantages of our approach.
Adaptive Online Transmission of 3D TexMesh Using Scale-Space Analysis.
Wavelet Coding of Structured Geometry Data Considering Rate-Distortion Properties.
Reliability and Judging Fatigue Reduction in 3D Perceptual Quality Estimation.
Content-Based Image Retrieval from Large Medical Databases.
Markerless Human Motion Transfer.
In this paper we develop a computer vision-based system to transfer human motion from one subject to another. Our system uses a network of eight calibrated and synchronized cameras. We first build detailed kinematic models of the subjects based on our algorithms for extracting shape from silhouette across time [A 3D reconstruction algorithm combining shape-frame-shilhouette]. These models are then used to capture the motion (joint angles) of the subjects in new video sequences. Finally we describe an image-based rendering algorithm to render the captured motion applied to the articulated model of another person. Our rendering algorithm uses an ensemble of spatially and temporally distributed images to generate photo-realistic video of the transferred motion. We demonstrate the performance of the system by rendering throwing and kungfu motions on subjects who did not perform them.
Topology and Geometry of Unorganized Point Clouds.
We present a new method for defining neighborhoods, and assigning principal curvature frames, and mean and Gauss curvatures to the points of an unorganized oriented point-cloud. The neighborhoods are estimated by measuring implicitly the surface distance between points. The 3D shape recovery is based on conformal geometry, works directly on the cloud, does not rely on the generation of polygonal, or smooth models. Test results on publicly available synthetic data, as ground truth, demonstrate that the method compares favorably to the established approaches for quantitative 3D shape recovery. The proposed method is developed to serve applications involving point based rendering and reliable extraction of differential properties from noisy unorganized point-clouds.
A Content-Based Retrieval System with a Customizeable 3D Output Visualizer.
Octree-based Fusion of Shape from Silhouette and Shape from Structured Light.
Uncalibrated 3 metric reconstruction and flattened texture acquisition from a single view of a surface of revolution.
Inpainting from Multiple Views.
Face Recognition from 3D Data using Iterative Closest Point Algorithm and Gaussian Mixture Models.
A new approach to face verification from 3D data is presented. The method uses 3D registration techniques designed to work with resolution levels typical of the irregular point cloud representations provided by Structured Light scanning. Preprocessing using a-priori information of the human face and the Iterative Closest Point algorithm are employed to establish correspondence between test and target and to compensate for the non-rigid nature of the surfaces. Statistical modelling in the form of Gaussian Mixture Models is used to parameterise the distribution of errors in facial surfaces after registration and is employed to differentiate between intra- and extra-personal comparison of range images. An Equal Error Rate of 2.67% was achieved on the 30 subject manual subset of the the 3d_rma database.
Entire Model Acquisition System using Handheld 3D Digitizer.
In this paper, a real-time, handheld 3D model acquisition system consisting of a laser projector, a video camera and a turntable is described. The user projects a stripe of light at the 3D object by hand while rotating the object on a turntable. The projected light and LED markers attached to the laser projector and turntable are captured by the video camera. By estimating the 3D orientation of the laser projector and the turntable angle from the 2D locations of the markers, the 3D location of the surface lit by the laser can be calculated. In addition, post-processing algorithms for refining the estimated 3D data have been proposed. The algorithm not only improves the accuracy of the 3D measurement, but also achieves to decrease the number of LEDs for 3D data estimation; therefore, it significantly improves the userýs convenience in scanning the object. With this system, users can measure an entire 3D object in real-time.
3D Effect Generation from Monocular View.
Synthetic Image of Multiresolution Sketch Leads to New Features.
A new approach to construction of robust features is proposed and applied to an instance of the correspondence problem. The main idea is to construct a synthetic image by a multiresolution sketch (MS) of an image and involve it into extraction of the invariants. The MS is constructed by processing the image with a scalable detector of the semi-local 1D-elements. Then, a synthetic image is constructed with all elements of the MS. Local maxima of the first and second derivatives of the synthetic image along discrete curves of the MS lead to some singular elements represented by the points of a 4D manifold. It turns out that a representative subset of the singular elements is stable. To prove that, the pair-wise correspondence between subsets of singular elements of two shots of a film was established experimentally by a consistency technique, which, unlike past approaches, does not involve epipolar constraints.
Hierarchical 3D Surface Reconstruction based on Radial Basis Functions.
Volumetric methods based on implicit surfaces are commonly used in surface reconstruction from uniformly distributed sparse 3D data. The case of non-uniform distributed data has recently deserved more attention, because it occurs frequently in practice. This paper describes a volumetric approach to surface reconstruction from non-uniform data which is suitable for the reconstruction of surfaces from images, in particular from multiple views. Differently from volumetric methods which use both 3D surface points and surface normals, the approach does not use the surface normals because they are often unreliable when estimated from image data. The method is based on a hierarchical partitioning of the volume data set. The working volume is split and classified at different scales of spatial resolution into surface, internal and external voxels and this hierarchy is described by an octree structure in a multiscale framework. The octree structure is used to build a multiresolution description of the surface by means of compact support radial basis functions (RBF). A hierarchy of surface approximations at different levels of details is built by representing the voxels at the same octree level as RBF of similar spatial support. At each scale, information related to the reconstruction error drives the reconstruction process at the following finer scale. Preliminary results on synthetic data and future perspectives are presented.
Shape Matching using the 3D Radon Transform.
In this paper a novel method for 3D model content-based search and retrieval based on the 3D Radon Transform and a querying-by-3D-model approach, is presented. Descriptors are extracted using the 3D Radon Transform and applying a set of functionals on the transform coefficients. Similarity measures are then created for the extracted descriptors and introduced into a 3D model-matching algorithm. This results to a very fast and accurate matching method. Experiments were performed using two different databases and comparing the proposed method with others. Experimental results show that the proposed method can be used for 3D model search and retrieval in a highly efficient manner.
Archiving Technology for Plant Inspection Images Captured by Mobile Active Cameras - 4D Visible Memory.
Filling Holes in Complex Surfaces using Volumetric Diffusion.
Robust Concealment for Erroneous Block Bursts in Stereoscopic Images.
With the increasing number of image communication applications especially in the low complexity domain, error concealment has become a very important field of research. Since many compression standards for images and videos are block-based a lot of methods were applied to conceal block losses in monocular images. The fast progress of capture, representation and display technologies for 3D image data advances the efforts on 3D concealment strategies. Because of their psycho-visual characteristics, stereoscopic images have to fulfill a very high quality demand. We propose an algorithm that makes use of the redundancies between two views of a stereo image pair. In many cases erroneous block bursts occur and can be highly disturbing, thus we will mainly concentrate on these errors. In addition, we focused on the quality assessment of several error concealment strategies. Beside the objective evaluation measures, we carried out a subjective quality test following the DSCQS methodology as proposed by MPEG. The results of this test demonstrate the efficiency of our approach.
Fast Landmark-Based Registration via Deterministic and Efficient Processing, Some Preliminary Results.
Texture at the Terminator.
Using 3D-Bresenham for Resampling Structured Grids.
Structured grids and some of their applications in natural sciences are discussed. The problem of their visualization and quantitative evaluation is considered and possible ways for itssolution sketched. Resampling a structured grid onto a regular one is such a possible solution offering the additional benefit of enabling quantitative evaluations, too. This resampling is achieved by a preliminary tetrahedronization of the structured grid(s)and a subsequent digitalization of the constituent tetrahedrons using an adaptation of the 3D-Bresenham algorithm. Advantages and disadvantages of this approach as compared to other possible schemes are discussed. An implementation based on our open source f3d-file format for storage and transmission of volumetric data is presented, and results from applying it to real-world data shown.
Interactive Walkthroughs using "Morphable 3D-Mosaics".
This paper presents a hybrid (geometry- & image-based) technique suitable for providing interactive walkthroughs of large, complex outdoor scenes. Motion is restricted along a smooth predefined path and the input to the system is a sparse set of stereoscopic views at certain points (key-positions) along that path (one view per position). An approximate local 3D model is constructed from each view, capable of capturing photometric and geometric properties of the scene only locally. Then during the rendering process, a continuous morphing (both photometric & geometric) takes place between successive local 3D models, using what we call a "morphable 3D-model". The morphing proceeds in a physically-valid way. For this reason, a wide-baseline image matching technique is proposed, handling cases where the wide baseline between the two images is mainly due to a looming of the camera. Our system can be extended in the event of multiple stereoscopic views (and therefore multiple local models) per key-position of the path (related by a camera rotation). In that case one local 3D-mosaic (per key-position) is constructed comprising all local 3D models therein and a "morphable 3D-mosaic" is used during the rendering process. A partial-differential equation is adopted to handle the problem of geometric consistency of each 3D-mosaic.
Color, Fusion, and Stereopsis.
A Graph Cut based Adaptive Structured Light Approach for Real-Time Range Acquisition.
This paper describes a new algorithm that yields dense range maps in real-time.Reconstruction are based on a single frame structured light illumination.On-the-fly adaptation of the projection pattern renders the system more robust against scene variability. A continuous trade off between speed and quality is made. The correspondence problem is solved by using geometric pattern coding in combination with sparse color coding. Only local spatial and temporal continuity are assumed. This allows to construct a neighbor relationship within every frame and to track correspondences over time.All cues are integrated in one consistent labeling.This is achieved by reformulating the problem as a graph cut.Every cue is weighted based on its average consistency with the result within a small time window.Integration and weighting of additional cues is straightforward. The correctness of the range maps is not guaranteed, but an estimation of the uncertainty is provided for each part of the reconstruction.Our prototype is implemented using unmodified consumer hardware only.Frame rates vary between 10 and 25fps dependent on scene complexity.
3D Volume Extraction and Mesh Generation Using Energy Minimization Techniques.
Multi-Spectral Stereo Image Matching using Mutual Information.
Mutual information (MI) has shown promise as an effective stereo matching measure for images affected by radiometric distortion. This is due to the robustness of MI against changes in illumination. However, MI-based approaches are particularly prone to the generation of false matches due to the small statistical power of the matching windows. Consequently, most previous MI approaches utilise large matching windows which smooth the estimated disparity field. This paper proposes extensions to MI-based stereo matching in order to increase the robustness of the algorithm. Firstly, prior probabilities are incorporated into the MI measure in order to considerably increase the statistical power of the matching windows. These prior probabilities, which are calculated from the global joint histogram between the stereo pair, are tuned to a two level hierarchical approach. A 2D match surface, in which the match score is computed for every possible combination of template and matching window, is also utilised. This enforces left-right consistency and uniqueness constraints. These additions to MI-based stereo matching significantly enhance the algorithm's ability to detect correct matches while decreasing computation time and improving the accuracy. Results show that the MI measure does not perform quite as well for standard stereo pairs when compared to traditional area-based metrics. However, the MI approach is far superior when matching across multi-spectra stereo pairs.
3D Object Modelling in Mobile Robot Environment Using B-Spline Surfaces.
Automated Multi-view 3D Image Acquisition in Human Genome Research.
Pictorial Techniques and Intrinsic Images.
High-Resolution Cytometry Network Project: Towards Remote a d Distributed Acquisition, Processing and Visualisation of 3D Image Data in Human Genome Research.
Towards Automatic Modeling of Monuments and Towers.
Feature Based Registration of Range Images for Mapping of Natural Outdoor Feature Based Registration of Range Images for Mapping of Natural Outdoor.
Challenges related to viewpoint registration in rough forest terrain can be quite different compared to those faced in structured environments. Manoeuvring the sensor between measurement positions introduces large error into the a priori estimates of the registration coordinates. As a consequence, locally optimal registration methods may not work properly. Moreover, due to the clutter, the scene contents can change substantially even due to a relative small displacement of the sensor. Often, the sensor has to be moved to the other side of the target object, such as a group of trees, to get a good coverage of its geometry. In both cases, overlap between the two 3D data sets will be minimal ruling out conventional registration methods. In this paper, a feature-based method for registering 3D range scans for mapping natural outdoor environments is proposed. The method utilizes cylindrical, rotation symmetric features extracted from the 3D measurement data for viewpoint registration. The method is tested on real range images.
Multiresolution Distance Volumes for Progressive Surface Compression.
Progressive Compression of Volumetric Subdivision Meshes.
We present a progressive compression technique for volumetric subdivision meshes based on the slow growing refinement algorithm. The system is comprised of a wavelet transform followed by a progressive encoding of the resulting wavelet coefficients. We compare the efficiency of two wavelet transforms. The first transform is based on the smoothing rules used in the slow growing subdivision technique. The second transform is a generalization of lifted linear B-spline wavelets to the same multi-tier refinement structure. Direct coupling with a hierarchical coder produces progressive bit streams. Rate distortion metrics are evaluated for both wavelet transforms. We tested the practical performance of the scheme on synthetic data as well as data from laser indirect-drive fusion simulations with multiple fields per vertex. Both wavelet transforms result in high quality trade off curves and produce qualitatively good coarse representations.
Robust identification and matching of fiducial points for the reconstruction of 3D human faces from raw video sequences.
Effects of Joystick Mapping and Field-of-View on Human Performance in Virtual Walkthroughs.
Applications of 3D Medical Imaging in Orthopaedic Surgery: Introducing the Hip-Op System.
A Real-time Realization of Geometrical Valid View Synthesis for Tele-conferencing with Viewpoint Adaptation.
Reconstruction of Euclidean Planes from Voxels.
In this paper, we aim to formulate the recognition of a planes from a discrete point set as a nonlinear optimization problem, and we prove a uniqueness theorem for the solution of this problem. We deal with the supercover model in a space for the expression of discrete planes. The algorithm achieves invertible data compression of digital objects, since the algorithm transforms a collection voxels to a collection of plane parameters, which classify the voxels.
Thickness Histogram and Statistical Harmonic Representation for 3D Model Retrieval.
Similarity measuring is a key problem for 3D model retrieval. In this paper, we propose a novel shape descriptor "Thickness Histogram" (TH) by uniformly estimating thickness of a model using statistical methods. It is translation and rotation-invariant, discriminative to different shapes, and very efficient to compute with the Shape Distribution (SD) proposed by Osada etc. For high performance of the retrieval, we propose a robust method for translating the directional form of the statistical distribution to the harmonic representation. By summing up energies at different frequencies, a matrix shape signature is formed to provide an exhaustive characterization of 3D geometry. Experiments show that the performance of the statistical harmonic representation is among the top ones of existing shape descriptors.
Motion-induced error correction in ultrasound imaging.
3D Image Sensing for Bit Plane Method of Progressive Transmission.
Image Compression has received a lot of interest over the years. Almost all the compression algorithms and standards, discussed in literature, gather statistics and compress on the complete image and compresses to suit various requirements such as lossy / lossless, baseline/progressive, spatial, region on interest. Natural images such as gray scale images and color images are best compressed in the existing literature based on the local and global properties such as attributes of constituent pixels of the given image. In this paper, it is proposed to quantize the amplitudes of pixel values to form a number of bit planes and these bit planes are transmitted in either lossy, lossless, progressive manner. Bit plane formation is attempted from the image acquiring stage to compress and then to transmission stage. Results obtained are promising and give rise to new method or ideology in image sensing, acquiring, storage and transmission.
Small CPU Times and Fast Interactivity in Sonar Seabottom Surveys.
Sonar profiling of the seabottom provide 3D data sets that can cover huge survey areas with many gaps. This paper describes a multiresolution framework or visualization pipeline that is being optimized for dealing with such data, taking into account both the CPU time and the user interactivity. This paper describes the techniques employed: (a) the construction of a quadtree that allows to eliminate gaps by interpolating available 3D data, (b) a first but coarse visualization at a high tree level in order to rapidly change or adjust the region of interest, and (c) a very efficient triangulation (mesh reduction) that allows for a fast interactivity even at the highest detail level. By using one single octree, all processing can be combined because (1) gaps can be filled by interpolation since they are smaller at higher tree levels, and (2) connected components can be projected down the tree and refined using the data available there. As a result, huge data sets can be visualized in near realtime on normally-sized discrete grids using shading instead of wire-frames, and this enables a fast searching for objects in the seabottom. Real CPU times are presented for a real sonar data set which is visualized at a low resolution, showing the overall shape of the seabottom, and at a high resolution, showing a (semi-)buried pipeline. In order to detect an object at such a high resolution additional techniques are applied to the data: (a) an interslice interpolation in order to cope with the increased data sparseness and (b) a maximum-homogeneity filtering in order to cope with the decreased signal-to-noise-ratio. After the extraction of the pipeline a thinning technique is applied in order to be able to quantify its length.
Visualizing I/O Predictability.
3D Performance Capture for Facial Animation.
This paper describes how a photogrammetry based 3D capture system can be used as an input device for animation. The 3D Dynamic Capture System is used to capture the motion of a human face which is extracted from a sequence of 3D models captured at TV frame rate. Initially the positions of a set of landmarks on the face are extracted. These landmarks are then used to provide motion data in two different ways. First, a high level description of the movements are extracted, and these can be used as input to a procedural animation package (i.e. CreaToon). Second the landmarks can be used as registration points for a conformation process where the model to be animated is modified to match the captured model. This approach gives a new sequence of models which have the structure of the drawn model but the movement of the captured sequence.
On Robustness and Localization Accuracy of Optical Flow Computation from Color Imagery.
Accurate and efficient optical flow estimation is a major step in many computational vision problems, including tracking and 2-D/3-D mapping applications. Processing of grayscale images has been the dominant approach, with only a few studies investigating selected aspects in the use of color imagery. In a physics-based analysis to study the impact of the spectral-dependent medium attenuation on the color channels, we have shown merit in the use of color cues in the computation of optical flow for underwater imagery-the primary motivation of the investigation [Robust optical flow estimation using underwater color images]. Comparisons among various color representations and traditional intensity component on the optical flow computation are given, suggesting that the HSV representation could be the most suitable. For both underwater and terrestrial imagery, even where data in the 3 color channels are highly correlated, one expects multiple constraints from color channels to give increased robustness due to the independent channel noises. Results of experiments are given to demonstrate improved localization and accuracy.
Data-Driven Approaches to Digital Human Modeling.
Data-driven approach is an appealing way to depict people in a virtual world. The captured shape and movement data from real people are structured and combined to reproduce or create new samples in an intuitive and controllable way. We focus on the body shape modeling and elucidate the issues related to data-driven methods. The difficulty of adopting data-driven approach for human body shape modeling is due in part to the intrinsic articulated structure of the body. Since such internal structure is not measured with most of existing capture devices available today, it has to be calculated through estimation. We develop a framework for collecting and managing range scan data that automatically estimates this structure from user-tagged landmarks. By framing the captured and structurally annotated data so that statistic implicit is exploited for synthesizing new body shapes, our technique support time-saving generation of animatable body models with high realism.
Spacetime-Coherent Geometry Reconstruction from Multiple Video Streams.
By reconstructing time-varying geometry one frame at a time, one ignores the continuity of natural motion, wasting useful information about the underlying video-image formation process and taking into account temporally discontinuous reconstruction results. In 4D spacetime, the surface of a dynamic object describes a continuous 3D hyper-surface. This hyper-surface can be implicitly defined as the minimum of an energy functional designed to optimize photo-consistency. Based on an Euler-Lagrange reformulation of the problem, we find this hyper-surface from a handful of synchronized video recordings. The resulting object geometry varies smoothly over time, and intermittently invisible object regions are correctly interpolated from previously and/or future frames.
3WPS : A 3D Web-based Process Visualization Framework.
A Gesture Recognition System Using 3D Data.
Concentric Strips: Algorithms and Architecture for the Compression/Decompression of Triangle Meshes.
Description of Simple Method in 3D Reconstruction in Medical Imaging.
Encoding Volumetric Grids For Streaming Isosurface Extraction.
Gridded volumetric data sets representing simulation or tomography output are commonly visualized by displaying a triangulated isosurface for a particular isovalue. When the grid is stored in a standard format, the entire volume must be loaded from disk, even though only a fraction of the grid cells may intersect the isosurface. We propose a compressed on-disk representation for regular volume grids that allows streaming, I/O-efficient, out-of-core isosurface extraction. Unlike previous methods, we provide a guaranteed bound on the ratio between the number of cells loaded and number of cells intersecting the isosurface that applies for any isovalue. As grid cells are decompressed, we immediately extract vertices and triangles of the isosurface. Our output is a coherent streaming mesh, which facilitates subsequent processing, including on-the-fly simplification and compression.
Object Shape Modelling from Multiple Range Images by Matching Signed Distance Fields.
Filling the Signed Distance Field by Fitting Local Quadrics.
We propose a method of filling unmeasured regions of shape models integrated from multiple measurements of surface shapes. We use the signed distance field (SDF) as shape representation that contains information of the surface normal along with the signed distance at the closest point on the surface from the sampling point. We solve this problem by iteratively fitting quadratic function to generate smoothly connected SDF. We analyzed the relationship between the quadratic coefficients and the surface curvature, and by using the coefficients, we evenly propagated the SDF so that it satisfies the constraints of the field. The proposed method was tested on synthetic data and real data that was generated by integrating multiple range images.
Generation, Visualization, and Editing of 3D Video.
Effectivity of Spherical Object Reconstruction Using Star -Shaped Simplex Meshes.
High-Resolution Cytometry Network Project: Client/Server System for 3D Optical Microscope Data Storage and Analysis.
If several laboratories work in the same field and need to cooperate, it is necessary to exchange and mutually compare their computer results. The quick way to solve this problem is to unite their file formats (if it is practicable) or to install new software systems, which are data or file format compatible. A much better solution is to use only one common system. This article describes the architecture of a new client-server system, which offers possibilities of effective cooperation for laboratories doing the research in the field of the spatial organization of human genome. However, the system design is determined mainly by optical microscopy principles and therefore the system can easily be modified for use in other applications or environments. The most pressing problem during the system design was how to share and process large 3D image data in client-server architecture.
A Model (In)Validation Approach to Gait Recognition.
Edge-Constrained Marching Triangles.
Enhanced Surface Reconstruction from Wide Baseline Images.
This paper deals with the problem of dense matching from stereo images under wide baseline conditions. By considering the characteristic properties of widely separated views, we propose an extension to a recently published algorithm for detailed reconstruction of continuous surfaces. The algorithm compensates for the occurrent affine distortion between the views to allow low level intensity based comparison necessary for dense matching. The matching itself is performed by an enhanced region rowing based affine propagation method that takes surface distortion into account to handle complex piecewise-smooth surfaces. It is experimentally shown that this new method can achieve smooth and accurate reconstruction from wide baseline images of both indoor and outdoor scenes. To quantify the reconstruction results we have created realistic synthetic datasets with ground truth. These datasets form the core of a future testbed for comparison of different wide baseline surface reconstruction techniques. In the current study, results on the synthetic images are compared to the ground truth to measure the accuracy of our method.
An Easy Viewer for Out-of-Core Visualization of Huge Point-Sampled Models.
In this paper, we propose a viewer for huge point-sampled models by combining out-of-core technologies with view-dependent level-of-detail (LOD) control. This viewer is designed on the basis of a multiresolution data structure we have developed for gaze-guided visualization and transmission of 3D point sets. In order to reduce memory loads for huge point sets on general PC platforms, we introduce a partition-based out-of-core strategy to balance usage of main and external memories. At first, the data surface is partitioned into small blocks and points in each block are reorganized into error-controlled LODs by hierarchical clustering and LOD organization. In the interactive rendering process, a data block scheduling algorithm is used to realize the view-dependent paging. Experimental results show that the viewer can perform interactive visualization of huge point models on commodity graphics platforms with ease.
Direct and Robust Voxelization and Polygonization of Free-Form CSG Solids.
Network-based raditional Japanese Crafting Presentation System Using Agent and Virtual Reality Technologies.
Network-based raditional Japanese Crafting Presentation System Using Agent and Virtual Reality echnologies.
3D objects visualization for remote interactive medical applications.
Adaptive Space Carving.
In this work, we present an adaptive space carving method for scene reconstruction from a set of images obtained from low cost calibrated webcams. Our method uses a combination of silhouette and photometric information to efficiently carve the shape of the observed scene out of a volumetric space represented by an octree data structure. In this method different resolutions are considered both in object space and in image space. This led us to adopt a strategy in which the information used by the photo-consistency test is registered in scene space by projective texture mapping. Another important question addressed in this work is the high level of noise present in low cost webcams. To deal with this problem we devised a statistical photo-consistency test that uses statistical estimators for the noise introduced by the sensors of the cameras.
Dense Multiple View Stereo with General Camera Placement using Tensor Voting.
We present a computational framework for the inference of dense descriptions from multiple view stereo with general camera placement. Thus far research on dense multiple view stereo has evolved along three axes: computation of scene approximations in the form of visual hulls; merging of depth maps derived from simple configurations, such as binocular or trinocular; and multiple view stereo with restricted camera placement. These approaches are either sub-optimal, since they do not maximize the use of available information, or cannot be applied to general camera configurations. Our approach does not involve binocular processing other than the detection of tentative pixel correspondences. We require calibration information for all cameras and that there exist camera pairs which enable automatic pixel matching. The inference of scene surfaces is based on the premise that correct pixel correspondences, reconstructed in 3-D, form salient, coherent surfaces, while wrong correspondences form less coherent structures. The tensor voting framework is suitable for this task since it can process the very large datasets we generate with reasonable computational complexity. We show results on real images that present numerous challenges.
Fast 3D Model Acquisition from Stereo Images.
Bayesian Surface Reconstruction.
In this paper we illustrate an innovative method which estimates the surfaces -modelled as polygonal meshes-bounding objects present in a scene, viewed by arbitrarily placed cameras. We present a Montecarlo based iterative approach which, at every step, increases its knowledge about the scene sampling the unknown volume around the current estimation. Then, the samples which mostly appear to be consistent with the measurements are used to extend the mesh representing the surface. The reconstruction is regularized applying a filter -based on a dynamic system- to the mesh. This operation will preserve the high curvature areas of the surface, while smoothing away the noise in the estimation.
A Non Causal Bayesian Framework for Object Tracking and Occlusion Handling for the Synthesis of Stereoscopic Video.
This paper presents a framework for the synthesis of stereoscopic video using as input only a monoscopic image sequence. Initially, bi-directional 2D motion estimation is performed, which is followed by an efficient method for the reliable tracking of object contours. Rigid 3D motion and structure is recovered utilizing extended Kalman filtering. Finally, occlusions are dealt with a novel Bayesian framework, which exploits future information to correctly reconstruct occluded areas. Experimental evaluation shows that the layered object scene representation, combined with the proposed methods for object tracking throughout the sequence and occlusion handling, yields very accurate results.
Exploring Boundary Concavities in Active Contours and Surfaces.
Active contours and surfaces are deformable models used for 2D and 3D image segmentation. These models generally lack of convergence into boundary concavities, when segmenting highly concave objects. In this paper, we propose a method for exploring concavities, applicable on discrete active contours and surfaces deformed thanks to the greedy algorithm, which minimizes the energy functional. As a basis of this method, we introduce the gradient line energy (GLE), which goal is to make discrete parts of the surface coincide with boundaries, and the exploration force, computed from this energy, driving the surface into the object concavities. Our method lies on a general framework, enabling application on discrete 2D contours and 3D meshes with easy adaptation between both models.
Segmenting Correlation Stereo Range Images using Surface Elements.
This paper describes methods for segmenting planar surfaces from noisy 3D data obtained from correlation stereo vision. We make use of local planar surface elements called patchlets. Patchlets have 3D position, orientation and size parameters. As well, they have positional confidence measures based on the stereo sensor model. Patchlet orientations (i.e., surface normals) provide important additional dimensionality that reduces the ambiguity of segmentation-by-clustering. Patchlet size allows the use of continuity or coverage constraints when segmenting bounded surfaces from depth images. We use a region-growing approach to identify the number of surfaces that exist in a stereo image and obtain an initial estimate of the surface parameters. We refine segmentation using a maximum likelihood clustering approach that is optimised with Expectation-Maximisation. Confidence measures on the patchlet parameters allow proper weighting of patchlet contributions to the solution. We provide experimental results of the segmentation on complex outdoor scenes.
Viewpoint Consistent Texture Synthesis.
The purpose of this work is to synthesize textures of rough, real world surfaces under freely chosen viewing and illumination directions. Moreover, such textures are produced for continuously changing directions in such a way that the different textures are mutually consistent, i.e. emulate the same piece of surface. This is necessary for 3D animation. It is assumed that the mesostructure (small-scale) geometry of a surface is not known, and that the only input consists of a set of images, taken under different viewing and illumination directions. These are automatically aligned to build an appropriate Bidirectional Texture Function (BTF). Directly extending 2D synthesis methods for pixels to complete BTF columns has drawbacks which are exposed, and a superior sequential but highly parallelizable algorithm is proposed. Examples demonstrate the quality of the results.
A Hierarchy of Cameras for 3D Photography.
The view-independent visualization of 3D scenes is most often based on rendering accurate 3D models or utilizes image-based rendering techniques. To compute the 3D structure of a scene from a moving vision sensor or to use image-based rendering approaches, we need to be able to estimate the motion of the sensor from the recorded image information with high accuracy, a problem that has been well-studied. In this work, we investigate the relationship between camera design and our ability to perform accurate 3D photography, by examining the influence of camera design on the estimation of the motion and structure of a scene from video data. By relating the differential structure of the time varying plenoptic function to different known and new camera designs, we can establish a hierarchy of cameras based upon the stability and complexity of the computations necessary to estimate structure and motion. At the low end of this hierarchy is the standard planar pinhole camera for which the structure from motion problem is non-linear and ill-posed. At the high end is a camera, which we call the full field of view polydioptric camera, for which the motion estimation problem can be solved independently of the depth of the scene which leads to fast and robust algorithms for 3D Photography. In between are multiple view cameras with a large field of view which we have built, as well as omni-directional sensors.
Automatic Passive Recovery of 3D from Images and Video.
This paper gives a summary of automatic passive reconstruction of 3D scenes from images and video. Features are tracked between the images. Relative camera poses are then estimated based on the feature tracks. Both the feature tracking and the robust method used to estimate the camera poses has recently been shown to provide robust real-time performance. Given the camera poses, a more elaborate method is used to derive dense textured surfaces. The dense reconstruction method was originally part of the authors thesis [Automatic Dense Reconstruction from Uncalibrated Video Sequences]. It first computesdepth maps using graph cuts, optimizing a Bayesian formulation. The graph cut operation is used to let depth map hypotheses from various sources compete in order to optimize the cost function. The hypothesis generators used are feature based surface triangulation, plane fitting and multi-hypothesis multi-scale patch-based stereo. The requirements for a cost function to fit into the graph cut framework were already given in [Automatic Dense Reconstruction from Uncalibrated Video Sequences], along with a procedure for constructing the graph weights corresponding to any cost function that satisfies the requirements. Depth maps from many different viewpoints are then robustly fused to give a consistent global result using a process called median fusion. The robustness of the median fusion is important. It departs from the too common practice of fusing results by either using the union of free-space constraints as the resulting free-space, or the union of all predicted surfaces as the resulting surfaces, which is tremendously error-prone, since it essentially corresponds to taking either the minimum or maximum of the individual results.
Heterogeneous Deformation Model for 3D Shape and Motion Recovery from Multi-Viewpoint Images.
This paper presents a framework for dynamic 3D shape and motion reconstruction from multi-viewpoint images using a deformable mesh model. By deforming a mesh at a frame to that at the next frame, we can obtain both 3D shape and motion of the object simultaneously. The deformation process of our mesh model is heterogeneous. Each vertex changes its deformation process according to its 1) photometric property (i.e., if it has prominent texture or not), and 2) physical property (i.e., if it is an element of rigid part of the object or not). This heterogeneous deformation model enables us to reconstruct the object which consists of different kinds of materials or parts with different motion models, e.g., rigidly acting body parts and deforming soft clothes or its skins, by a single and unified computational framework.
The Influence of Shape on Image Correspondence.
We examine the implications of shape on the process of finding dense correspondence and half-occlusions for a stereo pair of images. The desired property of the depth map is that it should be a piecewise continuous function which is consistent with the images and which has the minimum number of discontinuities. To zeroeth order, piecewise continuity becomes piecewise constancy. Using this approximation, we first discuss an approach for dealing with such a fronto-parallel shapeless world, and the problems involved therein. We then introduce horizontal and vertical slant to create a first order approximation to piecewise continuity. We highlight the fact that a horizontally slanted surface (ie. having depth variation in the direction of the separation of the two cameras) will appear horizontally stretched in one image as compared to the other image. Thus, while corresponding two images, N pixels on a scanline in one image may correspond to a different number of pixels M in the other image, which has consequences with regard to sampling and occlusion detection. We also discuss the asymmetry between vertical and horizontal slant, and the central role of non-horizontal edges in the context of vertical slant. Using experiments, we discuss cases where existing algorithms fail, and how the incorporation of new constraints provides correct results.
Registration of Range Images that Preserves Local Surface Structures and Color.
We propose an ICP-based registration method for range images that preserves fundamental features, i.e., local structures and color, of object surfaces. The method employs local surfaces as an attribute for establishing correspondences between range images where local surfaces are evaluated geometrically and photometrically. In estimating correspondences between range images, our method evaluates consistency of shape patterns and chromaticity of local surfaces together. In estimating transformation parameters relating the coordinates between different range images, on the other hand, our method evaluates skewness and chromaticity of correspondences. These two kinds of evaluation enhances accuracy of the estimation and results in preserving local structures and color of object surfaces.
Construction of Animal Models and Motion Synthesis in 3D Virtual Environments using Image Sequences.
In this paper, we describe a system that can build 3D animal models and synthesize animations in 3D virtual environments. The model is constructed by 2D images captured by specific views. The animation is synthesised by using physical motion models of the animal and tracking data from image sequences. Finally, the user selects some points of the 3D world and a smooth and safe motion path, which passes by these points, is created. The main assumption of the 3D modelling is that the animal could be divided into parts whose normal sections are ellipses. Joints and angles between skeleton points are used in order to decrease models complexity. Using the above methodology, a snake, a lizard and a goat are reconstructed.
The Virtual Boutique: a Synergic Approach to Virtualization, Content-based Management of 3D Information, 3D Data Mining an Virtual Reality for E-commerce.
Specularity Elimination in Range Sensing for Accurate 3D Modeling of Specular Objects.
We present a novel range sensing method that is capable of constructing accurate 3Dmodels of specular objects. Our method utilizes a new range imaging concept called multi-peak range imaging, which accounts for the effects of mutual reflections. False measurements generated by mutual reflections are then eliminated by applying a series of constraint tests based on local smoothness, global coordinate consistency and visibility consistency. We show the usefulness of our method by applying the method to three real objects with specular surfaces. The ground truth data for those three objects were also acquired in order to evaluate the elimination of false measurements and to justify the selection of the parameters in the constraint tests. Experimental results indicate that our method significantly improves upon the traditional methods for constructing reliable 3D models of specular objects with complex shapes.
3D Mesh Wavelet Coding Using Efficient Model-based Bit Allocation.
3D Non-Linear Invisible Boundary Detection Filters.
The human vision system can discriminate regions which differ up to the second order statistics only. A lot of malignant tumours have boundaries which are not visible to the human eye. We present an algorithm designed to reveal "hidden" boundaries in grey level images, by computing gradients in higher order statistics of the data. We demonstrate it by applying it to the identification of possible "hidden" boundaries of gliomas as manifest themselves in MRI 3D scans.
Surface Segmentation Using Geodesic Centroidal Tesselation.
In this paper, we solve the problem of mesh partition using intrinsic computations on the 3D surface. The key concept is the notion of centroidal tesselation that is widely used in an eucidan settings. Using the Fast Marching algorithm, we are able to recast this powerful tool in the language of mesh processing. This method naturally fits into a framework for 3D geometry modelling and processing that uses only fast geodesic computations. With the use of classical geodesic-based building blocks, we are able to take into account any available information or requirement such as a 2D texture or the curvature of the surface.
Detection and Compensation of Image Sequence Jitter Due to an Unstable CCD Camera for Video Tracking of a Moving Target.
Nowadays image motion analysis is considered as a significant subject in image processing and according to its characteristics is divided into different categories. This paper focuses on camera jitter as fast translations in image sequence due to an unstable platform. Our studies have led to do correlation matching [Motion Displacement Estimation Using an Affine Modelfor Image Matching] between well featured templates[Edge Detection using KL Transform, Extracting Good Features for Motion Estimation] of successive frames to prevent aperture problems and thus have good estimation of the translations. But camera vibration may also suffer from rotation and scaling as nonlinear motions [Detection of Non-Uniform Motion in Image Sequences using a Reduced Order Likelihood Ratio Test] that may defect thematching process. Therefore mapping of 3D image points into camera image plane has been investigated to trace the effects of these nonlinear parameters in 2D motion equations [Image Based Measurement Systems]. Now by approximating these equations for minute rotation and bounded scaling between frame i and i+1, the set of motion equations with four unknowns (rotation, scaling, horizontal /vertical translations) could be solved just by LSE [Kalman Filtering, Theory and Practice using MATLAB] method for a unique and optimized response. After all when you apply the estimated displacements on each frame for compensation, an stabilized sequence will conclude.
Surface Illuminance Flow.
We introduce the concept of "surface illuminance flow" in two steps.First we reiterate the notion of the "light vector", then we proceed to decompose the light vector into a scalar "normal illuminance" and a vector "surface illuminance flow".The scalar normal illuminance generates the familiar illuminance pattern that is often known as the "shading".The vector surface illuminance flow is irrelevant for smooth surfaces (which probably explains why it is conventionally ignored) but it generates the texture due to surface mesorelief.We show how observation of the illuminance induced texture leads to robust inferences of the surface illuminance flow direction modulo 180°. This makes surface illuminance flow a property that is observable in natural scenes.Muchl ike optical flow, the surface illuminance flow is largely due to a global property that exists independent of local structure, namely the direction of the general illuminating beam (sunlight or light from the overcast sky in a typical outdoors scene).However, due to the fact that there exists "true screening" in radiometry (thus removing any hope for a "field theory" of radiometry) the local scene structure has an influence on the local light field.Such "vignetting" is a main cause of the complexity of radiometry, the other cause being the ever present multiple scatterings.This complicates matters but also introduces additional sources of information.We illustrate instances of the observation of illuminance flow in scenes.We also present a data base of roughly spherical objects, illuminated in a number of standard ways that we have used to study illuminance flow over actual rough objects.
Interactive Modeling from Dense Color and Sparse Depth.
We present research in scene modeling. The task is to build digital models of natural scenes that support interactive, photorealistic rendering. Scene modeling is the bottleneck in many computer graphics applications, notably virtual training, geometric modeling for physical simulation, cultural heritage preservation, internet marketing, and gaming. Capturing complex scenes with current modeling technology is slow, difficult, and expensive. We describe an interactive modeling system that has the potential to solve these problems.
Comparison of 3D Measurement Techniques in Cultural Heritage Application: User Point of View.
3D Model Retrieval Based on 2D Slice Similarity Measurements.
Volume Rendering on the Internet.
Surface Normals and Height from Non-Lambertian Image Data.
It is well known that many surfaces exhibit reflectance that is not well modelled by Lambert's law. This is the case not only for surfaces that are rough or shiny, but also those that are matte and composed of materials that are particle suspensions. As a result, standard Lambertian shape-from-shadingmethods can not be applied directly to the analysis of rough and shiny surfaces. In order to overcome this difficulty, in this paper, weconsider how to reconstruct the Lambertian component for rough and shiny surfaceswhen the object is illuminated in the viewing direction. To do this we make use of the diffuse reflectance models described by Oren and Nayar, and by Wolff. Our experiments with synthetic and real-world data reveal the effectiveness of the correction method, leading to improved surface normal and height recovery.
Testing Reflectance Models Against Radiance Data.
This paper describes an empirical investigation of departures from Lambert's law for rough and shiny surfaces. We commence by using a recently reported method to recover estimates ofthe surface radiance function for objects illuminated in the viewer direction. This method is non-parametric, and offers the advantage that it is simple to use since it does not require detailed camera calibration. We compare the radiance data with a number of phenomenological and physics-basedreflectance models. The models studied include those of Oren-Nayar, Wolff and different variants of the Beckmann model. The main conclusion of the study is that among these models the best fit to the empirical data is found to be the Wolff model for smooth objects and the modified Beckmann model for rough objects.
A Unified Approach for Motion Analysis and View Synthesis.
Image based rendering (IBR) consists of several steps: (i) Calibration (or ego-motion computation) of all input images. (ii) Determination of regions in the input images used to synthesize the new view. (iii) Interpolating the new view from the selected areas of the input images. We propose a unified representation for all these aspects of IBR using the Space-Time (x-y-t) volume. The presented approach is very robust, and allows to use IBR in general conditions even with a hand-held camera. To take care of (i), the Space-Time volume is constructed by placing frames at locations along the time axis so that image features create straight lines in the EPI (epipolar plane images). Different slices of the Space-Time volume are used to produce new views, taking care of (ii). Step (iii) is done by interpolating between image samples using the feature lines in the EPI images. IBR Examples are shown for various cases: sequences taken from a driving car, from a hand held camera, or when using a tripod.
A Surface Partitioning Spectrum (SPS) for Retrieval and Indexing of 3D CAD Models.
Manual indexing of large databases of geometric information is costly and often impracticable. Because of this research into retrieval and indexing schemes has focused on the development of various 3D to 2D mappings that characterise a shape as a histogram with a small number of parameters. Many methods of generating such 2D signatures (i.e. histograms) have been proposed, generally based on geometric measures of say curvature or distance. However these geometric signatures lack information about topology and tend to become indistinct as the complexity of the shape increases. This paper describes a new method for characterising both the geometry and topology of shapes in a single 2D graph, the Surface Partitioning Spectrum (SPS). We evaluate the effectiveness of using the SPS with a Neural Network to assess the similarity of shapes within a test set.
Visualization of Arbitrary-Shaped 3D Scenes on Depth-Limited 3D Displays.
We propose a depth scaling method that enables visualization of arbitrary-shaped 3D scenes on 3D displays.Most current 3D displays have a depth limitation, while the scene to be displayed has not.The trivial solutions as clipping or linear scaling of the scene's 3D bounding box suffer from non-optimal utilization of the display's capabilities. Our approach uses spatially adaptive depth scaling that maximizes the perceptual 3D effect.From the original scene geometry, the topology and local depth ordering among objects are preserved, while depth linearity is disregarded. The scaling method applies to nearly all 3D displays, such as glasses-based, head-tracked, multi-view, holographic and volumetric 3D displays.Subjective tests with the Dynamic Dimension display system show that our method significantly increases the perceptual 3D effect.
ATTEST: Advanced Three-dimensional Television System Technologies.
Accurate and robust marker localization algorithm for camera calibration.
Empirical Calibration Method for Adding Colour to Range Images.
Estimating the Surface Radiance Function from Single Images.
This paper describes a simple method for estimating the surface radiance function from single images of smooth surfaces made of materials whose reflectance function is isotropic and monotonic. The method makes implicit use of the Gauss map between the surface and a unit sphere. We assume that the material brightness is monotonic with respect to the angle between the illuminant direction and the surface normal. Under conditions in which the light source and the viewer directions are identical, we show how a tabular representation of the surface radiance function can be estimated using the cumulative distribution of image gradients. Using this tabular representation of the radiance function, surfaces may be rendered under varying light source direction by rotating the corresponding reflectance map on the Gauss sphere about the specular spike direction. We present a sensitivity study on synthetic and real-world imagery. We also present two applications which make use of the estimated radiance function. The first of these illustrates how the radiance function estimates can be used to render objects when the light and viewer directions are no longer coincident. The second application involves applying corrected Lambertian radiance to rough and shiny surfaces.
Surface Height Recovery Using Heat Flow and Manifold Embedding.
This paper makes two contributions to the problem of shape-from-shading. First, we develop a new method for surface normal recovery. We pose the problem as that of solving the steady state heat equation subject to the hard constraint that Lambert's law is satisfied. According to this picture, the surface normals are found by taking the gradient of a scalar field. The heat equation for the scalar field can be solved using simple finite difference methods and leads to an iterative procedure for surface normal estimation. The second contribution is to show how surface height recovery from the field of surface normals can be posed as one of low dimensional embedding. We experiment with the resulting method on a variety of real-world image data, where it produces qualitatively good reconstructed surfaces.
Audio effects to enhance spatial informition displays.
Estimating Curvatures and Their Derivatives on Triangle Meshes.
The computation of curvature and other differential properties of surfaces is essential formany techniques in analysis and rendering. We present a finite-differences approach for estimating curvatures on irregular triangle meshes that may be thought of as an extension of a common method for estimating per-vertex normals. The technique is efficient in space and time, and results in significantly fewer outlier estimates while more broadly offering accuracy comparable to existing methods. It generalizes naturally to computing derivatives of curvature and higher-order surface differentials.
Colon Centreline Calculation for CT Colonography using Optimised 3D Topological Thinning.
Octree approximation and compression methods.
Modeling of Free-Form Surfaces and Shape From Shading.
Modeling a free-form 3D-surface from a single view has been a widely pursued problem. The existing schemes are either fully-automatic shape-from-X techniques or involve adept interaction from the user but little or no geometric (photometric) basis. We propose a novel scheme of interactive modeling of free-form lambertian surfaces where the solution obtained is consistent with the Shape from Shading model. To this end, a reinforcement learning based scheme has been adopted which allows user intervention at any stage of the algorithm to guide the SFS solution to a global minimum.
Automatic Extraction of Planar Projections from Panoramic Range Images.
This paper presents a segmentation technique to decompose automatically a panoramic range image into a set of planar projections. It consists of three stages. Firstly, two orthogonal surface orientation histograms are generated. Secondly, from these histograms the major surfaces' orientations are extracted. Finally, a histogram of distances is computed for each one of these orientations; it will be used to define the position of the projection planes as well as the corresponding clipping planes. The original panoramic range image is divided into as many planar projections as main directions in the orientation histograms are extracted. This technique can be used with both indoor and outdoor scenes. Experimental result with a panoramic range image is presented.
Surface Model Generation from Range Images of Industrial Environments.
This paper presents an hybrid segmentation technique that combines both the speed of an edge based approach with the robustness of a surface based approach. It consists of three stages. In the first stage a scan line approximation process extracts the edges contained into the given range image. These edges are later on used to define the positions of seed points. Through the second stage a two steps region growing technique is applied. First a 2D growing process enlarges the original seed points generating bigger regions. Next, each region is fitted to a plane and a cylinder. The one that best fit the given points is selected to represent that region and used during the 3D growing stage. The 3D growing stage is carried out taking into account the approximation error from candidate points to be added to the fitted surface. In this way, each surface is grown until no points can be added according to a user defined threshold. Finally, in the third stage, a post-processing algorithm merges neighbour regions that belong to the same surface. Experimental results by using industrial environments are presented.
Unsupervised Motion Classification by Means of Efficient Feature Selection and Tracking.
This paper presents an efficient technique for human motion recognition; in particular, it is focused on labeling a movement as a walking or running displacement, which are the most frequent type of locomotion. The proposed technique consists of two stages and is based on the study of feature points' trajectories. The first stage detects peaks and valleys of points' trajectories, which are used on the second stage to discern whether the movement corresponds to a walking or a running displacement. Prior knowledge of human body kinematics structure together with the corresponding motion model are the basis for the motion recognition. Experimental results with different video sequences are presented.
Modeling closed surfaces in a multi-resolution fashion.
Compression of Isosurfaces for Structured Volumes with Context Modelling.
Second Order Local Analysis for 3D Reconstruction of Specular Surfaces.
Implementation of a Shadow Carving System for Shape Capture.
3D Reality Modelling: Photo-Realistic 3D Models of Real World Scenes.
A Formulation of Boundary Mesh Segmentation.
We present a formulation of boundary mesh segmentation as an optimization problem. Previous segmentation solutions are classified according to the different segmentation goals, the optimization criteria and the various algorithmic techniques used. We identify two primarily distinct types of mesh segmentation, namely partssegmentation and patch segmentation. We also define generic algorithms for the major techniques used for segmentation.
Extraction and Description of 3D (Articulated) Moving Objects.
Pyramid Coordinates for Morphing and Deformation.
Many model editing operations, such as morphing, blending, and shape deformation, require the ability to interactively transform the surface of a model in response to some control mechanism. For most computer graphics applications, it is important to preserve the local shape properties of input models during editing operations. Our work introduces the first, to our knowledge, mesh editing technique that explicitly preserves local shape properties. The method is based on a local shape representation, which we refer to as pyramid coordinates. The pyramid coordinates capture the local shape of the mesh around each vertex and help maintain this shape under various editing operations. They are based on a set of angles and lengths relating a vertex to its immediate neighbors. This representation is invariant under rigid transformations. Using pyramid coordinates, we introduce a new technique for mesh deformation and morphing based on a small number of user-specified control vertices. Our algorithm generate natural looking deformations and morphing sequences in seconds with minimal user interaction.
Unifying Measured Point Sequences of Deforming Objects.
Recent progress in digitizing technologies is making it possible to capture the 3D shapes of moving objects. To efficiently utilize time series records of spatial data, the information must be unified to yield coherent deforming models. This paper presents a general method that unifies unregistered 3D point sequences to generate deforming mesh models. The method does not assume any specific kinematic structure, and is applicable to any digitizer. The method first polygonizes the initial points and then deforms meshes to best fit the subsequent data points while minimizing the deformation energy. Experiments are conducted on real measured data and CG data, and successful results are obtained. As an application of the method, we examine data compression and achieve a 380 fold reduction rate for a measured data sequence.
Surface Reconstruction from Multiple Views using Rational B-Splines and Knot Insertion.
Efficient algorithm for the computation of 3D Fourier descriptors.
Visual-Hull Reconstruction from Uncalibrated and Unsynchronized Video Streams.
We present an approach for automatic reconstruction of a dynamic event using multiple video cameras recording from different viewpoints. Those cameras do not need to be calibrated or even synchronized. Our approach recovers all the necessary information by analyzing the motion of the silhouettes in the multiple video streams. The first step consists of computing the calibration and synchronization for pairs of cameras. We compute the temporal offset and epipolar geometry using an efficient RANSAC-based algorithm to search for the epipoles as well as for robustness. In the next stage the calibration and synchronization for the complete camera network is recovered and then refined through maximum likelihood estimation. Finally, a visual-hull algorithm is used to the recover the dynamic shape of the observed object. For unsynchronized video streams silhouettes are interpolated to deal with subframe temporal offsets. We demonstrate the validity of our approach by obtaining the calibration, synchronization and 3D reconstruction of a moving person from a set of 4 minute videos recorded from 4 widely separated video cameras.
Image-Based Photo Hulls.
Facial View Synthesis from a Single Image using Shape from Shading.
We present a facial view synthesis technique based on explicit shape and reflectance information extracted from a single image. The technique combines an image based reflectance estimation process with a novel method of interpolating between needle-maps recovered using shape from shading. This allows images of a face to be synthesised under novel lighting, pose and skin reflectance given only one example image. We exploit facial symmetry by reflecting the needle-map of a rotated face to yield the needle-map of the face rotated in the opposite direction. This provides two needle-maps between which interpolation can be performed.
A Variational Analysis of Shape from Specularities using Sparse Data.
Looking around in our every day environment, many of the encountered objects are specular to some degree. Actively using this fact when reconstructing objects from image sequences is the scope of the shape from specularities problem. One reason why this problem is important is that standard structure from motion techniques fail when the object surfaces are specular. Here this problem is addressed by estimating surface shape using information from the specular reflections. A specular reflection gives constraints on the surface normal. The approach taken in this paper differs significantly from many earlier shape from specularities methods since the normal data used is sparse. The main contribution of this paper is to give a solid foundation for shape from specularities problems. Estimation of surface shape using reflections is formulated as a variational problem and the surface is represented implicitly using a level set formulation. A functional incorporating all surface constraints is proposed and the corresponding level set motion PDE is explicitly derived. This motion is then proven to minimize the functional. As a part of this functional a variational approach to normal alignment is proposed and analyzed. Also novel methods for implicit surface interpolation to sparse point sets are presented together with a variational analysis. Experiments on both real and synthetic data support the proposed method.
Surface Reconstruction from the Projection of Points, Curves and Contours.
In this paper the problem of building and reconstructing geometrical surface models from multiple calibrated images is considered. We build an appropriate statistical 3D model from the images alone and show how this a priori model can be used to automatically reconstruct new instances of the object category from one or several images. The surface reconstruction method is based on a level set representation and one of the main novel contributions lie within the level set framework. Standard methods use either image-correlation or point correspondences to achieve this goal. We show how this framework can be extended to incorporate image curves and apparent contours (i.e. the projections of silhouettes). In order to automatically obtain feature correspondences, we use a statistical shape modelfor the object category of interest. The model is based on the Active Shape Model using Probabilistic PCA. The scheme is applied to build and automatically reconstruct 3D surface models of faces. The resulting system is demonstrated on a database of real face images.
Browsing 3-D spaces with 3-D vision: body-driven navigation through the Internet city.
f3d - A File Format and Tools for Storage and Manipulation of Volumetric Data Sets.
Efficient Model Creation of Large Structures based on Range Segmentation.
This paper describes an efficient 3D modeling method from 3D range data-sets that is utilizing range data segmentation. Our algorithm starts with a set of unregistered 3D range scans of a large scale scene. The scans are being pre-processed for noise removal and hole filling. The next step is range segmentation and the extraction of planar and linear features. These features are utilized for the automatic registration of the range scans into a common frame of reference [Automated feature-based range registration of urban scenes of large scale]. A volumetric-based algorithm is used for the construction of a coherent 3D mesh that encloses all range scans. Finally, the original segmented scans are used in order to simplify the constructed mesh. The mesh can now be represented as a set of planar regions at areas of low complexity and as a set of dense mesh triangular elements at areas of high complexity. This is achieved by computing the overlaps of the original segmented planar areas on the generated 3D mesh. The example of the construction of the 3D model of a building in the NYC area is presented.
PDE-based Multi-view Depth Estimation.
Triangle Mesh-Based Surface Modeling Using Adaptive Smoothing and Implicit Surface Texture Integration.
Image Matching based on Co-Motion Statistics.
This paper presents a method for matching partially overlapping image-pairs where the object of interest is in motion, even if the motion is discontinuous and in an unstructured environment. In a typical outdoor multi-camera surveillance system, an observed object as seen by separate cameras may appear very different, due to the variable influence of factors such as lighting conditions and camera angles. Thus static features such as object color, shape, and contours cannot be used for image matching. In this paper a different method is proposed for matching partially overlapping images captured by such cameras. The matching is achieved by calculation of co-motion statistics, followed by detection and rejection of points outside the overlap area and a nonlinear optimization process. The robust algorithm we describe finds point correspondences in two images without searching for any structures and without the need for tracking continuous motion. Trials using statistical motion-based image cross-registration, a robust rejection algorithm, and automatic 3D image-transformation and camera calibration on real-life outdoor images have demonstrated the feasibility of this approach.
Robust 3D Segmentation for Underwater Acoustic Images.
In this paper, a new technique for 3D acoustic image segmentation and modelling is proposed. Especially, in the underwater environment, in which optical sensors suffer from visibility problems, the acoustical devices may provide efficient solutions, but, on the other hand, acoustic image interpretation is surely more difficult for a human operator. The proposed application involves the use of an acoustic camera which directly acquires images structured as a set of 3D points. Due to the noisy nature of this type of data, the segmentation problem becomes more challenging and the standard algorithms for range image segmentation are likely to fail. The proposed method is based on a simplified version of the so called recover and select paradigm in which the seed areas, from which the segmentation starts, are generated by adopting a robust approach based on the RANSAC (RANdom Sample And Consensus) algorithm. Superquadric primitives are directly recovered from raw data without any pre-segmentation processing. Experimental trials using both synthetic and real acoustical images confirm the goodness of the method, and a large robustness of the resulting segmented images, associated to a relatively low computational load.
A Tensor Voting Approach for the Hierarchical Segmentation of 3-D Acoustic Images.
Advances in Mesh Signal Processing and Geometry Compression.
3D image sequence acquisition for TV & film production.
Panoramic Image Transform of Omnidirectional Images Using Discrete Geometry Techniques.
This paper proposes an omnidirectional-to-panoramic image transform with high accuracy using PDE-based resampling models. For the application of computer-vision techniques to omnidirectional images, the transformation of omnidirectional images to uniform-resolution quadric-surface images is needed in the two reasons. First, an omni-directional image does not have a uniform resolution. Second, the development of the computer-vision-based techniques on the quadric surface is mathematically accurate compared with the development of the techniques on the omnidirectional image directly. Therefore, our aim is to generate uniform-resolution panoramic images on cylindrical surface from nonuniform-resolution omnidirectional images. The uniform-resolution panoramic images allow us to reconstruct 3D objects and scenes from omnidirectional images robustly. Our panoramic transformation selects the uniform resolution pixels on omnidirectional images employing the geometrical configuration of cameras in the estimation and resampling process. Therefore, our method is mathematically accurate comparing to the traditional panoramic transformation using point-to-point correspondences with the geometries of cameras and the cubic convolution.
Generalized RANSAC Framework for Relaxed Correspondence Problems.
Finding correspondences between two (widely) separated views is essential for several computer vision tasks, such as structure and motion estimation and object recognition. In the wide-baseline matching using scale and/or affine invariant features the search for correspondences typically proceeds in two stages. In the first stage a putative set of correspondences is obtained based on distances between feature descriptors. In the second stage the matches are refined by imposing global geometric constraints by means of robust estimation of the epipolar geometry and the incorrect matches are rejected as outliers. For a feature in one view, usually only one "best" feature (the nearest neighbor) in the other view is chosen as corresponding feature, despite the fact that several match candidates exist. In this paper, we will consider multiple candidate matches for each feature, and integrate this choice with the robust estimation stage, thus avoiding the early commitment to the "best" one. This yields a generalized RANSAC framework for identifying the true correspondences among sets of matches. We examine the effectiveness of different sampling strategies for sets of correspondences and test the approach extensively using real examples of hard correspondence problems caused by a large motion between views and/or ambiguities due to repetitive scene structures.
Exploitation of 3D Images for Face Authentication Under Pose and Illumination Variations.
An appearance-based face authentication system integrating 2D color or intensity images and 3D data is presented in this paper. The proposed system is based on a low-cost 3D and color sensor, capable of synchronous real-time acquisition of 3D images and associated color images. Novel algorithms are proposed that exploit depth information and prior knowledge of face geometry and symmetry to achieve robust face detection, localization and authentication under conditions of background clutter, occlusion, face pose alteration and harsh illumination. A method for the enrichment of face databases with synthetically generated views depicting variations in pose and illumination is proposed to cope with head rotations and illumination variations, avoiding a cumbersome enrolment process. The performance of the proposed authentication scheme is tested thoroughly on a face database of 1500 images, recorded with the 3D acquisition system. Experimental results demonstrate significant gains resulting from the combined use of depth and color or intensity information, in comparison to the use of 2D images alone.
Archiving 3D Cultural Objects with Surface Point-Wise Database Information.
A Unified Representation for Interactive 3D Modeling.
Interactive 3D modeling is the process of building a 3D model of an object or a scene in real-time while the 3D (range) data is acquired. This is possible only if the computational complexity of all involved algorithms is linear with respect to the amount of data. We propose a new framework for 3D modeling where a complete modeling chain meets with this requirement. The framework is based on the use of vector fields as an implicit surface representation. Each modeling step, registration, surface reconstruction, geometric fusion, compression and visualization is solved and explained using the vector fields without anyintermediate representations. The proposed framework allows model reconstruction from any type of 3D data, surface patches, curves, unorganized sets of points or a combination of these.
A Volumetric Approach for Interactive 3D Modeling.
Range image registration and surface reconstruction have been traditionally considered as two independent processes where the latter relies on the results of the former. This paper presents a new approach to surface recovery from range images where the two processes are unified and performed in a common volumetric representation. While the reconstructed surface is described in its implicit form as a signed distance field within a volume, registration information for matching partial surfaces is encoded in the same volume as the gradient of the distance field. This allows coupling of both reconstruction and registration and leads to an algorithm whose complexity is linear with respect to the number of images and the number of measured 3D points. The close integration and performance gain improve interactivity in the process of modeling from range image acquisition to surface reconstruction. The distances computed in the direction of filtered normals improve robustness while preserving the sharp details of the initial range images. It is shown that the integrated algorithm is tolerant to initial registration errors as well as to measurement errors. The paper describes the representation and formalizes the approach. Experimental results demonstrate performance advantages and tolerance to aforementioned types of errors.
A Closed-Form Solution for a Two-View Self-Calibration Problem under Fixation.
It is well known that the epipolar geometry between two uncalibrated perspective views is completely encapsulated in the fundamental matrix. Since the fundamental matrix has seven degrees of freedom (DOF), self-calibration is possible if at most seven of the intrinsic or extrinsic camera parameters are unknown by extracting them from the fundamental matrix. This paper presents a linear algorithm for self-calibrating a perspective camera which undergoes fixation, that is, a special motion in which the camera's optical axis is confined in a plane. Since this fixation has four degrees of freedom, which is one smaller than that of general motion, we can extract at most three intrinsic parameters from the fundamental matrix. We here assume that the focal length (1 DOF) and the principal point (2 DOF) are unknown but fixed for two views. It will be shown that these three parameters are obtained from the fundamental matrix in an analytical fashion and a closed-form solution is derived. We also characterize all the degenerate motions under which there exists an infinite set of solutions.
Long-range high-performance time-of-flight-based 3D imaging sensors.
3-Dimensional Object Modeling with Mesh Simplification Based Resolution Adjustment.
A 3-Dimensional (3-D) object modeling technique with mesh simplification and refinement based resolution adjustment is proposed in this paper. Polygonal models which are widely utilized in the modeling of 3-D objects are taken as basis, making use of the polygonal structure and vertex coordinates for the display of 3-D models. The amount of polygons and vertices of a model is proportional to the resolution as well as data quantity. In other words the resolution and data increases with the number of polygons. In this paper, it is proposed to utilize resolution, and hence data amount, adjustable 3-D modeling so that model resolution and transmitted data amount can be regulated according to access constraints.
Active Polygon for Object Tracking.
A Practical Approach for 3D Model Indexing by combining Local and Global Invariants.
Protein folding using inter-residue contacts.
Analysis of Three-Dimensional Motion of an Object Using a Fixed Monocular Camera .
The research on analysis of three-dimensional motion by using a monocular camera instead of a stereo camera has important applications for making the microscopes used in microbiology or constructing the autonomous robots used in various fields of industry. In this paper, we propose a fixed monocular camera whose focus changed cyclically to recognize three-dimensional absolute motion of a rigid object.
3D Interactive, On-site Visualization of Ancient Olympia.
Design of a Service-Based Framework for Generic 3D Information Visualization.
Posture Recognition nd Segmentation from 3D Human Body Scans.
Fan-Meshes: A Geometric Primitive for Point-Based Description of 3D Models and Scenes.
We propose a new data structure, called Fan-Meshes (FM), for reconstructing 3D models and scenes represented by dense scanning point clouds. It is a local piecewise linear approximation to the data geometry, and can serve as primitives in reconstruction with a good balance between computational loads and reconstruction quality. In our algorithm, local remeshing is performed in preprocessing to obtain regular FMs, and a three-level-point data structure called Triangle Selection Record (TSR) is then used to reduce redundancies in the raw data and overlapping in the original FMs. Furthermore, to apply the method to raw 3D scanning data, we use a smoothing operator to the point cloud in order to eliminate some sensor noises. Experimental results demonstrate that our scheme is effective even for large-scale scenes with real data.
Uncalibrated Narrow Baseline Augmented Reality.
A Surface Evolution Approach o Probabilistic Space Carving.
Reconstruction of Three Dimensional Models from Real Images.
Speech-Driven Face Synthesis from 3D Video.
This paper presents a framework for speech-driven synthesis of real faces from a corpus of 3D video of a person speaking.Video-rate capture of dynamic 3D face shape and colour appearance provides the basis for a visual speech synthesis model.A displacement map representation combines face shape and colour into a 3D video.This representation is used to efficiently register and integrate shape and colour information captured from multiple views.To allow visual speech synthesis viseme primitives are identified from the corpus using automatic speech recognition. A novel non-rigid alignment algorithm is introduced to estimate dense correspondence between 3D face shape and appearance for different visemes.The registered displacement map representation together with a novel optical flow optimisation using both shape and colour, enables accurate and efficient non-rigid alignment.Face synthesis from speech is performed by concatenation of the corresponding viseme sequence using the non-rigid correspondence to reproduce both 3D face shape and colour appearance. Concatenative synthesis reproduces both viseme timing and co-articulation.Face capture and synthesis has been performed for a database of 51 people.Results demonstrate synthesis of 3D visual speech animation with a quality comparable to the captured video of a person.
Multi-Camera Reconstruction based on Surface Normal Estimation and Best Viewpoint Selection.
In this paper, we present a new algorithm for reconstructing an environment from images recorded by multiple calibrated cameras. Multiple camera systems challenge traditional stereo algorithms in many issues including view registration, selection of commonly visible image parts for matching, and the fact that surfaces are imaged differently from different viewpoints and poses. On the other hand, multiple cameras have the advantage of revealing surfaces at occluding contours and covering wide areas. The presented algorithm makes no assumption on camera loci and outputs an occupancy voxel grid, with occupied voxels being accompanied by a surface normal. It is correlation-based, however, outperforms the conventional correlation-based approach in reconstruction quality. It is highly parallelizable, and most importantly, is robust against artifacts due to camera registration errors that are typically encountered when using multiple cameras.
Rapid Shape Acquisition Using Color Structured Light and Multi-pass Dynamic Programming.
The Meaning and Limitations of Protein Structure Alignments.
Mosaic Construction from a Sparse Set of Views.
Focal Region-Guided Feature-Based Volume Rendering.
3D Modelling and Visualization of the Human Lung.
A method for modelling and visualizing human lungs using knowledge of lung anatomy and High Resolution CT (HRCT) images is presented. The model consists of a symbolic description of lung anatomy and a 3D atlas. The 3D atlas is constructed using HRCT volume data. A few anatomical landmarks are determined and are used to divide the lungs into anatomically and diagnostically important regions. The landmarks and the lung regions enable accurate mapping of the model to patient data and enable the system to deal with image and human variability. The model can be displayed as a set of labelled axial slices and as a 3D model of the lungs. The 3D visualization enables rotation and viewing of lung structures, lungfeatures and lung regions from different angles.
Hue Flows and Scene Structure.
Geometry of Contour-based Correspondence for Stereo.
Hierarchical Representation of Virtual Cities for Progressive Transmission over Networks.
Interactive network-based navigation over large urban environments raises difficult problems due to the size and complexity of these scenes. In this paper, we present a clientserver system allowing navigation over 3D cities in real time. Due to a novel progressive and hierarchical representation of 3D models of densely built urban areas, only perceptible details for all the regions visible from a given viewpoint are progressively streamed to visualisation clients. Furthermore, efficient coding methods are used to compress the representation data allowing quick start-up of the interactive visualisation with a highly-detailed model. This is achieved through a set of dedicated algorithms allowing a very large city model to be structured into a multi-resolution representation. The method efficiently exploits the fact that most automated modelling techniques of urban scenes provides 2D\frac{1} {2} models (building footprint, height, altitude, ...). So as to efficiently and faithfully model complex buildings, a procedural representation for roofs and facades is proposed. Finally, we present an MPEG4 compatible implementation based on the introduction of new node types with the associated bitstream.
Image Guided Geometry Inference.
We introduce a new method for filling holes in geometry obtained from 3D range scanners. Our method makes use of 2D images of the areas where geometric data is missing. The 2D images guide the filling using the relationship between the images and geometry learned from the existing 3D scanned data. Our method builds on existing techniques for using scanned geometry and for estimating shape from shaded images. Rather than creating plausibly filled holes, we attempt to approximate the missing geometry. We present results for scanned data from both triangulation and time-of-flight scanners for various types of materials. To quantitatively validate our proposed method, we also compare the filled areas with ground-truth data.
Scanline Optimization for Stereo on Graphics Hardware.
In this work we propose a scanline optimization procedure for computational stereo using a linear smoothness cost model performed by programmable graphics hardware. The main idea for an efficient implementation of this dynamic programming approach is a recursive scheme to calculate the min-convolution in a manner suitable for the parallel stream computation model of graphics processing units. Since many image similarity functions can be efficiently calculated by modern graphics hardware, it is reasonable to address the final disparity extraction by graphics processors as well. Our timing results indicate that the proposed approach is beneficial for larger image resolutions and disparity ranges in particular.
A Robust Correlation Measure for Correspondence Estimation.
A median correlation for the estimation of corresponding points in stereovision is proposed. It is based on the normalised correlation coefficient using the median instead of the mean. Its performance appears to be superior than conventional correlation specially in depth discontinuities image areas. This conclusion is derived from an empirical evaluation in which the proposed correlation is compared with the normalised correlation coefficient and the sum of absolute difference. The results show that the median correlation produces higher scores and lower estimation errors.
Scale Selection for the Analysis of Point-Sampled Curves.
An important task in the analysis and reconstruction of curvilinear structures from unorganized 3-D point samples is the estimation of tangent information at each data point. Its main challenges are in (1) the selection of an appropriate scale of analysis to accommodate noise, density variation and sparsity in the data, and in (2) the formulation of a model and associated objective function that correctly expresses their effects. We pose this problem as one of estimating the neighborhood size for which the principal eigenvector of the data scatter matrix is best aligned with the true tangent of the curve, in a probabilistic sense. We analyze the perturbation on the direction of the eigenvector due to finite samples and noise using the expected statistics of the scatter matrix estimators, and employ a simple iterative procedure to choose the optimal neighborhood size. Experiments on synthetic and real data validate the behavior predicted by the model, and show competitive performance and improved stability over leading polynomial-fitting alternatives that require a preset scale.
3D Reconstruction of Natural Scenes with View-Adaptive Multi-Texturing.
We present a 3D reconstruction and modeling system that operates on a number of input photographs that show a natural scene. Approaches from computer graphics and image processing are combined and performance is shown via experiments. Furthermore, reconstruction quality is analyzed w.r.t. the number and distribution of textures, used for reconstruction. The reconstruction pipeline starts with image acquisition, which consists of a number of photographs of the scene that are sequentially taken at different positions. Since the photographs are not acquired concurrently, they are influenced by different illumination conditions that we mandate to be preserved in the final 3D representation. In the second step, object segmentation is applied and camera calibration provided. This allows the application of shape-from-silhouette approaches, namely a hierarchical voxel approach, where different resolution layers are organized within an octree structure. For applying texture mapping, the voxel model is transformed into a wireframe, which provides smoothing of the object's surface and also reduces the number of surface primitives. Finally, a subset of original images is mapped onto the 3D geometry to provide texture information. Here, view-adaptive multi-texturing is used to preserve natural illumination. Intermediate views are interpolated automatically using adaptive real-time weight calculations for original textures.
Efficient Constraint Evaluation Algorithms for Hierarchical Next-Best-View Planning.
We recently proposed a new and efficient next-best-view algorithm for 3D reconstruction of indoor scenes using active range sensing. We overcome the computation difficulty of evaluating the view metric function by using an adaptive hierarchical approach to exploit the various spatial coherences inherent in the acquisition constraints and quality requirements. The impressive speedups have allowed our NBV algorithm to become the first to be able to exhaustively evaluate a large set of 3D views with respect to a large set of surfaces, and to include many practical acquisition constraints and quality requirements. The success of the algorithm is greatly dependent on the implementation efficiency of the constraint and quality evaluations. In this paper, we describe the algorithmic details of the hierarchical view evaluation, and present efficient algorithms that evaluate sensing constraints and surface sampling densities between a view volume and a surface patch instead of simply between a single view point and a surface point. The presentation here provides examples for the design of efficient algorithms for new sensing constraints.
3D Face Recognition with Region Committee Voting.
In this paper, we introduce a new system for face recognition by matching 3D face shape. This algorithm selects multiple regions of the face for matching in an attempt to reduce the effects caused by variations in expression between gallery and probe images. Experimental results are reported using the Face Recognition Grand Challenge v2.0 data set. Our results demonstrate improved performance relative to those of four previous papers using this same data set.
Towards On-Line Digital Doubles.
We present a modular system for real-time 3D-scanning of human bodies under motion. The high-resolution shape and colour appearance is captured by several scanning units positioned around the object of interest. Each of these units performs a foreground-background segmentation and computes a valid depth-range for the spatially neighbouring units. Multiple depth-ranges are combined in a visual hull representation, which limits the search-range for the 3D-reconstruction. Depth-estimation is based on a hierarchical mult-view-stereo plane sweep approach. Robustness and accuracy is increased by incorporating imperceptible infrared illumination as well as adding local pixel gradient information. All parts of the processing pipeline, involving camera color conversions, segmentation, depth-range computation, visual-hull generation, lossless image compression, network transfer of the infrared and colour images, and the plane sweep algorithm, are implemented on the GPU and highly optimized for speed, allowing scanning times of less than 40ms per frame. Experimental results demonstrate the applicability of our system to the creation of high-density on-line digital doubles.
High-Quality Real-Time Stereo Using Adaptive Cost Aggregation and Dynamic Programming.
We present a stereo algorithm that achieves high quality results while maintaining real-time performance. The key idea is simple: we introduce an adaptive aggregation step in a dynamic-programming (DP) stereo framework. The per-pixel matching cost is aggregated in the vertical direction only. Compared to traditional DP, our approach reduces the typical "streaking" artifacts without the penalty of blurry object boundaries. Evaluation using the benchmark Middlebury stereo database shows that our approach is among the best (ranked first in the new evaluation system) for DP-based approaches. The performance gain mainly comes from a computationally expensive weighting scheme based on color and distance proximity. We utilize the vector processing capability and parallelism in commodity graphics hardware to speed up this process over two orders of magnitude. Over 50 million disparity evaluations per second (MDE/s)1 are achieved in our current implementation.
A Spatio-Temporal Modeling Method for Shape Representation.
The spherical harmonic (SPHARM) description is a powerful surface modeling technique that can model arbitrarily shaped but simply connected three dimensional (3D) objects. Because SPHARM based 3D models can derive functional information analysis and classify different pathological symptoms, it has been used in many applications in biomedical image computing. There is an urgent requirement for efficient spatio-temporal shape modeling to represent the dynamic anatomical structures in many applications (e.g., medical image analysis, geospatial information systems). In this paper we propose a novel real spherical harmonics based spatio-temporal shape modeling method to efficiently and flexibly represent the shapes sequence of anatomical structures in medical images. Our method works well on the simply connected 3D objects and the effectiveness of our approach is demonstrated through theoretic and experimental exploration of a set of medical image applications. Furthermore, an evaluation criterion for spatio-temporal shape modeling efficiency is proposed and the comparison results showed the good performance of our method.
Efficient, Precise, and Accurate Utilization of the Uniqueness Constraint in Multi-View Stereo.
In this paper, the depth cue due to the assumption of texture uniqueness is reviewed. The spatial direction over which a similarity measure is optimized, in order to establish a stereo correspondence, is considered and methods to increase the precision and accuracy of stereo reconstructions are presented. An efficient implementation of the above methods is offered, based on optimizations that evaluate potential correspondences hierarchically, in the spatial and angular dimensions. Furthermore, the expansion of the above techniques in a multi-view framework where calibration errors cause the misregistration of individually obtained reconstructions are considered, and a treatment of the data is proposed for the elimination of duplicate reconstructions of a single surface point. Finally, a processing step is proposed for the increase of reconstruction precision and post-processing of the final result. The above contributions are integrated in a generic and parallelizable implementation of the uniqueness constraint to observe speedup and increase in the fidelity of surface reconstruction.
Orientation of Fragments of Rotationally Symmetrical 3D-Shapes for Archaeological Documentation.
Motivated by the requirements of modern archaeologists, we are developing a documentation system based on structured light for acquisition of ceramics. Fragments of ceramics are daily finds at excavations and important for archaeological research, because their shape leads to information about ancient cultures. The shapes used for documentation are called profile lines and estimated by a vertical cross-section of orientated fragments. As ceramics have been produced using rotational plates for several thousands of years, the rotational axis can be used for orientation. Therefore we conducted experiments using existing methods for estimation of the rotational axis. The drawbacks of these methods are the requirement of either complete objects or industrialized quality of symmetry. Therefore we show a new method using circle templates, which has been inspired by the manual method of archaeologists. In this work we present results using previous and related work in comparison with the estimation of the rotational axis using circle templates. The results of the presented methods are shown for synthetic data, well-known fragments and real data acquired at an archaeological excavation. Finally a conclusion and an outlook is given.
Point Containment in Discrete Arbitrary Dimension.
The point containment predicate which specifies if a point is part of a mathematically defined shape is one of the most elementary operations in computer graphics and is a natural way to perform the many raster calculations. It plays an essential role in several important processes such as filling, stroking, anti-aliasing, geometric modeling and volume rendering. This paper presents a generalized point containment algorithm for arbitrary dimension discrete objects whose main characteristics are low complexity, simple data structures and suitability for hardware implementation.
VisTRE: A Visualization Tool to Evaluate Errors in Terrain Representation.
New data sources and sensors bring new possibilities for terrain representations, and new types of characteristic errors. We develop a system to visualize and compare terrain representations and the errors they produce.
Exploiting 3D Spatial Continuity for Robust Automatic Horizon Matching across Faults.
Oil and gas exploration decisions are made based on inferences obtained from seismic data interpretation. The interpretation task is getting very time-consuming as seismic data sets become larger. Image processing tools such as auto-trackers assist manual interpretation of horizons-visible boundaries between certain sediment layers in seismic data. Auto-trackers assume data continuities; therefore, their assistance is very limited in areas of discontinuities such as faults. In this paper, we present a method for automatic horizon matching across faults based on a Bayesian approach. A stochastic matching model which integrates 3d spatial information of seismic data and prior geological knowledge is introduced. The optimal matching solution is found by MAP estimate of this model. A simulated annealing with reversible jump Markov Chain Monte Carlo algorithm is employed to sample from a-posteriori distribution. The model was applied to real 3d seismic data, and has shown to produce geologically acceptable horizons matchings.
A Range Camera Collecting Multi-Spectral Texture for Architecture Applications.
This work proposes a system for the automatic construction of multi-spectral 3D models of architecture. Besides the specific application which concerns the interactive visualization and the restoration of historical buildings, the interest of the proposed techniques lays in the multi-spectral nature of the textures which allow rendering with faithful colors and in the automatism of 3D model construction. The proposed system is an effective tool for producing 3D content amenable to a great number of applications.
Qualitative Characterization of Deforming Surfaces.
This paper extends the idea of classification schemes for static surface curvature into the temporal domain. We seek to identify regions in sequences of depth data that exhibit variations in shape change, and to characterise the nature of the deformation. From observing the change in principle curvatures we show how it is possible to decouple the type of change into one of fifteen classes, and also reveal the extent of alteration. Results are presented for synthetic and real data sequences, with additional alignment performed to accommodate global motion. This technique shows promise in analysing data from video-rate range sensors, with potential applications in biometric and psychological analysis of the face and other deformable objects.
Constraint Integration for Multiview Pose Estimation of Humans with Self-Occlusions.
Detection of articulated objects such as humans is an important task in computer vision. We present a system that incorporates a variety of constraints in a unified multi-view framework to automatically detect humans in possibly crowded scenes. These constraints include the kinematic constraints, the occlusion of one part by another and the high correlation between the appearance of parts such as the two arms. The graphical structure (non-tree) obtained is optimized in a nonparametric belief propagation framework using prior based search.
Structured Light Based Reconstruction under Local Spatial Coherence Assumption.
3D scanning techniques based on structured light usually achieve robustness against outliers by performing multiple projections to simplify correspondence. However, for cases such as dynamic scenes, the number of frames captured from a certain view must be kept as low as possible, which makes it difficult to reconstruct complex scenes with high frequency shapes and inappropriate reflection properties. To tackle this problem, we present a novel set of color stripe patterns and a robust correspondence algorithm that assume local spatial coherence in the captured data. This assumption allows us to design our stripe sequences with globally unique neighborhood properties to effectively avoid wrong correspondences. The concept of local spatial coherence is further exploited to make the ensuing surface reconstruction practically insensitive to noise, outliers, and anisotropic sampling density. Thus, the recovery of a topologically consistent manifold surface can be drastically simplified. We have successfully generated high quality meshes of various colored objects using a minimalistic projector-camera system. In particular, the full sampling capabilities of our devices can be exhausted by taking only three shots.
Recognition of Free-Form Objects in Complex Scenes Using DGI-BS Models.
Object recognition in 3D scenes with occlusion means identifying an incomplete and unknown object, which is arbitrarily posed, in an object database. This hard computer vision problem is solved in this paper through a new 3D shape representation called Depth Gradient Image Based on Silhouette (DGI-BS). DGI-BS can be used to obtain a complete model as well as a partial model of an object. This property allows us to use it in complex scenes where incomplete surfaces of objects are available. The complete DGI-BS version synthesizes surface information (through depth image) and shape information (through contour) of the whole object in a single image smaller than 1 mega pixel. Object recognition is carried out by means of a simple matching algorithm in the DGI-BS space which yields a correspondence point-to-point between scene and model. This method has been successfully tested in real scenes with no special restrictions using range sensors.
Visual Hull Construction in the Presence of Partial Occlusion.
In this paper, we propose a visual hull algorithm, which guarantees a correct construction even in the presence of partial occlusion, while "correct" here means that the real shape is located inside the visual hull. The algorithm is based on a new idea of the "extended silhouette", which requires the silhouette from background subtraction and the "occlusion mask" of the same view. In order to prepare the occlusion mask, we also propose a novel concept of "effective boundary" of moving foreground objects in a video obtained from a static camera. The accumulation of the effective boundary through time automatically gives robust occluder boundaries. We theoretically prove that our algorithm deterministically computes the tightest, correct visual hull in the presence of occlusion. Both synthetic and real examples are given as a demonstration of the correctness of the algorithm. Finally we analyze that this new algorithm is still within the time complexity of the traditional method.
Aerial LiDAR Data Classification Using Support Vector Machines (SVM).
We classify 3D aerial LiDAR scattered height data into buildings, trees, roads, and grass using the Support Vector Machine (SVM) algorithm. To do so we use five features: height, height variation, normal variation, LiDAR return intensity, and image intensity. We also use only LiDAR-derived features to organize the data into three classes (the road and grass classes are merged). We have implemented and experimented with several variations of the SVM algorithm with soft-margin classification to allow for the noise in the data. We have applied our results to classify aerial LiDAR data collected over approximately 8 square miles. We visualize the classification results along with the associated confidence using a variation of the SVM algorithm producing probabilistic classifications. We observe that the results are stable and robust. We compare the results against the ground truth and obtain higher than 90% accuracy and convincing visual results.
Resolution Scalable Coding and Region of Interest Access with Three-Dimensional SBHP Algorithm.
A low-complexity three-dimensional image compression algorithm based on wavelet transforms and set-partitioning strategy is presented. The Subband Block Hierarchial Partitioning (SBHP) algorithm is modified and extended to three dimensions, and applied to every code block independently. The resultant algorithm, 3D-SBHP, efficiently encodes 3D image data by the exploitation of the dependencies in all dimensions, while enabling progressive SNR and resolution decompression and Region-of-Interest (ROI) access from the same bit stream. The code-block selection method by which random access decoding can be achieved is outlined.The resolution scalable and random access performances are empirically investigated. The results show 3D-SBHP is a good candidate to compress 3D image data sets for multimedia applications.
Estimating a-priori Unknown 3D Axially Symmetric Surfaces from Noisy Measurements of Their Fragments.
In this paper, we present a computationally efficient technique for solving the difficult problem of estimating the global shape of a ceramic pot from measurements of its fragments. Each unknown pot is modeled as a surface of revolution, i.e., a 3D line-- the central axis of the pot -- and a 2D profile curve with respect to that axis. For each fragment, a probabilistic distribution is estimated which models both the geometric shape of the fragment and the variability of the estimated fragment shape. Estimation of the global pot shape is then a Maximum Likelihood Estimation (MLE) problem where we seek the values of the Euclidean transformation parameters that maximize the joint probability of the matched fragments' axis/profile-curvemodels (which includes the additional constraint that the matched fragments must share a common central axis). This is a new type of curve-analysis problem and our solution is a new and effective approach applicable for generic constrained 2D curve alignment and for modeling of 3D axially-symmetric surfaces and for comparing geometric models which may correspond over a subset of the complete model.
On 3D Retrieval from Photos.
In this paper, we propose a method for 3D-model retrieval from one or more photos. This method provides an "optimal" selection of 2D views to represent a 3D-model, and a probabilistic Bayesian method for 3D-model retrieval from realistic photos and sketches using these views. The characteristic view selection algorithm is based on an adaptive clustering algorithm and uses statistical model distribution scores to select the optimal number of views. We also introduce a Bayesian approach to score the probability of correspondence between the queries and the 3D-models. We present our results on the Princeton 3D Shape Benchmark database (1814 3D-models) and 50 photos (real photographs, sketches, synthesised images). A practical on-line 3D-model retrieval system based on our approach is available on the web to asset our results [1].
The Reverse Projection Correlation Principle for Depth from Defocus.
In this paper, we address the problem of finding depth from defocus in a fundamentally new way. Most previous methods have used an approximate model in which blurring is shift invariant and pixel area is negligible. Our model avoids these assumptions. We consider the area in the scene whose radiance is recorded by a pixel on the sensor, and relate the size and shape of that area to the scene's position with respect to the plane of focus. This is the notion of reverse projection, which allows us to illustrate that, when out of focus, neighboring pixels will record light from overlapping regions in the scene. This overlap results in a measurable change in the correlation between the pixels' intensity values. We demonstrate that this relationship can be characterized in such a way as to recover depth from defocused images. Experimental results show the ability of this relationship to accurately predict depth from correlation measurements.
Direct and Indirect 3-D Reconstruction from Opti-Acoustic Stereo Imaging.
Utilization of an acoustic camera for range measurements is a significant advantage for 3-D shape recovery of underwater targets by opti-acoustic stereo imaging, where the associated epipolar geometry of visual and acoustic image correspondences is described in terms of conic sections and trigonometric functions. In this paper, we propose and analyze a number of methods based on direct and indirect approaches that provide insight on the merits of the new imaging and 3-D object reconstruction paradigm. We have devised certain indirect methods, built on a regularization formulation, to first compute from noisy correspondences maximum likelihood estimates that satisfy the epipolar geometry. The 3-D target points can then be determined from a number of closed-form solutions applied to these ML estimates. An alternative direct approach is also presented for 3-D reocnstruction directly from noisy correspondences. Computer simulations verify consistency between the analytical and experimental reconstruction SNRs -- the criterion applied in performance assessment of these various solutions.
Automatic Registration of Multiple Range Images by the Local Log-Polar Range Images.
We propose a method for automatic registration of multiple range images by matching invariant feature vectors generated from the local log-polar range images. Point pairs are corresponded by finding the nearest neighbor of invariant feature vectors. The correspondence is validated, and the pairwise transformations between the input range images are determined by using the RANSAC algorithm. The registration of all input range images are determined by constructing the view tree of the input range images. The registration result of the proposed method is used as the initial value of a fine registration methods for object shape modeling.
A Modular Scheme for 2D/3D Conversion of TV Broadcast.
The 3D reconstruction from 2D broadcast video is a challenging problem with many potential applications, such as 3DTV, free-viewpoint video or augmented reality. In this paper, a modular system capable of efficiently reconstructing 3D scenes from broadcast video is proposed. The system consists of four constitutive modules: tracking and segmentation, self-calibration, sparse reconstruction and, finally, dense reconstruction. This paper also introduces some novel approaches for moving object segmentation and sparse and dense reconstruction problems. According to the simulations for both synthetic and real data, the system achieves a promising performance for typical TV content, indicating that it is a significant step towards the 3D reconstruction of scenes from broadcast video.
Fast and Efficient Dense Variational Stereo on GPU.
Thanks to their high performance and programmability, the latest graphics cards can now be used for scientific purpose. They are indeed very efficient parallel Single Instruction Multiple Data (SIMD) machines. This new trend is called General Purpose computation on Graphics Processing Unit (GPGPU [4]). Regarding the stereo problem, variational methods based on deformable models provide dense, smooth and accurate results. Nevertheless, they prove to be slower than usual disparity-based approaches. In this paper, we present a dense stereo algorithm, handling occlusions, using three cameras as inputs and entirely implemented on a Graphics Processing Unit (GPU). Experimental speedups prove that our approach is efficient and perfectly adapted to the GPU, leading to nearly video frame rate reconstruction.
Automatic 3D Face Detection, Normalization and Recognition.
A fully automatic 3D face recognition algorithm is presented. Several novelties are introduced to make the recognition robust to facial expressions and efficient. These novelties include: (1) Automatic 3D face detection by detecting the nose; (2) Automatic pose correction and normalization of the 3D face as well as its corresponding 2D face using the Hotelling Transform; (3) A Spherical Face Representation and its use as a rejection classifier to quickly reject a large number of candidate faces for efficient recognition; and (4) Robustness to facial expressions by automatically segmenting the face into expression sensitive and insensitive regions. Experiments performed on the FRGC Ver 2.0 dataset (9,500 2D/3D faces) show that our algorithm outperforms existing 3D recognition algorithms. We achieved verification rates of 99.47% and 94.09% at 0.001 FAR and identification rates of 98.03% and 89.25% for probes with neutral and non-neutral expression respectively.
Extracting 3D Shape Features in Discrete Scale-Space.
3D shape features are inherently scale-dependent. For instance, on a 3D model of a human body, the top of the head and a fingertip can both be detected as corner points, however, at entirely different scales. In this paper, we present a method for extracting and integrating 3D shape features in the discrete scale-space of a triangular mesh model. We first parameterize the surface of the mesh model on a 2D plane and then construct a dense surface normal map. In general, the parametrization is not isometric. To account for this, we compute the relative stretch of the original edge lengths. Next, we compute a dense distortion map which is used to approximate the geodesic distances on the normal map. Then, we construct a discrete scale-space of the original 3D shape by successively convolving the normal map with distortion-adapted Gaussian kernels of increasing standard deviation. We derive corner and edge detectors to extract 3D features at each scale in the discrete scale-space. Furthermore, we show how to combine the detector responses from different scales to form a unified representation of the 3D features.
Philips 3D Solutions: From Content Creation to Visualization.
Philips is realizing an end-to-end 3D display solution from 3D content creation to visualization. This development fits in our long-standing tradition of combining expertise in video processing with our strength in display development to create the most exciting and best viewing experience. Philips developed several high-quality 3D displays, ranging in resolution, viewing angle, depth experience, and sizes from 4" to 40" and up. Backwards compatibility with 2D content is enabled via signal-processing or opto-electronic 3D & 2D dual mode displays. Content creation and conversion methods are provided, which are a key factor for the success of 3D displays. Fully automatic conversion from monoscopic 2D content into 3D enables the re-use of all existing 2D video material. Further methods enable 3D animation/design, 2D to 3D conversion in post-production and live capture of new 3D content. Our efforts in MPEG standardization towards the "2D-plus-depth" format for 3D video enables a flexible interface between the variety in 3D content creation methods and the range in 3D displays. Furthermore, the 3D format is compatible with existing 2D content, standards and infrastructure. Currently, Philips offers several commercial 3D products for professional use such as digital signage, and progress is being made towards consumer products such as 3DTV.
Gaze Tracking by Using Factorized Likelihoods Particle Filtering and Stereo Vision.
This paper describes a non-intrusive method to estimate the gaze direction of a person by using stereo cameras. First, facial features are tracked with an adapted particle filtering algorithm using factorized likelihoods to estimate the 3D head pose. Next the 3D gaze vector is calculated by estimating the eyeball center and the cornea center of both eyes. For the intended application of visual perception research, we also propose a new screen registration scheme to accurately locate a planar screen in world coordinates within 2 mm error. We combine the 3D screen location with the 3D gaze vectors from both eyes to establish a point of focus on the screen. The experimental results indicate that an average error of the gaze direction of about 4.6± can be achieved and an average error of about 4 mm for the focus point location at a viewing distance of 50 cm.
Invariant High Level Reeb Graphs of 3D Polygonal Meshes.
Many applications in computer graphics need high level shape descriptions, in order to benefit from a global understanding of shapes. Topological approaches enable pertinent surface decompositions, providing structural descriptions of 3D polygonal meshes; but in practice, their use raises several difficulties. In this paper, we present a novel method for the construction of invariant high level Reeb graphs, topological entities that give a good overview of the shape structure. With this aim, we propose an accurate and straightforward feature point extraction algorithm for the computation of an invariant and meaningful quotient function. Moreover, we propose a new graph construction algorithm, based on an analysis of the connectivity evolutions of discrete level lines. This algorithm brings a practical solution for the suppression of non-significant critical points over piecewise continuous functions, providing meaningful Reeb graphs. Presented method gives accurate results, with satisfactory execution times and without input parameter. The geometrical invariance of resulting graphs and their robustness to variation in model pose and mesh sampling make them good candidates for several applications, like shape deformation (experimented in this paper), recognition, compression, indexing, etc.
Recovering Illumination and Texture Using Ratio Images.
In this paper we consider the problem of factoring illumination and texture from a pair of images of a diffuse object of known geometry. This problem arises frequently in 3D photography applications that use images to acquire photometric properties of a scanned object. Our approach uses the ratio of the images and the geometry information to compute the relative incident irradiance of one image with respect to the other. After the irradiance maps are recovered, we build a spatially varying albedo map, which can then be used to render the object under different illumination conditions. We present two algorithms, one for point-light source illumination, and another one based on spherical harmonics for more general illumination conditions.
A 3D Outdoor Scene Scanner Based on a Night-Vision Range-Gated Active Imaging System.
We present a 3D outdoor scene scanner for the acquisition of kilometers-deep scenes in night conditions. Its imaging system is based on a compact and low-cost pulsed laser illuminator and a light-intensifier equipped CCD camera. By precisely synchronizing both the illuminator and the camera shutter, it is possible to acquire "slices" of the scene at specific known distances. We show that even with large laser pulses and without megahertz-capable electronics, the third dimension can be recovered for the whole range of the scene by processing only two images acquired in specific conditions. As the pixel intensities of the images produced by active imaging systems vary with the square of the range, and due to the limited dynamics of image sensors, scanning long-range scenes with shorter "slices" allows the camera gain to be adjusted with respect to the range and the accuracy to be enhanced. The imaging system as well as the different image processing steps are detailed in this paper and an example of typical results is given.
Deformable Mesh Model for Complex Multi-Object 3D Motion Estimation from Multi-Viewpoint Video.
We propose a new algorithm using deformable mesh model for complex 3D motion estimation of multiple objects from multi-viewpoint video. In this paper, we define "complex motion" as motion which includes global change of the object shape topology. In complex motion, a part of the object may touch the other parts. To manage this effect, we introduce (1) "repulsive force" into deformable mesh model for simple motion estimation which integrates texture and silhouette information into unified computation scheme, and (2) efficient collision detection algorithm for deformable mesh model. Our deformable mesh model with repulsive force keeps hidden, collided surfaces to be touched each other, and gives dense, non-rigid complex 3D motion of the object. Some experimental results show that our deformation model can estimate motions of multiple objects and the object's motion with time-varying global topology, and gives topologically-consistent mesh models which can be compressed efficiently by conventional inter-frame 3D data compression algorithms and be used for 3D motion analysis.
Computing the Camera Motion Direction from Many Images.
We analyze the problem of estimating a camera's motion direction from a calibrated multi-image sequence. We assume that the camera moves roughly along a line and that its velocity and orientation are unknown and can vary over time. For infinitesimal camera motion (multiple flows rather than multiple images), we give a closed-form expression for the result of minimizing the true least-squares error over all variables but the camera's motion direction. Our result includes the rigidity constraint that the scene stays fixed over time. For finite motion, we present a noniterative algorithm that approximates the exact multi-image coplanarity error to better than a percent. Also, we define a new error contribution that incorporates the rigidity constraint and is analogous to the rigidity component of the error for infinitesimal motion. By adding this to the coplanarity error, we obtain a noniterative algorithm that approximates the complete finite motion least-squares error--including rigidity--as a function just of the translation direction.
Minimum Spanning Tree Pose Estimation.
The extrinsic camera parameters from video stream images can be accurately estimated by tracking features through the image sequence and using these features to compute parameter estimates. The poses for long video sequences have been estimated in this manner. However, the poses of large sets of still images cannot be estimated using the same strategy because wide-baseline correspondences are not as robust as narrow-baseline feature tracks. Moreover, video pose estimation requires a linear or hierarchically-linear ordering on the images to be calibrated, reducing the image matches to the neighboring video frames. We propose a novel generalization to the linear ordering requirement of video pose estimation by computing the Minimum Spanning Tree of the camera adjacency graph and using the tree hierarchy to determine the calibration order for a set of input images. We validate the pose accuracy using an error metric that is functionally independent of the estimation process. Because we do not rely on feature tracking for generating feature correspondences, our method can use internally calibrated wide- or narrow-baseline images as input, and can estimate the camera poses from multiple video streams without special pre-processing to concatenate the streams.
Multiview 3D Tracking with an Incrementally Constructed 3D Model.
We propose a multiview tracking method for rigid objects. Assuming that a part of the object is visible in at least two cameras, a partial 3D model is reconstructed in terms of a collection of small 3D planar patches of arbitrary topology. The 3D representation, recovered fully automatically, allows to formulate tracking as gradient minimization in pose (translation, rotation) space. As the object moves, the 3D model is incrementally updated. A virtuous circle emerges: tracking enables composition of the partial 3D model; the 3D model facilitates and robustifies the multi-view tracking. We demonstrate experimentally that the interleaved track-and-reconstruct approach successfully tracks a 360 degrees turn-around and a wide range of motions. Monocular tracking is also possible after the model is constructed. Using more cameras, however, significantly increases stability in critical poses and moves. We demonstrate how to exploit the 3D model to increases stability in the presence of uneven and/or changing illumination.
Hemispherical Harmonic Surface Description and Applications to Medical Image Analysis.
The use of surface harmonics for rigid and nonrigid shape description is well known. In this paper we define a set of complete hemispherical harmonic basis functions on a hemisphere domain and propose a novel parametric shape description method to efficiently and flexibly represent the surfaces of anatomical structures in medical images. As the first application of hemispherical harmonic theory in shape description, our technique differs from the previous surface harmonics shape descriptors, all of which don't work efficiently on the hemisphere-like objects that often exist in medical anatomical structures (e.g., ventricles, atriums, etc.). We demonstrate the effectiveness of our approach through theoretic and experimental exploration of a set of medical image applications. Furthermore, an evaluation criterion for surface modeling efficiency is described and the comparison results demonstrated that our method outperformed the previous approaches using spherical harmonic models.
Belief Propagation for Panorama Generation.
We present an algorithm for generating panoramic images of complex scenes from a multi-sensor camera. We further present a programmable graphics hardware implementation to process the large data sets more quickly. Because the sensors do not share the same center of projection, nearby objects may not be properly aligned, creating a ghosting or echoing effect in the generated panorama, unless correct depth information is taken into account. Taking a cue from the similar problem of dense stereo, we approximate our scene with a Markov random field and use belief propagation to estimate the maximum a posteriori panoramic image for that scene.
Reflectance Modeling for Layered Dielectrics with Rough Surface Boundaries.
A new model for the scattering of light from layered dielectrics with rough surface boundaries is introduced. The model contains a surface scattering component together with a subsurface scattering component. The former component corresponds to the roughness on the upper surface boundary and is modeled using the modified Beckmann model. The latter component accounts for both refraction due to Fresnel transmission through the layer and rough scattering at the lower layer boundary. By allowing independent roughness parameters for each surface boundary we can achieve excellent fits of the model to the measured BRDF data. Using a well known method of testing reflectance models, we experiment with BRDF data from skin surface samples (human volunteers) and show that the new model outperforms alternative variants of the Beckmann model and the Lafortune et al. reflectance model. As an application in computer graphics, we also show that realistic images of 3D surfaces can be generated using the new model, by setting the values of its physical parameters.
Multi-View Multi-Exposure Stereo.
Multi-view stereo algorithms typically rely on same-exposure images as inputs due to the brightness constancy assumption. While state-of-the-art depth results are excellent, they do not produce high-dynamic range textures required for high-quality view reconstruction. In this paper, we propose a technique that adapts multi-view stereo for different exposure inputs to simultaneously recover reliable dense depth and high dynamic range textures. In our technique, we use an exposure-invariant similarity statistic to establish correspondences, through which we robustly extract the camera radiometric response function and the image exposures. This enables us to then convert all images to radiance space and selectively use the radiance data for dense depth and high dynamic range texture recovery. We show results for synthetic and real scenes.
Synthesis of 3D Model of a Magnetic Field-Influenced Body from a Single Image.
A method for recovery of a 3D model of a planet-sized cloud-like structure that is in motion and deforming but approximately governed by magnetic field properties is described. The method allows recovery of the model from a single intensity image in which the structure's silhouette can be observed. The method exploits envelope theory and a magnetic field model. Given one intensity image and the segmented silhouette in the image, the method proceeds without human intervention to produce the 3D model. In addition to allowing 3D model synthesis, the method's capability to yield a very compact description offers further utility. Application of the method to real-world data is also demonstrated.
A Factorization Based Self-Calibration for Radially Symmetric Cameras.
The paper proposes a novel approach for planar self-calibration of radially symmetric cameras. We model these camera images using notions of distortion center and concentric distortion circles around it. The rays corresponding to pixels lying on a single distortion circle form a right circular cone. Each of these cones is associated with two unknowns; optical center and focal length (opening angle). In the central case, we consider all distortion circles to have the same optical center, whereas in the non-central case they have different optical centers lying on the same optical axis. Based on this model we provide a factorization based self-calibration algorithm for planar scenes from dense image matches. Our formulation provides a rich set of constraints to validate the correctness of the distortion center. We also propose possible extensions of this algorithm in terms of non-planar scenes, non-unit aspect ratio and multi-view constraints. Experimental results are shown.
3D Content-Based Search Based on 3D Krawtchouk Moments.
In this paper a novel method for 3D content-based search and retrieval is proposed. Guided by the imperative need for a reliable 3D content based search tool and the very interesting results of research work done in the past on the performance of Krawtchouk moments and Krawtchouk moment invariants in image processing, Weighted 3D Krawtchouk moments are introduced for efficient 3D analysis which are suitable for content-based search and retrieval applications. The proposed method was tested on Princeton Shape Benchmark. Experiments have shown that the proposed method is superior in terms of precision-recall comparing with other well-known methods reported in the literature.
A CSC Based Classification Method for CT Bone Images.
The CSC (color structure code) is a robust and fast two dimensional segmentation method which has been already generalized to three dimensional images. As the CSC does not need any prior knowledge it can be used for different applications. In this paper we focus on the segmentation of bones from Computer Tomography data (CT) with the CSC. In the postprocessing step CSC segments will be classified according to their average hounsfield value. The classification is steered by some application specific topological rules.
Illumination Insensitive Model-Based 3D Object Tracking and Texture Refinement.
A common approach to model-based tracking is to use a model of the object to predict what will be observed, and then to compare that with real observations. For methods that make use of the object's photometric properties (appearance) in their measurements, illumination inconsistencies between the modeled and actual scene can cause tracking problems. In this paper we address one case: model-based tracking of Lambertian objects under directional light sources. We present an iterative optimization method that uses a Kalman filter to simultaneously refine estimates of the object motion, the illumination, and the model texture. We model the illumination variance between the real and predicted observation using the intensity ratios of corresponding surface points, which we then use to make model-based image predictions consistent with the real lighting. To demonstrate the effectiveness of our method we present experimental results using both synthetic (controlled) and real image sequences.
Object Centered Stereo: Displacement Map Estimation Using Texture and Shading.
We consider the problem of recovering 3D surface displacements using both shading and multi-view stereo cues. In contrast to traditional disparity or depth map representations, the object centered displacement map representation enables the recovery of complete 3D objects while also ensuring the reconstruction is not biased towards a particular image. Although displacement mapping requires a base surface, this base mesh is easily obtained using traditional computer vision techniques (e.g., shape-from-silhouette or structure-from-motion). Our method exploits shading variation due to object rotation relative to the light source, allowing the recovery of displacements in both textured and textureless regions in a common framework. In particular, shading cues are integrated into a multi-view stereo photo-consistency function through the surface normals that are implied by the displacement map. The analytic gradient of this photo-consistency function is used to drive a multiresolution conjugate gradient optimization. We demonstrate the geometric quality of the reconstructed displacements on several example objects including a human face.
Flatness and Orientation Signature for Modeling and Matching 3D Objects.
This paper proposes a new technique for modeling and matching three-dimensional rigid objects by encoding the fluctuation of the surface and the variation of its normal around an oriented point on the surface as the surface expands. The surface of the object is encoded into two two-dimensional curves as the surface signature on each point, and then the collection of the signatures are used to model and match the object. The signatures implicitly encode the curvature and symmetry of the surface around an oriented point. This modeling technique is robust to scale, orientation, noise, patch resolution, occlusion, and cluttering.
Large-Scale Modeling of Parametric Surfaces Using Spherical Harmonics.
We present an approach for large-scale modeling of parametric surfaces using spherical harmonics (SHs). A standard least square fitting (LSF) method for SH expansion is not scalable and cannot accurately model large 3D surfaces. We propose an iterative residual fitting (IRF) algorithm, and demonstrate its effectiveness and scalability in creating accurate SH models for large 3D surfaces. These large-scale and accurate parametric models can be used in many applications in computer vision, graphics, and biomedical imaging. As a simple extension of LSF, IRF is very easy to implement and requires few machine resources.
Graph Cut Based Multiple View Segmentation for 3D Reconstruction.
In this paper we propose a novel framework for efficiently extracting foreground objects in so called shortbaseline image sequences. We apply the obtained segmentation to improve subsequent 3D reconstruction results. Essentially, our framework combines a graph cut based optimization algorithm with an intuitive user interface. At first a meanshift segmentation algorithm partitions each image of the sequence into a certain number of regions. Additionally we provide an intelligent graphical user interface for easy specification of foreground as well as background regions across all images of the sequence. Within the graph cut optimization algorithm we define new energy terms to increase the robustness and to keep the segmentation of the foreground object coherent across all images of the sequence. Finally, a refined graph cut segmentation and several adjustment operations allow an accurate and effective foreground extraction. The obtained results are demonstrated on several real world data sets.
Depth Images: Representations and Real-Time Rendering.
Depth Images are viable representations that can be computed from the real world using cameras and/or other scanning devices. The depth map provides 2\frac{1}{2}D structure of the scene. A set of Depth Images can provide hole-free rendering of the scene. Multiple views need to blended to provide smooth hole-free rendering, however. Such a representation of the scene is bulky and needs good algorithms for real-time rendering and efficient representation. In this paper, we present a discussion on the Depth Image representation and provide a GPU-based algorithm that can render large models represented using DIs in real time. We then present a proxy-based compression scheme for Depth Images and provide results for the same. Results are shown on synthetic scenes under different conditions and on some scenes generated from images. Lastly, we initiate discussion on varying quality levels in IBR and show a way to create representations using DIs with different trade-offs between model size and rendering quality. This enables the use of this representation for a variety of rendering situations.
How Far Can We Go with Local Optimization in Real-Time Stereo Matching.
Applications such as robot navigation and augmented reality require high-accuracy dense disparity maps in real-time and online. Due to time constraint, most real-time stereo applications rely on local winner-take-all optimization in the disparity computation process. These local approaches are generally outperformed by offline global optimization based algorithms. However, recent research shows that, through carefully selecting and aggregating the matching costs of neighboring pixels, the disparity maps produced by a local approach can be more accurate than those generated by many global optimization techniques. We are therefore motivated to investigate whether these cost aggregation approaches can be adopted in real-time stereo applications and, if so, how well they perform under the real-time constraint. The evaluation is conducted on a real-time stereo platform, which utilizes the processing power of programmable graphics hardware. Several recent cost aggregation approaches are also implemented and optimized for graphics hardware so that real-time speed can be achieved. The performances of these aggregation approaches in terms of both processing speed and result quality are reported.
A Bayesian Approach to Building Footprint Extraction from Aerial LIDAR Data.
Building footprints have been shown to be extremely useful in urban planning, infrastructure development, and roof modeling. Current methods for creating these footprints are often highly manual and rely largely on architectural blueprints or skilled modelers. In this work we will use aerial LIDAR data to generate building footprints automatically. Existing automatic methods have been mostly unsuccessful due to large amounts of noise around building edges. We present a novel Bayesian technique for automatically constructing building footprints from a pre-classified LIDAR point cloud. Our algorithm first computes a boundederror approximate building footprint using an application of the shortest path algorithm. We then determine the most probable building footprint by maximizing the posterior probability using linear optimization and simulated annealing techniques. We have applied our algorithm to more than 300 buildings in our data set and observe that we obtain accurate building footprints compared to the ground truth. Our algorithm is automatic and can be applied to other man-made shapes such as roads and telecommunication lines with minor modifications.
A System for Reconstructing Integrated Texture Maps for Large Structures.
We consider the problem of creating integrated texture maps of large structures scanned with a time-of-flight laser scanner and imaged with a digital camera. The key issue in creating integrated textures is correcting for the spatially varying illumination across the structure. In most cases, the illumination cannot be controlled, and dense spatial estimates of illumination are not possible. We present a system for processing multiple color images into an integrated texture that makes use of the laser scanner return intensity and the captured geometry, together with color balancing and mapping of illumination-corrected images onto the target geometry after filtering into two spatial frequency bands.
Motion Editing in 3D Video Database.
As the next generation of media, 3D video is attracting increased attention. 3D video is a sequence of three dimensional mesh models, captured and generated for a real dynamic object. In this paper, we present a simple framework of motion editing in 3D video database to re-use 3D video data. Our system is composed of two modules. In the first module, a motion database is automatically set up from original 3D video sequences off-line by analyzing the feature vectors of each frame. It is observed that our original 3D video sequences have a two-level temporal structure. A fine-to-coarse method is proposed to extract such a structure. 3D video is segmented into the fine-level structure by a three reference frame strategy and then is clustered into the coarse-level structure. In the second module, users can synthesize the motions to edit a new 3D video sequence online. A cost function is optimized to transit between two motions with the users' requirements. All the algorithms in the system are based on the analysis in feature vector space and the edited 3D video sequence is played using OpenGL.
The ASDMCon Project: The Challenge of Detecting Defects on Construction Sites.
Techniques for three dimensional (3D) imaging and analysis of as-built conditions of buildings are gaining acceptance in the Architecture, Engineering, and Construction (AEC) community. Early detection of defects on construction sites is one domain where these techniques have the potential to revolutionize an industry, since construction defects can consume a significant portion of a project's budget. The ASDMCon project is developing methods to aid site managers in detecting and managing construction defects using 3D imaging and other advanced sensor technologies. This paper presents an overview of the project, its 4D visualization environment, and the 3D segmentation and recognition strategies that are being employed to automate defect detection.
High-Performance Multi-View Reconstruction.
We present a high performance reconstruction approach, which generates true 3D models from multiple views with known camera parameters. The complete pipeline from depth map generation over depth image integration to the final 3D model visualization is performed on programmable graphics processing units (GPUs). The proposed pipeline is suitable for long image sequences and uses a plane-sweep depth estimation procedure optionally employing robust image similarity functions to generate a set of depth images. The subsequent volumetric fusion step combines these depth maps into an impicit surface representation of the final model, which can be directly displayed using GPU-based raycasting methods. Depending on the number of input views and the desired resolution of the final model the computing times range from several seconds to a few minutes. The quality of the obtained models is illustrated with real-world datasets.
Multiple Camera Calibration Using Robust Perspective Factorization.
In this paper we address the problem of recovering structure and motion from a large number of intrinsically calibrated perspective cameras. We describe a method that combines (1) weak-perspective reconstruction in the presence of noisy and missing data and (2) an algorithm that updates weak-perspective reconstruction to perspective reconstruction by incrementally estimating the projective depths. The method also solves for the reversal ambiguity associated with affine factorization techniques. The method has been successfully applied to the problem of calibrating the external parameters (position and orientation) of several multiple-camera setups. Results obtained with synthetic and experimental data compare favourably with results obtained with nonlinear minimization such as bundle adjustment.
Angle Independent Bundle Adjustment Refinement.
Obtaining a digital model of a real-world 3D scene is a challenging task pursued by computer vision and computer graphics. Given an initial approximate 3D model, a popular refinement process is to perform a bundle adjustment of the estimated camera position, camera orientation, and scene points. Unfortunately, simultaneously solving for both camera position and camera orientation is an ill-conditioned problem. To address this issue, we propose an improved, camera-orientation independent cost function that can be used instead of the standard bundle adjustment cost function. This yields a new bundle adjustment formulation which exhibits noticeably better numerical behavior, but at the expense of an increased computational cost. We alleviate the additional cost by automatically partitioning the dataset into smaller subsets. Minimizing our cost function for these subsets still achieves significant error reduction over standard bundle adjustment. We empirically demonstrate our formulation using several different size models and image sequences.
Image Based Localization in Urban Environments.
